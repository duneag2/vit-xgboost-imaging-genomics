{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d89bae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Sat Feb 26 2022\\nLast revised on Thu Apr 20 2023\\n\\nA Jupyer Notebook for Multimodal integration of neuroimaging and genetic data for the diagnosis of mood disorders based on computer vision models\\nViT-XGBoost (imaging genomics)\\n\\n@author: \\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Feb 26 2022\n",
    "Last revised on Mon Mar 18 2024\n",
    "\n",
    "A Jupyer Notebook for Multimodal integration of neuroimaging and genetic data for the diagnosis of mood disorders based on computer vision models\n",
    "ViT-XGBoost (imaging genomics)\n",
    "\n",
    "@author: \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eed9df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.13.1-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchsummary\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5efdbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "torch.manual_seed(256)\n",
    "torch.cuda.manual_seed(256)\n",
    "torch.cuda.manual_seed_all(256)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f5a4bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61934ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fe5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrlvalue = []\n",
    "\n",
    "temp = os.listdir(\"/path/to/control/\")\n",
    "for i in range(len(temp)):\n",
    "     os.chdir(os.path.join(\"/path/to/control/\", temp[i]))\n",
    "     temp2 = os.listdir(os.getcwd())\n",
    "     for j in range(len(temp2)):\n",
    "         if temp2[j][-3:] == 'nii':\n",
    "             nii = nib.load(os.path.join(os.getcwd(), temp2[j])).get_fdata()\n",
    "             nii2 = nii.reshape(512, -1)\n",
    "             nii2 = nii2[80:410, :]\n",
    "             nii3 = scaler.fit_transform(nii2)\n",
    "             ctrlvalue.append(nii3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd05357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ctrlvalue)):\n",
    "    ctrlvalue[i] = cv2.resize(ctrlvalue[i], (384, 384), interpolation= cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc619b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ctrlvalue)):\n",
    "    ctrlvalue[i] = torch.stack([torch.tensor(ctrlvalue[i]), torch.tensor(ctrlvalue[i]), torch.tensor(ctrlvalue[i])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43502789",
   "metadata": {},
   "outputs": [],
   "source": [
    "mddvalue = []\n",
    "\n",
    "temp = os.listdir(\"/path/to/MDD/\")\n",
    "for i in range(len(temp)):\n",
    "     os.chdir(os.path.join(\"/path/to/MDD/\", temp[i]))\n",
    "     temp2 = os.listdir(os.getcwd())\n",
    "     for j in range(len(temp2)):\n",
    "         if temp2[j][-3:] == 'nii':\n",
    "             nii = nib.load(os.path.join(os.getcwd(), temp2[j])).get_fdata()\n",
    "             nii2 = nii.reshape(512, -1)\n",
    "             nii2 = nii2[80:410, :]\n",
    "             nii3 = scaler.fit_transform(nii2)\n",
    "             mddvalue.append(nii3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b93227",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mddvalue)):\n",
    "    mddvalue[i] = cv2.resize(mddvalue[i], (384, 384), interpolation= cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c676851",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mddvalue)):\n",
    "    mddvalue[i] = torch.stack([torch.tensor(mddvalue[i]), torch.tensor(mddvalue[i]), torch.tensor(mddvalue[i])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b35e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipvalue = []\n",
    "\n",
    "temp = os.listdir(\"/path/to/bipolar/\")\n",
    "for i in range(len(temp)):\n",
    "     os.chdir(os.path.join(\"/path/to/bipolar/\", temp[i]))\n",
    "     temp2 = os.listdir(os.getcwd())\n",
    "     for j in range(len(temp2)):\n",
    "         if temp2[j][-3:] == 'nii':\n",
    "             nii = nib.load(os.path.join(os.getcwd(), temp2[j])).get_fdata()\n",
    "             nii2 = nii.reshape(512, -1)\n",
    "             nii2 = nii2[80:410, :]\n",
    "             nii3 = scaler.fit_transform(nii2)\n",
    "             bipvalue.append(nii3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d006a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bipvalue)):\n",
    "    bipvalue[i] = cv2.resize(bipvalue[i], (384, 384), interpolation= cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2afc46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bipvalue)):\n",
    "    bipvalue[i] = torch.stack([torch.tensor(bipvalue[i]), torch.tensor(bipvalue[i]), torch.tensor(bipvalue[i])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "778da40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = ctrlvalue + mddvalue + bipvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a50ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idlist = pd.read_csv(\"id_list_for_fusion.csv\").drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "260f287a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0    24\n",
       "1    52\n",
       "2    93\n",
       "3    54\n",
       "4    49\n",
       "..   ..\n",
       "316  61\n",
       "317  16\n",
       "318  12\n",
       "319   0\n",
       "320  28\n",
       "\n",
       "[321 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f9f45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(data1)):\n",
    "    data.append(data1[idlist.loc[i, :].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c12b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c3248e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr1:8418644:C:A_C</th>\n",
       "      <th>chr1:8418650:T:C_T</th>\n",
       "      <th>chr1:8421203:T:C_T</th>\n",
       "      <th>chr1:8422676:T:C_T</th>\n",
       "      <th>chr1:8526142:G:A_G</th>\n",
       "      <th>chr1:8557320:C:G_C</th>\n",
       "      <th>chr1:8557691:A:C_A</th>\n",
       "      <th>chr1:52289220:TGTATTAAGGTCAA:T_TGTATTAAGGTCAA</th>\n",
       "      <th>chr1:52293570:G:A_G</th>\n",
       "      <th>chr1:52306063:ATCT:A_ATCT</th>\n",
       "      <th>...</th>\n",
       "      <th>chr19:19359854:TTGTGTGTG:T_T</th>\n",
       "      <th>chr19:19359854:T:TTGTG_TTGTG</th>\n",
       "      <th>chr20:47639464:G:C_G</th>\n",
       "      <th>chr20:47675176:T:A_T</th>\n",
       "      <th>chr22:41215421:G:GGC_G</th>\n",
       "      <th>chr22:41215421:G:GGT_GGT</th>\n",
       "      <th>chr22:42525952:C:A_C</th>\n",
       "      <th>chr22:42537023:T:C_T</th>\n",
       "      <th>chr22:42538399:C:T_C</th>\n",
       "      <th>chr22:42539623:C:A_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     chr1:8418644:C:A_C  chr1:8418650:T:C_T  chr1:8421203:T:C_T  \\\n",
       "0                   1.0                 1.0                 1.0   \n",
       "1                   0.0                 0.0                 0.0   \n",
       "2                   0.0                 0.0                 0.0   \n",
       "3                   1.0                 1.0                 1.0   \n",
       "4                   1.0                 1.0                 1.0   \n",
       "..                  ...                 ...                 ...   \n",
       "316                 0.0                 0.0                 0.0   \n",
       "317                 0.0                 0.0                 0.0   \n",
       "318                 0.0                 0.0                 0.0   \n",
       "319                 0.0                 0.0                 0.0   \n",
       "320                 0.0                 0.0                 0.0   \n",
       "\n",
       "     chr1:8422676:T:C_T  chr1:8526142:G:A_G  chr1:8557320:C:G_C  \\\n",
       "0                   1.0                 0.0                 0.0   \n",
       "1                   0.0                 0.0                 0.0   \n",
       "2                   0.0                 1.0                 NaN   \n",
       "3                   1.0                 0.0                 0.0   \n",
       "4                   1.0                 0.0                 0.0   \n",
       "..                  ...                 ...                 ...   \n",
       "316                 0.0                 0.0                 0.0   \n",
       "317                 0.0                 0.0                 0.0   \n",
       "318                 0.0                 0.0                 0.0   \n",
       "319                 0.0                 1.0                 1.0   \n",
       "320                 0.0                 1.0                 1.0   \n",
       "\n",
       "     chr1:8557691:A:C_A  chr1:52289220:TGTATTAAGGTCAA:T_TGTATTAAGGTCAA  \\\n",
       "0                   0.0                                            0.0   \n",
       "1                   0.0                                            0.0   \n",
       "2                   1.0                                            0.0   \n",
       "3                   0.0                                            0.0   \n",
       "4                   0.0                                            0.0   \n",
       "..                  ...                                            ...   \n",
       "316                 0.0                                            0.0   \n",
       "317                 0.0                                            0.0   \n",
       "318                 0.0                                            0.0   \n",
       "319                 1.0                                            0.0   \n",
       "320                 1.0                                            0.0   \n",
       "\n",
       "     chr1:52293570:G:A_G  chr1:52306063:ATCT:A_ATCT  ...  \\\n",
       "0                    0.0                        0.0  ...   \n",
       "1                    0.0                        0.0  ...   \n",
       "2                    0.0                        0.0  ...   \n",
       "3                    0.0                        0.0  ...   \n",
       "4                    0.0                        0.0  ...   \n",
       "..                   ...                        ...  ...   \n",
       "316                  0.0                        0.0  ...   \n",
       "317                  0.0                        0.0  ...   \n",
       "318                  0.0                        0.0  ...   \n",
       "319                  0.0                        0.0  ...   \n",
       "320                  0.0                        0.0  ...   \n",
       "\n",
       "     chr19:19359854:TTGTGTGTG:T_T  chr19:19359854:T:TTGTG_TTGTG  \\\n",
       "0                             NaN                           NaN   \n",
       "1                             0.0                           0.0   \n",
       "2                             NaN                           NaN   \n",
       "3                             NaN                           NaN   \n",
       "4                             NaN                           NaN   \n",
       "..                            ...                           ...   \n",
       "316                           0.0                           0.0   \n",
       "317                           0.0                           0.0   \n",
       "318                           0.0                           0.0   \n",
       "319                           NaN                           NaN   \n",
       "320                           NaN                           NaN   \n",
       "\n",
       "     chr20:47639464:G:C_G  chr20:47675176:T:A_T  chr22:41215421:G:GGC_G  \\\n",
       "0                     1.0                   1.0                     0.0   \n",
       "1                     0.0                   0.0                     NaN   \n",
       "2                     1.0                   1.0                     NaN   \n",
       "3                     1.0                   NaN                     NaN   \n",
       "4                     0.0                   0.0                     NaN   \n",
       "..                    ...                   ...                     ...   \n",
       "316                   1.0                   1.0                     1.0   \n",
       "317                   1.0                   1.0                     NaN   \n",
       "318                   0.0                   0.0                     NaN   \n",
       "319                   1.0                   1.0                     NaN   \n",
       "320                   1.0                   1.0                     NaN   \n",
       "\n",
       "     chr22:41215421:G:GGT_GGT  chr22:42525952:C:A_C  chr22:42537023:T:C_T  \\\n",
       "0                         0.0                   1.0                   NaN   \n",
       "1                         NaN                   NaN                   NaN   \n",
       "2                         NaN                   NaN                   NaN   \n",
       "3                         NaN                   NaN                   NaN   \n",
       "4                         NaN                   0.0                   NaN   \n",
       "..                        ...                   ...                   ...   \n",
       "316                       0.0                   NaN                   NaN   \n",
       "317                       NaN                   NaN                   0.0   \n",
       "318                       NaN                   0.0                   NaN   \n",
       "319                       NaN                   NaN                   0.0   \n",
       "320                       NaN                   1.0                   NaN   \n",
       "\n",
       "     chr22:42538399:C:T_C  chr22:42539623:C:A_C  \n",
       "0                     NaN                   NaN  \n",
       "1                     NaN                   NaN  \n",
       "2                     NaN                   NaN  \n",
       "3                     NaN                   NaN  \n",
       "4                     0.0                   0.0  \n",
       "..                    ...                   ...  \n",
       "316                   NaN                   NaN  \n",
       "317                   NaN                   NaN  \n",
       "318                   0.0                   0.0  \n",
       "319                   NaN                   1.0  \n",
       "320                   0.0                   1.0  \n",
       "\n",
       "[321 rows x 281 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp = pd.read_csv('snp.csv').drop(['Unnamed: 0'],axis=1)\n",
    "snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "882e197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320]\n"
     ]
    }
   ],
   "source": [
    "idx_list = []\n",
    "for i in range(len(data)):\n",
    "  idx_list.append(i)\n",
    "\n",
    "print(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002131ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Control\n",
       "1      Control\n",
       "2      Control\n",
       "3      Control\n",
       "4      Control\n",
       "        ...   \n",
       "316    Control\n",
       "317    Control\n",
       "318    Control\n",
       "319    Control\n",
       "320    Control\n",
       "Name: trait, Length: 321, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = pd.read_csv(\"target.csv\")\n",
    "original['trait']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05999293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control: 0, MDD: 1, Bipolar: 2 \n",
    "\n",
    "target = []\n",
    "for i in range(321):\n",
    "  if original['trait'][i] == 'Control':\n",
    "    target.append(0)\n",
    "  elif original['trait'][i] == 'MDD':\n",
    "    target.append(1)\n",
    "  else:\n",
    "    target.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82298abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split (9 : 1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx_train, idx_test = train_test_split(idx_list, test_size = 0.1, shuffle = True, random_state = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eadec499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array(target)\n",
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27754f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([270, 281, 301,  86,  42, 168, 269, 315,  72, 275, 262,  73, 178,\n",
       "       254,   6,  94,   8, 111, 259,  10, 225, 112,  47, 248,  74, 291,\n",
       "       261,  43, 295,  20, 280, 185,  60, 277, 143,  29, 241, 317,  79,\n",
       "       258, 145,  41, 278, 109,  50, 222, 200, 105, 182,  39, 237, 108,\n",
       "       232, 205, 199,  44,  21, 209, 250, 228, 183, 318,  49,  30, 204,\n",
       "       276, 103, 191, 226, 224,  76, 244, 147,  92,   5, 305, 149, 106,\n",
       "       221, 210,  51, 187,  19, 267, 319,  34, 231, 286, 189,  16, 246,\n",
       "       265, 144, 193, 233, 190, 133, 264,  53, 161,  81,  98,  37, 268,\n",
       "       303,  83,  23, 159, 129,  54,  38, 213, 242, 107, 135, 279,  64,\n",
       "       123, 151,   7,  99, 195, 229,  17, 218,  11, 158, 206, 100,  59,\n",
       "         0,  40,  13, 175,  18, 288, 184, 140,  46,  55, 217, 141, 128,\n",
       "       155, 153, 116, 294,  45,  82,  93,  75, 142, 273,  15, 251, 134,\n",
       "       196, 238, 126, 115, 271, 298, 282, 101, 181, 192, 211,  96, 306,\n",
       "       114, 308, 113, 110, 131, 219, 121,  66, 312, 320, 252, 285, 289,\n",
       "       266, 255, 172, 138, 247,  48,  84, 313,  62, 156, 290,   9, 163,\n",
       "       139, 274, 174,  12, 223, 245, 260,  97, 176, 299, 164, 127, 216,\n",
       "        35, 239,  52,  80, 136,  90, 173, 272,   1, 169, 157, 235,  88,\n",
       "       284, 194, 150,  71,  95, 287, 220,  14,  57, 170, 304, 316, 314,\n",
       "       227,  85, 249, 300, 188,  61,  68,  28, 146,  27,  25, 214, 152,\n",
       "        56, 283, 293, 234, 198,  32, 215,  22,  33, 310, 179,   4, 162,\n",
       "       201, 118, 236,  67, 207, 120, 256, 311, 167,  63,  58, 124,  78,\n",
       "        70,  36, 263,  24, 122, 130, 307,  87, 102, 117,  89, 240,  65,\n",
       "       186, 160])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "171ec4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31,   2,  91, 125, 203, 132, 257, 119, 180, 177, 302, 292, 166,\n",
       "       296, 197, 230, 202, 243, 297,   3, 208, 165, 154,  69, 212, 104,\n",
       "        26, 148, 309, 253, 171, 137,  77])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ddd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "traindata_snp = []\n",
    "trainidx = []\n",
    "for i in range(len(idx_train)):\n",
    "  traindata.append(data[idx_train[i]])\n",
    "  traindata_snp.append(snp.loc[idx_train[i], :].values)\n",
    "  trainidx.append(target[idx_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcc818a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = []\n",
    "testdata_snp = []\n",
    "testidx = []\n",
    "\n",
    "for i in range(len(idx_test)):\n",
    "  testdata.append(data[idx_test[i]])\n",
    "  testdata_snp.append(snp.loc[idx_test[i], :].values)\n",
    "  testidx.append(target[idx_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0ea2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainidx = np.array(trainidx)\n",
    "testidx = np.array(testidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79525823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "model = timm.create_model('vit_large_patch32_384', pretrained=True).cuda()\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "num_features = model.head.in_features\n",
    "model.head = nn.Linear(num_features, 3)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4ae4687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(32, 32), stride=(32, 32))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (12): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (13): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (14): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (15): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (16): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (17): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (18): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (19): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (20): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (21): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (22): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (23): Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb4e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "270a0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfinal = []\n",
    "testfinal2 = []\n",
    "\n",
    "for i in range(len(testdata)):\n",
    "    testfinal.append((testdata[i], testidx[i]))\n",
    "    testfinal2.append((testdata_snp[i], testidx[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e60e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testfinal, batch_size = batchsize, shuffle = False)\n",
    "testloader2 = DataLoader(testfinal2, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b9352c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer, loss function, learning rate scheduler, epoch \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch : 0.95 ** epoch)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c1c4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84179c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "  model.train()\n",
    "  prob=[]\n",
    "  labellist=[]\n",
    "  for batch_idx, (image, label) in enumerate(train_loader):\n",
    "    image = image.to(device, dtype=torch.float)\n",
    "    label = label.to(device)\n",
    "    labellist.append(label)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(image)\n",
    "    prob.append(output)\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % log_interval == 0:\n",
    "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "      \n",
    "  scheduler.step()\n",
    "  return prob, labellist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88d9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device, dtype = torch.float)\n",
    "\n",
    "            label = label.to(device)\n",
    "            temp1.append(label)\n",
    "            output = model(image)\n",
    "            prob.append(output)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            temp2.append(prediction)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, temp1, temp2, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a7df6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    snpdata = []\n",
    "    snplabel = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            snpdata.append(image)\n",
    "            snplabel.append(label)\n",
    "    \n",
    "    return snpdata, snplabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b07f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kfold = 10\n",
    "skf = StratifiedKFold(n_splits = kfold, shuffle = True, random_state = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddeb1de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/342623223.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  traindata1 = np.array(traindata)\n",
      "/tmp/ipykernel_562499/342623223.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  traindata1 = np.array(traindata)\n"
     ]
    }
   ],
   "source": [
    "traindata1 = np.array(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd943146",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(traindata1) == len(traindata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0aa76866",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth':7, 'eta':0.1, 'objective':'multi:softprob', 'eval_metric':'mlogloss', 'early_stopping':100, 'num_class':3, 'n_estimators':1000}\n",
    "num_rounds = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f3410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bd123f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/10]\n",
      "[Fold 1/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 1.139391\n",
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.705568\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.024358, \tValidation Accuracy: 75.862069 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.578178\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.626871\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.024713, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.537964\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.612388\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.024227, \tValidation Accuracy: 65.517241 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.500290\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.618070\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.023018, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.493295\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.592262\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.023021, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.479698\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.578394\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.023133, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.470860\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.566989\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.022940, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.462803\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.559376\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.023053, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.454926\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.550252\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.023045, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.447726\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.541635\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.023038, \tValidation Accuracy: 68.965517 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.442390\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.533585\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.023065, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.436824\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.525016\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.023105, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.431999\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.517058\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.023130, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.427553\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.509224\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.023158, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.423305\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.502077\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.023191, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.419342\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.495310\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.023219, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.415642\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.488853\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.023251, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.412211\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.482791\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.023279, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.408976\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.477040\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.023311, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.405940\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.471647\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.023340, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.403082\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.466591\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.023368, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.400390\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.461842\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.023394, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.397853\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.457333\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.023422, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.395451\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.453110\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.023448, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.393198\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.449127\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.023473, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.391063\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.445377\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.023496, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.389057\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.441846\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.023518, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.387147\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.438508\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.023541, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.385334\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.435350\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.023563, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.383652\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.432411\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.023582, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.382026\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.429597\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.023603, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.380504\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.426954\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.023620, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.379051\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.424452\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.023638, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.377683\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.422109\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.023654, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.376385\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.419857\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.023670, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.375138\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.417775\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.023686, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.373961\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.415774\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.023700, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.372866\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.413892\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.023713, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.371812\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.412096\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.023727, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.370821\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.410423\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.023740, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.369857\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.408789\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.023753, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.368981\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.407320\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.023765, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.368114\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.405888\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.023776, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.367304\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.404529\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.023787, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.366551\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.403223\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.023795, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.365785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.401992\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.023805, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.365104\n",
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.400860\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.023815, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.364445\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.399779\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.023823, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.363816\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.398753\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.023831, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.363215\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.397761\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.023840, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.362658\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.396794\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.023847, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.362119\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.395888\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.023853, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.361592\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.395062\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.023860, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.361109\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.394301\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.023866, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.360647\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.393510\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.023874, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.360205\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.392813\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.023878, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.359783\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.392115\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.023883, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.359393\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.391470\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.023891, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.359013\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.390828\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.023895, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.358659\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.390259\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.023901, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.358303\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.389718\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.023904, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.357983\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.389176\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.023909, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.357687\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.388680\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.023913, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.357385\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.388209\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.023918, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.357113\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.387777\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.023922, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.356836\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.387333\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.023925, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.356601\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.386920\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.023928, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.356352\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.386544\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.023931, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.356121\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.386151\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.023935, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.355895\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.385806\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.023937, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.355699\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.385485\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.023940, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.355486\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.385141\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.023943, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.355314\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.384850\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.023943, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.355139\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.384555\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.023947, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.354978\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.384307\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.023950, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.354820\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.384038\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.023952, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.354648\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.383788\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.023954, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.354508\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.383540\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.023956, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.354359\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.383348\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.023957, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.354240\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.383126\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.023961, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.354120\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.382922\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.023961, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.353983\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.382764\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.023963, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.353880\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.382557\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.023964, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.353766\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.382371\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.023965, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.353661\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.382206\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.023968, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.353562\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.382072\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.023970, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.353484\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.381920\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.023971, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.353384\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.381776\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.023971, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.353311\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.381634\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.023973, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.353225\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.381512\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.023973, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.353147\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.381398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.023975, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.353074\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.381247\n",
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.023976, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.352998\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.381152\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.023977, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.352921\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.381050\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.023977, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.352873\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.380953\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.023980, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.352808\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.380851\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.023979, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.352765\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.380782\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.023981, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.352715\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.380707\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.023981, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.352676\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.380627\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.023982, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.352627\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.380532\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.023982, \tValidation Accuracy: 72.413793 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0298, \tTest Accuracy: 66.6667 % \n",
      "\n",
      "[16:29:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.00831\tvalid-mlogloss:1.08959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-mlogloss:0.93386\tvalid-mlogloss:1.08700\n",
      "[2]\ttrain-mlogloss:0.86323\tvalid-mlogloss:1.08503\n",
      "[3]\ttrain-mlogloss:0.79854\tvalid-mlogloss:1.09221\n",
      "[4]\ttrain-mlogloss:0.73903\tvalid-mlogloss:1.09437\n",
      "[5]\ttrain-mlogloss:0.68634\tvalid-mlogloss:1.11089\n",
      "[6]\ttrain-mlogloss:0.63845\tvalid-mlogloss:1.11436\n",
      "[7]\ttrain-mlogloss:0.59393\tvalid-mlogloss:1.12249\n",
      "[8]\ttrain-mlogloss:0.55531\tvalid-mlogloss:1.13385\n",
      "[9]\ttrain-mlogloss:0.52203\tvalid-mlogloss:1.12686\n",
      "[10]\ttrain-mlogloss:0.48934\tvalid-mlogloss:1.13176\n",
      "[11]\ttrain-mlogloss:0.45843\tvalid-mlogloss:1.14146\n",
      "[12]\ttrain-mlogloss:0.42985\tvalid-mlogloss:1.15692\n",
      "[13]\ttrain-mlogloss:0.40296\tvalid-mlogloss:1.16131\n",
      "[14]\ttrain-mlogloss:0.37760\tvalid-mlogloss:1.17558\n",
      "[15]\ttrain-mlogloss:0.35753\tvalid-mlogloss:1.17795\n",
      "[16]\ttrain-mlogloss:0.33698\tvalid-mlogloss:1.18762\n",
      "[17]\ttrain-mlogloss:0.31819\tvalid-mlogloss:1.20106\n",
      "[18]\ttrain-mlogloss:0.30087\tvalid-mlogloss:1.21114\n",
      "[19]\ttrain-mlogloss:0.28631\tvalid-mlogloss:1.21509\n",
      "[20]\ttrain-mlogloss:0.27092\tvalid-mlogloss:1.22218\n",
      "[21]\ttrain-mlogloss:0.25797\tvalid-mlogloss:1.22421\n",
      "[22]\ttrain-mlogloss:0.24588\tvalid-mlogloss:1.23024\n",
      "[23]\ttrain-mlogloss:0.23427\tvalid-mlogloss:1.24078\n",
      "[24]\ttrain-mlogloss:0.22341\tvalid-mlogloss:1.24889\n",
      "[25]\ttrain-mlogloss:0.21266\tvalid-mlogloss:1.25716\n",
      "[26]\ttrain-mlogloss:0.20228\tvalid-mlogloss:1.26740\n",
      "[27]\ttrain-mlogloss:0.19327\tvalid-mlogloss:1.27070\n",
      "[28]\ttrain-mlogloss:0.18408\tvalid-mlogloss:1.28802\n",
      "[29]\ttrain-mlogloss:0.17641\tvalid-mlogloss:1.28806\n",
      "[30]\ttrain-mlogloss:0.16900\tvalid-mlogloss:1.29775\n",
      "[31]\ttrain-mlogloss:0.16164\tvalid-mlogloss:1.28992\n",
      "[32]\ttrain-mlogloss:0.15443\tvalid-mlogloss:1.29356\n",
      "[33]\ttrain-mlogloss:0.14819\tvalid-mlogloss:1.29459\n",
      "[34]\ttrain-mlogloss:0.14251\tvalid-mlogloss:1.28945\n",
      "[35]\ttrain-mlogloss:0.13700\tvalid-mlogloss:1.30314\n",
      "[36]\ttrain-mlogloss:0.13150\tvalid-mlogloss:1.30350\n",
      "[37]\ttrain-mlogloss:0.12634\tvalid-mlogloss:1.31949\n",
      "[38]\ttrain-mlogloss:0.12174\tvalid-mlogloss:1.31482\n",
      "[39]\ttrain-mlogloss:0.11743\tvalid-mlogloss:1.30877\n",
      "[40]\ttrain-mlogloss:0.11345\tvalid-mlogloss:1.31453\n",
      "[41]\ttrain-mlogloss:0.10954\tvalid-mlogloss:1.32448\n",
      "[42]\ttrain-mlogloss:0.10582\tvalid-mlogloss:1.33920\n",
      "[43]\ttrain-mlogloss:0.10237\tvalid-mlogloss:1.33707\n",
      "[44]\ttrain-mlogloss:0.09910\tvalid-mlogloss:1.34414\n",
      "[45]\ttrain-mlogloss:0.09594\tvalid-mlogloss:1.34881\n",
      "[46]\ttrain-mlogloss:0.09280\tvalid-mlogloss:1.35133\n",
      "[47]\ttrain-mlogloss:0.09020\tvalid-mlogloss:1.36251\n",
      "[48]\ttrain-mlogloss:0.08749\tvalid-mlogloss:1.36376\n",
      "[49]\ttrain-mlogloss:0.08505\tvalid-mlogloss:1.36848\n",
      "[50]\ttrain-mlogloss:0.08270\tvalid-mlogloss:1.36719\n",
      "[51]\ttrain-mlogloss:0.08043\tvalid-mlogloss:1.37342\n",
      "[52]\ttrain-mlogloss:0.07848\tvalid-mlogloss:1.37426\n",
      "[53]\ttrain-mlogloss:0.07655\tvalid-mlogloss:1.38085\n",
      "[54]\ttrain-mlogloss:0.07461\tvalid-mlogloss:1.39116\n",
      "[55]\ttrain-mlogloss:0.07282\tvalid-mlogloss:1.39410\n",
      "[56]\ttrain-mlogloss:0.07113\tvalid-mlogloss:1.39191\n",
      "[57]\ttrain-mlogloss:0.06950\tvalid-mlogloss:1.39644\n",
      "[58]\ttrain-mlogloss:0.06789\tvalid-mlogloss:1.40327\n",
      "[59]\ttrain-mlogloss:0.06626\tvalid-mlogloss:1.41267\n",
      "[60]\ttrain-mlogloss:0.06485\tvalid-mlogloss:1.41470\n",
      "[61]\ttrain-mlogloss:0.06348\tvalid-mlogloss:1.41566\n",
      "[62]\ttrain-mlogloss:0.06215\tvalid-mlogloss:1.42345\n",
      "[63]\ttrain-mlogloss:0.06082\tvalid-mlogloss:1.42909\n",
      "[64]\ttrain-mlogloss:0.05965\tvalid-mlogloss:1.42491\n",
      "[65]\ttrain-mlogloss:0.05851\tvalid-mlogloss:1.42854\n",
      "[66]\ttrain-mlogloss:0.05740\tvalid-mlogloss:1.42570\n",
      "[67]\ttrain-mlogloss:0.05632\tvalid-mlogloss:1.42530\n",
      "[68]\ttrain-mlogloss:0.05525\tvalid-mlogloss:1.42423\n",
      "[69]\ttrain-mlogloss:0.05422\tvalid-mlogloss:1.42935\n",
      "[70]\ttrain-mlogloss:0.05326\tvalid-mlogloss:1.43372\n",
      "[71]\ttrain-mlogloss:0.05228\tvalid-mlogloss:1.43277\n",
      "[72]\ttrain-mlogloss:0.05140\tvalid-mlogloss:1.43164\n",
      "[73]\ttrain-mlogloss:0.05053\tvalid-mlogloss:1.43102\n",
      "[74]\ttrain-mlogloss:0.04964\tvalid-mlogloss:1.43041\n",
      "[75]\ttrain-mlogloss:0.04886\tvalid-mlogloss:1.42738\n",
      "[76]\ttrain-mlogloss:0.04813\tvalid-mlogloss:1.42851\n",
      "[77]\ttrain-mlogloss:0.04735\tvalid-mlogloss:1.43253\n",
      "[78]\ttrain-mlogloss:0.04660\tvalid-mlogloss:1.43276\n",
      "[79]\ttrain-mlogloss:0.04592\tvalid-mlogloss:1.43335\n",
      "[80]\ttrain-mlogloss:0.04529\tvalid-mlogloss:1.43222\n",
      "[81]\ttrain-mlogloss:0.04460\tvalid-mlogloss:1.43150\n",
      "[82]\ttrain-mlogloss:0.04400\tvalid-mlogloss:1.42965\n",
      "[83]\ttrain-mlogloss:0.04339\tvalid-mlogloss:1.43192\n",
      "[84]\ttrain-mlogloss:0.04277\tvalid-mlogloss:1.43462\n",
      "[85]\ttrain-mlogloss:0.04221\tvalid-mlogloss:1.43255\n",
      "[86]\ttrain-mlogloss:0.04162\tvalid-mlogloss:1.43492\n",
      "[87]\ttrain-mlogloss:0.04106\tvalid-mlogloss:1.43683\n",
      "[88]\ttrain-mlogloss:0.04054\tvalid-mlogloss:1.44041\n",
      "[89]\ttrain-mlogloss:0.04001\tvalid-mlogloss:1.44016\n",
      "[90]\ttrain-mlogloss:0.03953\tvalid-mlogloss:1.44062\n",
      "[91]\ttrain-mlogloss:0.03902\tvalid-mlogloss:1.44470\n",
      "[92]\ttrain-mlogloss:0.03858\tvalid-mlogloss:1.44364\n",
      "[93]\ttrain-mlogloss:0.03813\tvalid-mlogloss:1.44588\n",
      "[94]\ttrain-mlogloss:0.03765\tvalid-mlogloss:1.44990\n",
      "[95]\ttrain-mlogloss:0.03719\tvalid-mlogloss:1.45123\n",
      "[96]\ttrain-mlogloss:0.03680\tvalid-mlogloss:1.45232\n",
      "[97]\ttrain-mlogloss:0.03637\tvalid-mlogloss:1.45640\n",
      "[98]\ttrain-mlogloss:0.03596\tvalid-mlogloss:1.45856\n",
      "[99]\ttrain-mlogloss:0.03557\tvalid-mlogloss:1.45875\n",
      "[100]\ttrain-mlogloss:0.03521\tvalid-mlogloss:1.45748\n",
      "[101]\ttrain-mlogloss:0.03485\tvalid-mlogloss:1.46054\n",
      "[102]\ttrain-mlogloss:0.03449\tvalid-mlogloss:1.46141\n",
      "[103]\ttrain-mlogloss:0.03419\tvalid-mlogloss:1.46475\n",
      "[104]\ttrain-mlogloss:0.03382\tvalid-mlogloss:1.46682\n",
      "[105]\ttrain-mlogloss:0.03352\tvalid-mlogloss:1.47120\n",
      "[106]\ttrain-mlogloss:0.03319\tvalid-mlogloss:1.47022\n",
      "[107]\ttrain-mlogloss:0.03287\tvalid-mlogloss:1.47273\n",
      "[108]\ttrain-mlogloss:0.03254\tvalid-mlogloss:1.47596\n",
      "[109]\ttrain-mlogloss:0.03224\tvalid-mlogloss:1.47996\n",
      "[110]\ttrain-mlogloss:0.03193\tvalid-mlogloss:1.48188\n",
      "[111]\ttrain-mlogloss:0.03163\tvalid-mlogloss:1.48259\n",
      "[112]\ttrain-mlogloss:0.03134\tvalid-mlogloss:1.48720\n",
      "[113]\ttrain-mlogloss:0.03103\tvalid-mlogloss:1.48304\n",
      "[114]\ttrain-mlogloss:0.03076\tvalid-mlogloss:1.48570\n",
      "[115]\ttrain-mlogloss:0.03048\tvalid-mlogloss:1.48505\n",
      "[116]\ttrain-mlogloss:0.03018\tvalid-mlogloss:1.48830\n",
      "[117]\ttrain-mlogloss:0.02991\tvalid-mlogloss:1.49109\n",
      "[118]\ttrain-mlogloss:0.02964\tvalid-mlogloss:1.49229\n",
      "[119]\ttrain-mlogloss:0.02939\tvalid-mlogloss:1.49407\n",
      "[120]\ttrain-mlogloss:0.02914\tvalid-mlogloss:1.49817\n",
      "[121]\ttrain-mlogloss:0.02893\tvalid-mlogloss:1.49644\n",
      "[122]\ttrain-mlogloss:0.02868\tvalid-mlogloss:1.49388\n",
      "[123]\ttrain-mlogloss:0.02847\tvalid-mlogloss:1.49463\n",
      "[124]\ttrain-mlogloss:0.02824\tvalid-mlogloss:1.49568\n",
      "[125]\ttrain-mlogloss:0.02799\tvalid-mlogloss:1.49960\n",
      "[126]\ttrain-mlogloss:0.02776\tvalid-mlogloss:1.50307\n",
      "[127]\ttrain-mlogloss:0.02756\tvalid-mlogloss:1.50359\n",
      "[128]\ttrain-mlogloss:0.02736\tvalid-mlogloss:1.50414\n",
      "[129]\ttrain-mlogloss:0.02719\tvalid-mlogloss:1.50495\n",
      "[130]\ttrain-mlogloss:0.02699\tvalid-mlogloss:1.50696\n",
      "[131]\ttrain-mlogloss:0.02681\tvalid-mlogloss:1.50625\n",
      "[132]\ttrain-mlogloss:0.02662\tvalid-mlogloss:1.50538\n",
      "[133]\ttrain-mlogloss:0.02640\tvalid-mlogloss:1.50393\n",
      "[134]\ttrain-mlogloss:0.02625\tvalid-mlogloss:1.50439\n",
      "[135]\ttrain-mlogloss:0.02608\tvalid-mlogloss:1.50701\n",
      "[136]\ttrain-mlogloss:0.02589\tvalid-mlogloss:1.50921\n",
      "[137]\ttrain-mlogloss:0.02568\tvalid-mlogloss:1.51167\n",
      "[138]\ttrain-mlogloss:0.02551\tvalid-mlogloss:1.51961\n",
      "[139]\ttrain-mlogloss:0.02534\tvalid-mlogloss:1.52045\n",
      "[140]\ttrain-mlogloss:0.02517\tvalid-mlogloss:1.52234\n",
      "[141]\ttrain-mlogloss:0.02502\tvalid-mlogloss:1.52413\n",
      "[142]\ttrain-mlogloss:0.02485\tvalid-mlogloss:1.52785\n",
      "[143]\ttrain-mlogloss:0.02470\tvalid-mlogloss:1.52792\n",
      "[144]\ttrain-mlogloss:0.02452\tvalid-mlogloss:1.52944\n",
      "[145]\ttrain-mlogloss:0.02437\tvalid-mlogloss:1.53510\n",
      "[146]\ttrain-mlogloss:0.02423\tvalid-mlogloss:1.53803\n",
      "[147]\ttrain-mlogloss:0.02406\tvalid-mlogloss:1.54131\n",
      "[148]\ttrain-mlogloss:0.02391\tvalid-mlogloss:1.54215\n",
      "[149]\ttrain-mlogloss:0.02376\tvalid-mlogloss:1.54442\n",
      "[150]\ttrain-mlogloss:0.02362\tvalid-mlogloss:1.54689\n",
      "[151]\ttrain-mlogloss:0.02344\tvalid-mlogloss:1.55307\n",
      "[152]\ttrain-mlogloss:0.02334\tvalid-mlogloss:1.55314\n",
      "[153]\ttrain-mlogloss:0.02322\tvalid-mlogloss:1.55462\n",
      "[154]\ttrain-mlogloss:0.02306\tvalid-mlogloss:1.55849\n",
      "[155]\ttrain-mlogloss:0.02291\tvalid-mlogloss:1.55739\n",
      "[156]\ttrain-mlogloss:0.02277\tvalid-mlogloss:1.55903\n",
      "[157]\ttrain-mlogloss:0.02262\tvalid-mlogloss:1.56056\n",
      "[158]\ttrain-mlogloss:0.02249\tvalid-mlogloss:1.56202\n",
      "[159]\ttrain-mlogloss:0.02237\tvalid-mlogloss:1.56139\n",
      "[160]\ttrain-mlogloss:0.02225\tvalid-mlogloss:1.56429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:0.02214\tvalid-mlogloss:1.56356\n",
      "[162]\ttrain-mlogloss:0.02201\tvalid-mlogloss:1.56382\n",
      "[163]\ttrain-mlogloss:0.02189\tvalid-mlogloss:1.56775\n",
      "[164]\ttrain-mlogloss:0.02177\tvalid-mlogloss:1.56850\n",
      "[165]\ttrain-mlogloss:0.02164\tvalid-mlogloss:1.57435\n",
      "[166]\ttrain-mlogloss:0.02153\tvalid-mlogloss:1.57795\n",
      "[167]\ttrain-mlogloss:0.02142\tvalid-mlogloss:1.57596\n",
      "[168]\ttrain-mlogloss:0.02133\tvalid-mlogloss:1.57556\n",
      "[169]\ttrain-mlogloss:0.02122\tvalid-mlogloss:1.57419\n",
      "[170]\ttrain-mlogloss:0.02110\tvalid-mlogloss:1.57462\n",
      "[171]\ttrain-mlogloss:0.02101\tvalid-mlogloss:1.57699\n",
      "[172]\ttrain-mlogloss:0.02091\tvalid-mlogloss:1.57789\n",
      "[173]\ttrain-mlogloss:0.02082\tvalid-mlogloss:1.57880\n",
      "[174]\ttrain-mlogloss:0.02071\tvalid-mlogloss:1.58171\n",
      "[175]\ttrain-mlogloss:0.02063\tvalid-mlogloss:1.58269\n",
      "[176]\ttrain-mlogloss:0.02054\tvalid-mlogloss:1.58449\n",
      "[177]\ttrain-mlogloss:0.02044\tvalid-mlogloss:1.58537\n",
      "[178]\ttrain-mlogloss:0.02035\tvalid-mlogloss:1.58961\n",
      "[179]\ttrain-mlogloss:0.02024\tvalid-mlogloss:1.58996\n",
      "[180]\ttrain-mlogloss:0.02014\tvalid-mlogloss:1.59206\n",
      "[181]\ttrain-mlogloss:0.02005\tvalid-mlogloss:1.59170\n",
      "[182]\ttrain-mlogloss:0.01996\tvalid-mlogloss:1.59229\n",
      "[183]\ttrain-mlogloss:0.01988\tvalid-mlogloss:1.59390\n",
      "[184]\ttrain-mlogloss:0.01980\tvalid-mlogloss:1.59334\n",
      "[185]\ttrain-mlogloss:0.01970\tvalid-mlogloss:1.59342\n",
      "[186]\ttrain-mlogloss:0.01962\tvalid-mlogloss:1.59640\n",
      "[187]\ttrain-mlogloss:0.01951\tvalid-mlogloss:1.59672\n",
      "[188]\ttrain-mlogloss:0.01944\tvalid-mlogloss:1.59731\n",
      "[189]\ttrain-mlogloss:0.01937\tvalid-mlogloss:1.59578\n",
      "[190]\ttrain-mlogloss:0.01929\tvalid-mlogloss:1.59868\n",
      "[191]\ttrain-mlogloss:0.01921\tvalid-mlogloss:1.59974\n",
      "[192]\ttrain-mlogloss:0.01912\tvalid-mlogloss:1.60293\n",
      "[193]\ttrain-mlogloss:0.01905\tvalid-mlogloss:1.60404\n",
      "[194]\ttrain-mlogloss:0.01898\tvalid-mlogloss:1.60340\n",
      "[195]\ttrain-mlogloss:0.01890\tvalid-mlogloss:1.60477\n",
      "[196]\ttrain-mlogloss:0.01884\tvalid-mlogloss:1.60667\n",
      "[197]\ttrain-mlogloss:0.01876\tvalid-mlogloss:1.60736\n",
      "[198]\ttrain-mlogloss:0.01869\tvalid-mlogloss:1.61044\n",
      "[199]\ttrain-mlogloss:0.01861\tvalid-mlogloss:1.61194\n",
      "[200]\ttrain-mlogloss:0.01852\tvalid-mlogloss:1.61150\n",
      "[201]\ttrain-mlogloss:0.01845\tvalid-mlogloss:1.61199\n",
      "[202]\ttrain-mlogloss:0.01839\tvalid-mlogloss:1.61678\n",
      "[Fold 2/10]\n",
      "[Fold 2/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.399863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.484409\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.012971, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.399774\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.483800\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.012970, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.399622\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.483150\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.012970, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.399461\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.482411\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.012969, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.399289\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.481701\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.012968, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.399149\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.481085\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.012967, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.399016\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.480549\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.012968, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.398897\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.480031\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.012969, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.398785\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.479610\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.012969, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.398683\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.479222\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.012970, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.398589\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.478840\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.012971, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.398494\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.478535\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.012971, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.398408\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.478234\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.012972, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.398333\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.477991\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.012973, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.398262\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.477716\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.012973, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.398198\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.477471\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.012974, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.398126\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.477245\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.012976, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.398072\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.477077\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.012977, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.398012\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.476913\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.012978, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.397966\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.476782\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.012979, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.397917\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.476628\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.012979, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.397873\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.476440\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.012980, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.397821\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.476305\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.012981, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.397783\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.476183\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.012982, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.397755\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.476128\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.012983, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.397721\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.476010\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.012983, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.397692\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.475892\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.012984, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.397659\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.475797\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.012985, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.397626\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.475716\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.012986, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.397593\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.475635\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.012986, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.397567\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.475543\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.012987, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.397548\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.475483\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.012988, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.397524\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.475446\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.012988, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.397504\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.475373\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.012989, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.397478\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.475308\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.012989, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.397460\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.475259\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.012989, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.397437\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.475164\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.012989, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.397417\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.475125\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.012990, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.397401\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.475072\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.012991, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.397385\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.475028\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.012991, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.397366\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.474974\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.012991, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.397351\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.474942\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.012991, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.397343\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.474903\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.012992, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.397332\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.474881\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.012993, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.397313\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.474846\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.012993, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.397303\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.474816\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.012994, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.397279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.474781\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.012994, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.397272\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.474757\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.012994, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.397257\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.474741\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.012994, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.397249\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.474730\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.012995, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.397241\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.474716\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.012995, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.397230\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.474694\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.012995, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.397226\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.474679\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.012996, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.397219\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.474665\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.012996, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.397215\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.474655\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.012996, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.397208\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.474633\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.397201\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.474614\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.397198\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.474590\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.397193\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.474574\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.397186\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.474551\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.397181\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.474539\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.397176\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.474517\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.397173\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.474506\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.397166\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.474501\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.397160\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.474485\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.397155\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.474475\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.397151\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.474470\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.397148\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.474452\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.397146\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.474443\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.012997, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.397145\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.474436\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.397141\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.474425\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.397139\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.474430\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.397136\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.474426\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.397135\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.474425\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.397131\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.474423\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.397131\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.474417\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.397128\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.474410\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.397124\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.474403\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.397123\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.474397\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.397124\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.474393\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.397124\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.474381\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.012998, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.397121\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.474374\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.397121\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.474376\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.397118\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.474367\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.397116\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.474370\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.397115\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.474367\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.397115\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.474359\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.397114\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.474359\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.397111\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.474359\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.397111\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.474350\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.397111\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.474349\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.397112\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.474345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.397111\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.474340\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.397109\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.474337\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.397109\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.474330\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.397108\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.474326\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.397108\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.474324\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.397107\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.474318\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.397107\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.474319\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.397107\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.474314\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.012999, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:31:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01080\tvalid-mlogloss:1.07223\n",
      "[1]\ttrain-mlogloss:0.93348\tvalid-mlogloss:1.06426\n",
      "[2]\ttrain-mlogloss:0.86569\tvalid-mlogloss:1.04794\n",
      "[3]\ttrain-mlogloss:0.80721\tvalid-mlogloss:1.05514\n",
      "[4]\ttrain-mlogloss:0.74610\tvalid-mlogloss:1.05510\n",
      "[5]\ttrain-mlogloss:0.69043\tvalid-mlogloss:1.06458\n",
      "[6]\ttrain-mlogloss:0.64527\tvalid-mlogloss:1.07084\n",
      "[7]\ttrain-mlogloss:0.59978\tvalid-mlogloss:1.06878\n",
      "[8]\ttrain-mlogloss:0.56314\tvalid-mlogloss:1.07493\n",
      "[9]\ttrain-mlogloss:0.53029\tvalid-mlogloss:1.08554\n",
      "[10]\ttrain-mlogloss:0.49330\tvalid-mlogloss:1.09490\n",
      "[11]\ttrain-mlogloss:0.46476\tvalid-mlogloss:1.10269\n",
      "[12]\ttrain-mlogloss:0.43492\tvalid-mlogloss:1.10426\n",
      "[13]\ttrain-mlogloss:0.40701\tvalid-mlogloss:1.10966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\ttrain-mlogloss:0.38401\tvalid-mlogloss:1.11948\n",
      "[15]\ttrain-mlogloss:0.36267\tvalid-mlogloss:1.12767\n",
      "[16]\ttrain-mlogloss:0.34232\tvalid-mlogloss:1.13738\n",
      "[17]\ttrain-mlogloss:0.32332\tvalid-mlogloss:1.14489\n",
      "[18]\ttrain-mlogloss:0.30596\tvalid-mlogloss:1.15345\n",
      "[19]\ttrain-mlogloss:0.28918\tvalid-mlogloss:1.16054\n",
      "[20]\ttrain-mlogloss:0.27393\tvalid-mlogloss:1.16291\n",
      "[21]\ttrain-mlogloss:0.25904\tvalid-mlogloss:1.16861\n",
      "[22]\ttrain-mlogloss:0.24570\tvalid-mlogloss:1.17967\n",
      "[23]\ttrain-mlogloss:0.23414\tvalid-mlogloss:1.19261\n",
      "[24]\ttrain-mlogloss:0.22254\tvalid-mlogloss:1.19524\n",
      "[25]\ttrain-mlogloss:0.21202\tvalid-mlogloss:1.20046\n",
      "[26]\ttrain-mlogloss:0.20260\tvalid-mlogloss:1.19408\n",
      "[27]\ttrain-mlogloss:0.19304\tvalid-mlogloss:1.20592\n",
      "[28]\ttrain-mlogloss:0.18484\tvalid-mlogloss:1.21267\n",
      "[29]\ttrain-mlogloss:0.17663\tvalid-mlogloss:1.21972\n",
      "[30]\ttrain-mlogloss:0.16907\tvalid-mlogloss:1.22207\n",
      "[31]\ttrain-mlogloss:0.16237\tvalid-mlogloss:1.23234\n",
      "[32]\ttrain-mlogloss:0.15629\tvalid-mlogloss:1.24069\n",
      "[33]\ttrain-mlogloss:0.14960\tvalid-mlogloss:1.24617\n",
      "[34]\ttrain-mlogloss:0.14430\tvalid-mlogloss:1.25095\n",
      "[35]\ttrain-mlogloss:0.13865\tvalid-mlogloss:1.26573\n",
      "[36]\ttrain-mlogloss:0.13363\tvalid-mlogloss:1.27101\n",
      "[37]\ttrain-mlogloss:0.12871\tvalid-mlogloss:1.27428\n",
      "[38]\ttrain-mlogloss:0.12416\tvalid-mlogloss:1.28027\n",
      "[39]\ttrain-mlogloss:0.12005\tvalid-mlogloss:1.28265\n",
      "[40]\ttrain-mlogloss:0.11545\tvalid-mlogloss:1.28309\n",
      "[41]\ttrain-mlogloss:0.11186\tvalid-mlogloss:1.28934\n",
      "[42]\ttrain-mlogloss:0.10813\tvalid-mlogloss:1.28872\n",
      "[43]\ttrain-mlogloss:0.10439\tvalid-mlogloss:1.28834\n",
      "[44]\ttrain-mlogloss:0.10118\tvalid-mlogloss:1.29391\n",
      "[45]\ttrain-mlogloss:0.09791\tvalid-mlogloss:1.29102\n",
      "[46]\ttrain-mlogloss:0.09497\tvalid-mlogloss:1.29830\n",
      "[47]\ttrain-mlogloss:0.09243\tvalid-mlogloss:1.30478\n",
      "[48]\ttrain-mlogloss:0.08971\tvalid-mlogloss:1.30214\n",
      "[49]\ttrain-mlogloss:0.08723\tvalid-mlogloss:1.30617\n",
      "[50]\ttrain-mlogloss:0.08500\tvalid-mlogloss:1.31254\n",
      "[51]\ttrain-mlogloss:0.08275\tvalid-mlogloss:1.32048\n",
      "[52]\ttrain-mlogloss:0.08062\tvalid-mlogloss:1.32654\n",
      "[53]\ttrain-mlogloss:0.07840\tvalid-mlogloss:1.33002\n",
      "[54]\ttrain-mlogloss:0.07652\tvalid-mlogloss:1.33492\n",
      "[55]\ttrain-mlogloss:0.07460\tvalid-mlogloss:1.33801\n",
      "[56]\ttrain-mlogloss:0.07272\tvalid-mlogloss:1.34460\n",
      "[57]\ttrain-mlogloss:0.07116\tvalid-mlogloss:1.35039\n",
      "[58]\ttrain-mlogloss:0.06959\tvalid-mlogloss:1.35891\n",
      "[59]\ttrain-mlogloss:0.06801\tvalid-mlogloss:1.36455\n",
      "[60]\ttrain-mlogloss:0.06648\tvalid-mlogloss:1.36899\n",
      "[61]\ttrain-mlogloss:0.06503\tvalid-mlogloss:1.37378\n",
      "[62]\ttrain-mlogloss:0.06380\tvalid-mlogloss:1.37656\n",
      "[63]\ttrain-mlogloss:0.06241\tvalid-mlogloss:1.37764\n",
      "[64]\ttrain-mlogloss:0.06120\tvalid-mlogloss:1.38010\n",
      "[65]\ttrain-mlogloss:0.06002\tvalid-mlogloss:1.38677\n",
      "[66]\ttrain-mlogloss:0.05877\tvalid-mlogloss:1.39198\n",
      "[67]\ttrain-mlogloss:0.05768\tvalid-mlogloss:1.39704\n",
      "[68]\ttrain-mlogloss:0.05667\tvalid-mlogloss:1.39949\n",
      "[69]\ttrain-mlogloss:0.05575\tvalid-mlogloss:1.40224\n",
      "[70]\ttrain-mlogloss:0.05478\tvalid-mlogloss:1.40556\n",
      "[71]\ttrain-mlogloss:0.05379\tvalid-mlogloss:1.41444\n",
      "[72]\ttrain-mlogloss:0.05287\tvalid-mlogloss:1.42241\n",
      "[73]\ttrain-mlogloss:0.05192\tvalid-mlogloss:1.42661\n",
      "[74]\ttrain-mlogloss:0.05103\tvalid-mlogloss:1.43315\n",
      "[75]\ttrain-mlogloss:0.05014\tvalid-mlogloss:1.43659\n",
      "[76]\ttrain-mlogloss:0.04941\tvalid-mlogloss:1.43454\n",
      "[77]\ttrain-mlogloss:0.04854\tvalid-mlogloss:1.44286\n",
      "[78]\ttrain-mlogloss:0.04787\tvalid-mlogloss:1.44459\n",
      "[79]\ttrain-mlogloss:0.04717\tvalid-mlogloss:1.44563\n",
      "[80]\ttrain-mlogloss:0.04640\tvalid-mlogloss:1.44826\n",
      "[81]\ttrain-mlogloss:0.04567\tvalid-mlogloss:1.45246\n",
      "[82]\ttrain-mlogloss:0.04494\tvalid-mlogloss:1.45232\n",
      "[83]\ttrain-mlogloss:0.04430\tvalid-mlogloss:1.45238\n",
      "[84]\ttrain-mlogloss:0.04360\tvalid-mlogloss:1.45513\n",
      "[85]\ttrain-mlogloss:0.04287\tvalid-mlogloss:1.45983\n",
      "[86]\ttrain-mlogloss:0.04221\tvalid-mlogloss:1.46637\n",
      "[87]\ttrain-mlogloss:0.04165\tvalid-mlogloss:1.47415\n",
      "[88]\ttrain-mlogloss:0.04109\tvalid-mlogloss:1.47871\n",
      "[89]\ttrain-mlogloss:0.04054\tvalid-mlogloss:1.48749\n",
      "[90]\ttrain-mlogloss:0.03997\tvalid-mlogloss:1.48880\n",
      "[91]\ttrain-mlogloss:0.03950\tvalid-mlogloss:1.49371\n",
      "[92]\ttrain-mlogloss:0.03905\tvalid-mlogloss:1.49421\n",
      "[93]\ttrain-mlogloss:0.03855\tvalid-mlogloss:1.50023\n",
      "[94]\ttrain-mlogloss:0.03802\tvalid-mlogloss:1.50761\n",
      "[95]\ttrain-mlogloss:0.03754\tvalid-mlogloss:1.50647\n",
      "[96]\ttrain-mlogloss:0.03713\tvalid-mlogloss:1.51174\n",
      "[97]\ttrain-mlogloss:0.03665\tvalid-mlogloss:1.51580\n",
      "[98]\ttrain-mlogloss:0.03620\tvalid-mlogloss:1.51597\n",
      "[99]\ttrain-mlogloss:0.03584\tvalid-mlogloss:1.52042\n",
      "[100]\ttrain-mlogloss:0.03543\tvalid-mlogloss:1.52685\n",
      "[101]\ttrain-mlogloss:0.03506\tvalid-mlogloss:1.53256\n",
      "[102]\ttrain-mlogloss:0.03467\tvalid-mlogloss:1.53909\n",
      "[103]\ttrain-mlogloss:0.03432\tvalid-mlogloss:1.54269\n",
      "[104]\ttrain-mlogloss:0.03397\tvalid-mlogloss:1.54250\n",
      "[105]\ttrain-mlogloss:0.03362\tvalid-mlogloss:1.54528\n",
      "[106]\ttrain-mlogloss:0.03327\tvalid-mlogloss:1.55153\n",
      "[107]\ttrain-mlogloss:0.03294\tvalid-mlogloss:1.55384\n",
      "[108]\ttrain-mlogloss:0.03261\tvalid-mlogloss:1.55483\n",
      "[109]\ttrain-mlogloss:0.03228\tvalid-mlogloss:1.55602\n",
      "[110]\ttrain-mlogloss:0.03196\tvalid-mlogloss:1.55483\n",
      "[111]\ttrain-mlogloss:0.03168\tvalid-mlogloss:1.55879\n",
      "[112]\ttrain-mlogloss:0.03137\tvalid-mlogloss:1.56465\n",
      "[113]\ttrain-mlogloss:0.03110\tvalid-mlogloss:1.56625\n",
      "[114]\ttrain-mlogloss:0.03083\tvalid-mlogloss:1.56957\n",
      "[115]\ttrain-mlogloss:0.03056\tvalid-mlogloss:1.57356\n",
      "[116]\ttrain-mlogloss:0.03029\tvalid-mlogloss:1.57532\n",
      "[117]\ttrain-mlogloss:0.03000\tvalid-mlogloss:1.57723\n",
      "[118]\ttrain-mlogloss:0.02975\tvalid-mlogloss:1.57884\n",
      "[119]\ttrain-mlogloss:0.02949\tvalid-mlogloss:1.57942\n",
      "[120]\ttrain-mlogloss:0.02923\tvalid-mlogloss:1.58153\n",
      "[121]\ttrain-mlogloss:0.02899\tvalid-mlogloss:1.58410\n",
      "[122]\ttrain-mlogloss:0.02874\tvalid-mlogloss:1.58847\n",
      "[123]\ttrain-mlogloss:0.02850\tvalid-mlogloss:1.58920\n",
      "[124]\ttrain-mlogloss:0.02826\tvalid-mlogloss:1.59368\n",
      "[125]\ttrain-mlogloss:0.02801\tvalid-mlogloss:1.59693\n",
      "[126]\ttrain-mlogloss:0.02781\tvalid-mlogloss:1.59523\n",
      "[127]\ttrain-mlogloss:0.02758\tvalid-mlogloss:1.59800\n",
      "[128]\ttrain-mlogloss:0.02738\tvalid-mlogloss:1.60196\n",
      "[129]\ttrain-mlogloss:0.02715\tvalid-mlogloss:1.60588\n",
      "[130]\ttrain-mlogloss:0.02697\tvalid-mlogloss:1.60887\n",
      "[131]\ttrain-mlogloss:0.02678\tvalid-mlogloss:1.60873\n",
      "[132]\ttrain-mlogloss:0.02659\tvalid-mlogloss:1.61347\n",
      "[133]\ttrain-mlogloss:0.02641\tvalid-mlogloss:1.61501\n",
      "[134]\ttrain-mlogloss:0.02620\tvalid-mlogloss:1.62021\n",
      "[135]\ttrain-mlogloss:0.02599\tvalid-mlogloss:1.61949\n",
      "[136]\ttrain-mlogloss:0.02581\tvalid-mlogloss:1.62213\n",
      "[137]\ttrain-mlogloss:0.02565\tvalid-mlogloss:1.62362\n",
      "[138]\ttrain-mlogloss:0.02549\tvalid-mlogloss:1.62845\n",
      "[139]\ttrain-mlogloss:0.02531\tvalid-mlogloss:1.62818\n",
      "[140]\ttrain-mlogloss:0.02514\tvalid-mlogloss:1.63214\n",
      "[141]\ttrain-mlogloss:0.02500\tvalid-mlogloss:1.63316\n",
      "[142]\ttrain-mlogloss:0.02484\tvalid-mlogloss:1.63576\n",
      "[143]\ttrain-mlogloss:0.02470\tvalid-mlogloss:1.63931\n",
      "[144]\ttrain-mlogloss:0.02456\tvalid-mlogloss:1.64199\n",
      "[145]\ttrain-mlogloss:0.02441\tvalid-mlogloss:1.64008\n",
      "[146]\ttrain-mlogloss:0.02427\tvalid-mlogloss:1.64202\n",
      "[147]\ttrain-mlogloss:0.02409\tvalid-mlogloss:1.64489\n",
      "[148]\ttrain-mlogloss:0.02395\tvalid-mlogloss:1.64771\n",
      "[149]\ttrain-mlogloss:0.02381\tvalid-mlogloss:1.65035\n",
      "[150]\ttrain-mlogloss:0.02366\tvalid-mlogloss:1.65333\n",
      "[151]\ttrain-mlogloss:0.02352\tvalid-mlogloss:1.65709\n",
      "[152]\ttrain-mlogloss:0.02339\tvalid-mlogloss:1.65908\n",
      "[153]\ttrain-mlogloss:0.02327\tvalid-mlogloss:1.66345\n",
      "[154]\ttrain-mlogloss:0.02314\tvalid-mlogloss:1.66456\n",
      "[155]\ttrain-mlogloss:0.02303\tvalid-mlogloss:1.66537\n",
      "[156]\ttrain-mlogloss:0.02291\tvalid-mlogloss:1.66808\n",
      "[157]\ttrain-mlogloss:0.02275\tvalid-mlogloss:1.67129\n",
      "[158]\ttrain-mlogloss:0.02262\tvalid-mlogloss:1.67273\n",
      "[159]\ttrain-mlogloss:0.02250\tvalid-mlogloss:1.67512\n",
      "[160]\ttrain-mlogloss:0.02240\tvalid-mlogloss:1.67771\n",
      "[161]\ttrain-mlogloss:0.02227\tvalid-mlogloss:1.67962\n",
      "[162]\ttrain-mlogloss:0.02215\tvalid-mlogloss:1.67967\n",
      "[163]\ttrain-mlogloss:0.02205\tvalid-mlogloss:1.67929\n",
      "[164]\ttrain-mlogloss:0.02192\tvalid-mlogloss:1.67996\n",
      "[165]\ttrain-mlogloss:0.02182\tvalid-mlogloss:1.68083\n",
      "[166]\ttrain-mlogloss:0.02170\tvalid-mlogloss:1.68100\n",
      "[167]\ttrain-mlogloss:0.02160\tvalid-mlogloss:1.68150\n",
      "[168]\ttrain-mlogloss:0.02149\tvalid-mlogloss:1.68300\n",
      "[169]\ttrain-mlogloss:0.02138\tvalid-mlogloss:1.68386\n",
      "[170]\ttrain-mlogloss:0.02127\tvalid-mlogloss:1.68512\n",
      "[171]\ttrain-mlogloss:0.02118\tvalid-mlogloss:1.68618\n",
      "[172]\ttrain-mlogloss:0.02107\tvalid-mlogloss:1.68775\n",
      "[173]\ttrain-mlogloss:0.02097\tvalid-mlogloss:1.69013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174]\ttrain-mlogloss:0.02086\tvalid-mlogloss:1.69018\n",
      "[175]\ttrain-mlogloss:0.02076\tvalid-mlogloss:1.69142\n",
      "[176]\ttrain-mlogloss:0.02067\tvalid-mlogloss:1.69589\n",
      "[177]\ttrain-mlogloss:0.02058\tvalid-mlogloss:1.69617\n",
      "[178]\ttrain-mlogloss:0.02048\tvalid-mlogloss:1.69616\n",
      "[179]\ttrain-mlogloss:0.02041\tvalid-mlogloss:1.69606\n",
      "[180]\ttrain-mlogloss:0.02032\tvalid-mlogloss:1.69615\n",
      "[181]\ttrain-mlogloss:0.02022\tvalid-mlogloss:1.69616\n",
      "[182]\ttrain-mlogloss:0.02013\tvalid-mlogloss:1.69843\n",
      "[183]\ttrain-mlogloss:0.02004\tvalid-mlogloss:1.69630\n",
      "[184]\ttrain-mlogloss:0.01997\tvalid-mlogloss:1.69724\n",
      "[185]\ttrain-mlogloss:0.01987\tvalid-mlogloss:1.69860\n",
      "[186]\ttrain-mlogloss:0.01978\tvalid-mlogloss:1.69753\n",
      "[187]\ttrain-mlogloss:0.01970\tvalid-mlogloss:1.69954\n",
      "[188]\ttrain-mlogloss:0.01960\tvalid-mlogloss:1.70066\n",
      "[189]\ttrain-mlogloss:0.01952\tvalid-mlogloss:1.70205\n",
      "[190]\ttrain-mlogloss:0.01944\tvalid-mlogloss:1.70332\n",
      "[191]\ttrain-mlogloss:0.01934\tvalid-mlogloss:1.70343\n",
      "[192]\ttrain-mlogloss:0.01926\tvalid-mlogloss:1.70500\n",
      "[193]\ttrain-mlogloss:0.01919\tvalid-mlogloss:1.70720\n",
      "[194]\ttrain-mlogloss:0.01912\tvalid-mlogloss:1.70900\n",
      "[195]\ttrain-mlogloss:0.01905\tvalid-mlogloss:1.70928\n",
      "[196]\ttrain-mlogloss:0.01896\tvalid-mlogloss:1.71014\n",
      "[197]\ttrain-mlogloss:0.01889\tvalid-mlogloss:1.71149\n",
      "[198]\ttrain-mlogloss:0.01882\tvalid-mlogloss:1.71331\n",
      "[199]\ttrain-mlogloss:0.01873\tvalid-mlogloss:1.71457\n",
      "[200]\ttrain-mlogloss:0.01867\tvalid-mlogloss:1.71687\n",
      "[201]\ttrain-mlogloss:0.01859\tvalid-mlogloss:1.71974\n",
      "[Fold 3/10]\n",
      "[Fold 3/10 Prediciton:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.414863\n",
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.361869\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.013823, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.414861\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.361868\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.013823, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.414856\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.361867\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.013823, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.414848\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.361865\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.013823, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.414843\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.361868\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.414838\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.361868\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.414837\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.361869\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.414835\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.361869\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.414833\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.361868\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.414830\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.361868\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.414828\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.361869\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.414825\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.361871\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.013824, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.414820\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.361871\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.414819\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.361870\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.414818\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.361871\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.414815\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.361872\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.414814\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.361873\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.414799\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.361872\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.414796\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.361872\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.414796\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.361872\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.013825, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.414795\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.361871\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.414792\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.361871\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.414791\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.361872\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.414790\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.361872\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.414788\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.361870\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.414788\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.361873\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.414788\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.361874\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.414786\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.361874\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.414786\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.414783\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.414782\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.414782\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.414781\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.414781\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.414780\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.414779\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.414779\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.414779\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.414775\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.414775\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.414775\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.414775\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.414774\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.414774\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.414774\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.361874\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.414772\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.361875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.414772\n",
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.013826, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.414772\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.414771\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.414771\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.361874\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.361876\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.414770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.414770\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.361875\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.013827, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:33:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01343\tvalid-mlogloss:1.09091\n",
      "[1]\ttrain-mlogloss:0.93960\tvalid-mlogloss:1.08746\n",
      "[2]\ttrain-mlogloss:0.86791\tvalid-mlogloss:1.07265\n",
      "[3]\ttrain-mlogloss:0.80936\tvalid-mlogloss:1.06557\n",
      "[4]\ttrain-mlogloss:0.75341\tvalid-mlogloss:1.07004\n",
      "[5]\ttrain-mlogloss:0.70287\tvalid-mlogloss:1.06610\n",
      "[6]\ttrain-mlogloss:0.65556\tvalid-mlogloss:1.05988\n",
      "[7]\ttrain-mlogloss:0.60942\tvalid-mlogloss:1.04865\n",
      "[8]\ttrain-mlogloss:0.57173\tvalid-mlogloss:1.04756\n",
      "[9]\ttrain-mlogloss:0.53227\tvalid-mlogloss:1.04757\n",
      "[10]\ttrain-mlogloss:0.49846\tvalid-mlogloss:1.04922\n",
      "[11]\ttrain-mlogloss:0.46869\tvalid-mlogloss:1.04711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\ttrain-mlogloss:0.43838\tvalid-mlogloss:1.04956\n",
      "[13]\ttrain-mlogloss:0.40940\tvalid-mlogloss:1.04709\n",
      "[14]\ttrain-mlogloss:0.38587\tvalid-mlogloss:1.05104\n",
      "[15]\ttrain-mlogloss:0.36460\tvalid-mlogloss:1.05593\n",
      "[16]\ttrain-mlogloss:0.34313\tvalid-mlogloss:1.06354\n",
      "[17]\ttrain-mlogloss:0.32360\tvalid-mlogloss:1.07624\n",
      "[18]\ttrain-mlogloss:0.30417\tvalid-mlogloss:1.07664\n",
      "[19]\ttrain-mlogloss:0.28618\tvalid-mlogloss:1.07625\n",
      "[20]\ttrain-mlogloss:0.27016\tvalid-mlogloss:1.07647\n",
      "[21]\ttrain-mlogloss:0.25688\tvalid-mlogloss:1.07969\n",
      "[22]\ttrain-mlogloss:0.24393\tvalid-mlogloss:1.08516\n",
      "[23]\ttrain-mlogloss:0.23148\tvalid-mlogloss:1.08989\n",
      "[24]\ttrain-mlogloss:0.22055\tvalid-mlogloss:1.09656\n",
      "[25]\ttrain-mlogloss:0.21016\tvalid-mlogloss:1.09859\n",
      "[26]\ttrain-mlogloss:0.20056\tvalid-mlogloss:1.10030\n",
      "[27]\ttrain-mlogloss:0.19128\tvalid-mlogloss:1.10386\n",
      "[28]\ttrain-mlogloss:0.18256\tvalid-mlogloss:1.10597\n",
      "[29]\ttrain-mlogloss:0.17436\tvalid-mlogloss:1.11207\n",
      "[30]\ttrain-mlogloss:0.16722\tvalid-mlogloss:1.10962\n",
      "[31]\ttrain-mlogloss:0.16058\tvalid-mlogloss:1.11188\n",
      "[32]\ttrain-mlogloss:0.15362\tvalid-mlogloss:1.11246\n",
      "[33]\ttrain-mlogloss:0.14768\tvalid-mlogloss:1.11387\n",
      "[34]\ttrain-mlogloss:0.14190\tvalid-mlogloss:1.11277\n",
      "[35]\ttrain-mlogloss:0.13639\tvalid-mlogloss:1.10940\n",
      "[36]\ttrain-mlogloss:0.13114\tvalid-mlogloss:1.10507\n",
      "[37]\ttrain-mlogloss:0.12648\tvalid-mlogloss:1.10275\n",
      "[38]\ttrain-mlogloss:0.12190\tvalid-mlogloss:1.10100\n",
      "[39]\ttrain-mlogloss:0.11740\tvalid-mlogloss:1.10678\n",
      "[40]\ttrain-mlogloss:0.11299\tvalid-mlogloss:1.10304\n",
      "[41]\ttrain-mlogloss:0.10900\tvalid-mlogloss:1.09957\n",
      "[42]\ttrain-mlogloss:0.10530\tvalid-mlogloss:1.09524\n",
      "[43]\ttrain-mlogloss:0.10170\tvalid-mlogloss:1.09458\n",
      "[44]\ttrain-mlogloss:0.09843\tvalid-mlogloss:1.09316\n",
      "[45]\ttrain-mlogloss:0.09558\tvalid-mlogloss:1.09278\n",
      "[46]\ttrain-mlogloss:0.09273\tvalid-mlogloss:1.09874\n",
      "[47]\ttrain-mlogloss:0.09020\tvalid-mlogloss:1.10017\n",
      "[48]\ttrain-mlogloss:0.08760\tvalid-mlogloss:1.10535\n",
      "[49]\ttrain-mlogloss:0.08518\tvalid-mlogloss:1.10923\n",
      "[50]\ttrain-mlogloss:0.08297\tvalid-mlogloss:1.11141\n",
      "[51]\ttrain-mlogloss:0.08069\tvalid-mlogloss:1.10975\n",
      "[52]\ttrain-mlogloss:0.07861\tvalid-mlogloss:1.11104\n",
      "[53]\ttrain-mlogloss:0.07678\tvalid-mlogloss:1.11290\n",
      "[54]\ttrain-mlogloss:0.07492\tvalid-mlogloss:1.11094\n",
      "[55]\ttrain-mlogloss:0.07306\tvalid-mlogloss:1.11277\n",
      "[56]\ttrain-mlogloss:0.07125\tvalid-mlogloss:1.11199\n",
      "[57]\ttrain-mlogloss:0.06970\tvalid-mlogloss:1.11380\n",
      "[58]\ttrain-mlogloss:0.06814\tvalid-mlogloss:1.12276\n",
      "[59]\ttrain-mlogloss:0.06670\tvalid-mlogloss:1.12325\n",
      "[60]\ttrain-mlogloss:0.06524\tvalid-mlogloss:1.12411\n",
      "[61]\ttrain-mlogloss:0.06386\tvalid-mlogloss:1.12646\n",
      "[62]\ttrain-mlogloss:0.06266\tvalid-mlogloss:1.12866\n",
      "[63]\ttrain-mlogloss:0.06132\tvalid-mlogloss:1.12988\n",
      "[64]\ttrain-mlogloss:0.06023\tvalid-mlogloss:1.13185\n",
      "[65]\ttrain-mlogloss:0.05907\tvalid-mlogloss:1.13103\n",
      "[66]\ttrain-mlogloss:0.05799\tvalid-mlogloss:1.13236\n",
      "[67]\ttrain-mlogloss:0.05683\tvalid-mlogloss:1.13450\n",
      "[68]\ttrain-mlogloss:0.05586\tvalid-mlogloss:1.13233\n",
      "[69]\ttrain-mlogloss:0.05487\tvalid-mlogloss:1.13596\n",
      "[70]\ttrain-mlogloss:0.05388\tvalid-mlogloss:1.14011\n",
      "[71]\ttrain-mlogloss:0.05294\tvalid-mlogloss:1.14703\n",
      "[72]\ttrain-mlogloss:0.05192\tvalid-mlogloss:1.15117\n",
      "[73]\ttrain-mlogloss:0.05112\tvalid-mlogloss:1.14892\n",
      "[74]\ttrain-mlogloss:0.05026\tvalid-mlogloss:1.15107\n",
      "[75]\ttrain-mlogloss:0.04945\tvalid-mlogloss:1.14924\n",
      "[76]\ttrain-mlogloss:0.04868\tvalid-mlogloss:1.15077\n",
      "[77]\ttrain-mlogloss:0.04796\tvalid-mlogloss:1.15291\n",
      "[78]\ttrain-mlogloss:0.04725\tvalid-mlogloss:1.15542\n",
      "[79]\ttrain-mlogloss:0.04652\tvalid-mlogloss:1.15820\n",
      "[80]\ttrain-mlogloss:0.04585\tvalid-mlogloss:1.15876\n",
      "[81]\ttrain-mlogloss:0.04520\tvalid-mlogloss:1.16026\n",
      "[82]\ttrain-mlogloss:0.04455\tvalid-mlogloss:1.16274\n",
      "[83]\ttrain-mlogloss:0.04392\tvalid-mlogloss:1.16230\n",
      "[84]\ttrain-mlogloss:0.04337\tvalid-mlogloss:1.16404\n",
      "[85]\ttrain-mlogloss:0.04279\tvalid-mlogloss:1.17069\n",
      "[86]\ttrain-mlogloss:0.04220\tvalid-mlogloss:1.17299\n",
      "[87]\ttrain-mlogloss:0.04166\tvalid-mlogloss:1.17314\n",
      "[88]\ttrain-mlogloss:0.04110\tvalid-mlogloss:1.17342\n",
      "[89]\ttrain-mlogloss:0.04056\tvalid-mlogloss:1.17323\n",
      "[90]\ttrain-mlogloss:0.04005\tvalid-mlogloss:1.17398\n",
      "[91]\ttrain-mlogloss:0.03959\tvalid-mlogloss:1.17378\n",
      "[92]\ttrain-mlogloss:0.03907\tvalid-mlogloss:1.17617\n",
      "[93]\ttrain-mlogloss:0.03864\tvalid-mlogloss:1.17878\n",
      "[94]\ttrain-mlogloss:0.03818\tvalid-mlogloss:1.17836\n",
      "[95]\ttrain-mlogloss:0.03770\tvalid-mlogloss:1.18110\n",
      "[96]\ttrain-mlogloss:0.03725\tvalid-mlogloss:1.18255\n",
      "[97]\ttrain-mlogloss:0.03684\tvalid-mlogloss:1.18365\n",
      "[98]\ttrain-mlogloss:0.03638\tvalid-mlogloss:1.18691\n",
      "[99]\ttrain-mlogloss:0.03602\tvalid-mlogloss:1.18657\n",
      "[100]\ttrain-mlogloss:0.03564\tvalid-mlogloss:1.18663\n",
      "[101]\ttrain-mlogloss:0.03527\tvalid-mlogloss:1.18870\n",
      "[102]\ttrain-mlogloss:0.03491\tvalid-mlogloss:1.18737\n",
      "[103]\ttrain-mlogloss:0.03452\tvalid-mlogloss:1.18613\n",
      "[104]\ttrain-mlogloss:0.03409\tvalid-mlogloss:1.18681\n",
      "[105]\ttrain-mlogloss:0.03377\tvalid-mlogloss:1.18979\n",
      "[106]\ttrain-mlogloss:0.03341\tvalid-mlogloss:1.19283\n",
      "[107]\ttrain-mlogloss:0.03311\tvalid-mlogloss:1.19832\n",
      "[108]\ttrain-mlogloss:0.03279\tvalid-mlogloss:1.19807\n",
      "[109]\ttrain-mlogloss:0.03246\tvalid-mlogloss:1.20243\n",
      "[110]\ttrain-mlogloss:0.03218\tvalid-mlogloss:1.20428\n",
      "[111]\ttrain-mlogloss:0.03188\tvalid-mlogloss:1.20219\n",
      "[112]\ttrain-mlogloss:0.03162\tvalid-mlogloss:1.20588\n",
      "[113]\ttrain-mlogloss:0.03133\tvalid-mlogloss:1.20587\n",
      "[114]\ttrain-mlogloss:0.03101\tvalid-mlogloss:1.20873\n",
      "[115]\ttrain-mlogloss:0.03076\tvalid-mlogloss:1.21192\n",
      "[116]\ttrain-mlogloss:0.03047\tvalid-mlogloss:1.21245\n",
      "[117]\ttrain-mlogloss:0.03021\tvalid-mlogloss:1.21139\n",
      "[118]\ttrain-mlogloss:0.02994\tvalid-mlogloss:1.21400\n",
      "[119]\ttrain-mlogloss:0.02967\tvalid-mlogloss:1.21558\n",
      "[120]\ttrain-mlogloss:0.02942\tvalid-mlogloss:1.21522\n",
      "[121]\ttrain-mlogloss:0.02916\tvalid-mlogloss:1.21549\n",
      "[122]\ttrain-mlogloss:0.02894\tvalid-mlogloss:1.21570\n",
      "[123]\ttrain-mlogloss:0.02872\tvalid-mlogloss:1.21536\n",
      "[124]\ttrain-mlogloss:0.02851\tvalid-mlogloss:1.21970\n",
      "[125]\ttrain-mlogloss:0.02828\tvalid-mlogloss:1.22375\n",
      "[126]\ttrain-mlogloss:0.02806\tvalid-mlogloss:1.22499\n",
      "[127]\ttrain-mlogloss:0.02784\tvalid-mlogloss:1.22632\n",
      "[128]\ttrain-mlogloss:0.02764\tvalid-mlogloss:1.22729\n",
      "[129]\ttrain-mlogloss:0.02743\tvalid-mlogloss:1.22578\n",
      "[130]\ttrain-mlogloss:0.02724\tvalid-mlogloss:1.22685\n",
      "[131]\ttrain-mlogloss:0.02701\tvalid-mlogloss:1.22544\n",
      "[132]\ttrain-mlogloss:0.02677\tvalid-mlogloss:1.22565\n",
      "[133]\ttrain-mlogloss:0.02659\tvalid-mlogloss:1.22396\n",
      "[134]\ttrain-mlogloss:0.02641\tvalid-mlogloss:1.22554\n",
      "[135]\ttrain-mlogloss:0.02623\tvalid-mlogloss:1.22578\n",
      "[136]\ttrain-mlogloss:0.02606\tvalid-mlogloss:1.22642\n",
      "[137]\ttrain-mlogloss:0.02590\tvalid-mlogloss:1.22710\n",
      "[138]\ttrain-mlogloss:0.02573\tvalid-mlogloss:1.22857\n",
      "[139]\ttrain-mlogloss:0.02551\tvalid-mlogloss:1.22967\n",
      "[140]\ttrain-mlogloss:0.02532\tvalid-mlogloss:1.23034\n",
      "[141]\ttrain-mlogloss:0.02516\tvalid-mlogloss:1.23200\n",
      "[142]\ttrain-mlogloss:0.02501\tvalid-mlogloss:1.23243\n",
      "[143]\ttrain-mlogloss:0.02486\tvalid-mlogloss:1.22968\n",
      "[144]\ttrain-mlogloss:0.02471\tvalid-mlogloss:1.23208\n",
      "[145]\ttrain-mlogloss:0.02457\tvalid-mlogloss:1.23447\n",
      "[146]\ttrain-mlogloss:0.02443\tvalid-mlogloss:1.23231\n",
      "[147]\ttrain-mlogloss:0.02430\tvalid-mlogloss:1.23122\n",
      "[148]\ttrain-mlogloss:0.02414\tvalid-mlogloss:1.23354\n",
      "[149]\ttrain-mlogloss:0.02401\tvalid-mlogloss:1.23400\n",
      "[150]\ttrain-mlogloss:0.02387\tvalid-mlogloss:1.23820\n",
      "[151]\ttrain-mlogloss:0.02372\tvalid-mlogloss:1.23832\n",
      "[152]\ttrain-mlogloss:0.02358\tvalid-mlogloss:1.24011\n",
      "[153]\ttrain-mlogloss:0.02345\tvalid-mlogloss:1.24021\n",
      "[154]\ttrain-mlogloss:0.02333\tvalid-mlogloss:1.23908\n",
      "[155]\ttrain-mlogloss:0.02321\tvalid-mlogloss:1.24133\n",
      "[156]\ttrain-mlogloss:0.02307\tvalid-mlogloss:1.24232\n",
      "[157]\ttrain-mlogloss:0.02295\tvalid-mlogloss:1.24256\n",
      "[158]\ttrain-mlogloss:0.02280\tvalid-mlogloss:1.24171\n",
      "[159]\ttrain-mlogloss:0.02267\tvalid-mlogloss:1.24088\n",
      "[160]\ttrain-mlogloss:0.02256\tvalid-mlogloss:1.24428\n",
      "[161]\ttrain-mlogloss:0.02242\tvalid-mlogloss:1.24185\n",
      "[162]\ttrain-mlogloss:0.02231\tvalid-mlogloss:1.24140\n",
      "[163]\ttrain-mlogloss:0.02219\tvalid-mlogloss:1.24206\n",
      "[164]\ttrain-mlogloss:0.02209\tvalid-mlogloss:1.23977\n",
      "[165]\ttrain-mlogloss:0.02196\tvalid-mlogloss:1.24113\n",
      "[166]\ttrain-mlogloss:0.02184\tvalid-mlogloss:1.24355\n",
      "[167]\ttrain-mlogloss:0.02173\tvalid-mlogloss:1.24121\n",
      "[168]\ttrain-mlogloss:0.02162\tvalid-mlogloss:1.24112\n",
      "[169]\ttrain-mlogloss:0.02151\tvalid-mlogloss:1.23950\n",
      "[170]\ttrain-mlogloss:0.02142\tvalid-mlogloss:1.24165\n",
      "[171]\ttrain-mlogloss:0.02131\tvalid-mlogloss:1.24363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172]\ttrain-mlogloss:0.02121\tvalid-mlogloss:1.24181\n",
      "[173]\ttrain-mlogloss:0.02110\tvalid-mlogloss:1.24244\n",
      "[174]\ttrain-mlogloss:0.02102\tvalid-mlogloss:1.24550\n",
      "[175]\ttrain-mlogloss:0.02091\tvalid-mlogloss:1.24797\n",
      "[176]\ttrain-mlogloss:0.02080\tvalid-mlogloss:1.24754\n",
      "[177]\ttrain-mlogloss:0.02070\tvalid-mlogloss:1.24790\n",
      "[178]\ttrain-mlogloss:0.02061\tvalid-mlogloss:1.24722\n",
      "[179]\ttrain-mlogloss:0.02050\tvalid-mlogloss:1.24763\n",
      "[180]\ttrain-mlogloss:0.02040\tvalid-mlogloss:1.24830\n",
      "[181]\ttrain-mlogloss:0.02030\tvalid-mlogloss:1.25143\n",
      "[182]\ttrain-mlogloss:0.02021\tvalid-mlogloss:1.25068\n",
      "[183]\ttrain-mlogloss:0.02011\tvalid-mlogloss:1.25204\n",
      "[184]\ttrain-mlogloss:0.02001\tvalid-mlogloss:1.25594\n",
      "[185]\ttrain-mlogloss:0.01992\tvalid-mlogloss:1.25785\n",
      "[186]\ttrain-mlogloss:0.01984\tvalid-mlogloss:1.25831\n",
      "[187]\ttrain-mlogloss:0.01976\tvalid-mlogloss:1.25913\n",
      "[188]\ttrain-mlogloss:0.01967\tvalid-mlogloss:1.26119\n",
      "[189]\ttrain-mlogloss:0.01958\tvalid-mlogloss:1.26256\n",
      "[190]\ttrain-mlogloss:0.01950\tvalid-mlogloss:1.26233\n",
      "[191]\ttrain-mlogloss:0.01943\tvalid-mlogloss:1.26426\n",
      "[192]\ttrain-mlogloss:0.01935\tvalid-mlogloss:1.26853\n",
      "[193]\ttrain-mlogloss:0.01928\tvalid-mlogloss:1.26973\n",
      "[194]\ttrain-mlogloss:0.01920\tvalid-mlogloss:1.27043\n",
      "[195]\ttrain-mlogloss:0.01913\tvalid-mlogloss:1.27101\n",
      "[196]\ttrain-mlogloss:0.01905\tvalid-mlogloss:1.27361\n",
      "[197]\ttrain-mlogloss:0.01897\tvalid-mlogloss:1.27457\n",
      "[198]\ttrain-mlogloss:0.01889\tvalid-mlogloss:1.27488\n",
      "[199]\ttrain-mlogloss:0.01883\tvalid-mlogloss:1.27569\n",
      "[200]\ttrain-mlogloss:0.01876\tvalid-mlogloss:1.27653\n",
      "[201]\ttrain-mlogloss:0.01868\tvalid-mlogloss:1.27703\n",
      "[202]\ttrain-mlogloss:0.01861\tvalid-mlogloss:1.27761\n",
      "[203]\ttrain-mlogloss:0.01854\tvalid-mlogloss:1.27934\n",
      "[204]\ttrain-mlogloss:0.01847\tvalid-mlogloss:1.28079\n",
      "[205]\ttrain-mlogloss:0.01841\tvalid-mlogloss:1.27949\n",
      "[206]\ttrain-mlogloss:0.01833\tvalid-mlogloss:1.27907\n",
      "[207]\ttrain-mlogloss:0.01827\tvalid-mlogloss:1.27991\n",
      "[208]\ttrain-mlogloss:0.01821\tvalid-mlogloss:1.28122\n",
      "[209]\ttrain-mlogloss:0.01814\tvalid-mlogloss:1.28133\n",
      "[210]\ttrain-mlogloss:0.01806\tvalid-mlogloss:1.28194\n",
      "[211]\ttrain-mlogloss:0.01800\tvalid-mlogloss:1.28326\n",
      "[212]\ttrain-mlogloss:0.01793\tvalid-mlogloss:1.28516\n",
      "[213]\ttrain-mlogloss:0.01787\tvalid-mlogloss:1.28471\n",
      "[Fold 4/10]\n",
      "[Fold 4/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.406462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.406462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.443659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.406462\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.443659\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.008273, \tValidation Accuracy: 96.551724 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:35:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01648\tvalid-mlogloss:1.09289\n",
      "[1]\ttrain-mlogloss:0.94434\tvalid-mlogloss:1.08705\n",
      "[2]\ttrain-mlogloss:0.86839\tvalid-mlogloss:1.08973\n",
      "[3]\ttrain-mlogloss:0.80143\tvalid-mlogloss:1.07760\n",
      "[4]\ttrain-mlogloss:0.74211\tvalid-mlogloss:1.08105\n",
      "[5]\ttrain-mlogloss:0.69063\tvalid-mlogloss:1.07860\n",
      "[6]\ttrain-mlogloss:0.64534\tvalid-mlogloss:1.07340\n",
      "[7]\ttrain-mlogloss:0.59971\tvalid-mlogloss:1.07557\n",
      "[8]\ttrain-mlogloss:0.56012\tvalid-mlogloss:1.08102\n",
      "[9]\ttrain-mlogloss:0.52450\tvalid-mlogloss:1.08750\n",
      "[10]\ttrain-mlogloss:0.49264\tvalid-mlogloss:1.09273\n",
      "[11]\ttrain-mlogloss:0.46303\tvalid-mlogloss:1.09337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\ttrain-mlogloss:0.43739\tvalid-mlogloss:1.09286\n",
      "[13]\ttrain-mlogloss:0.41113\tvalid-mlogloss:1.09635\n",
      "[14]\ttrain-mlogloss:0.38777\tvalid-mlogloss:1.11361\n",
      "[15]\ttrain-mlogloss:0.36538\tvalid-mlogloss:1.11764\n",
      "[16]\ttrain-mlogloss:0.34521\tvalid-mlogloss:1.12241\n",
      "[17]\ttrain-mlogloss:0.32631\tvalid-mlogloss:1.12383\n",
      "[18]\ttrain-mlogloss:0.30963\tvalid-mlogloss:1.12239\n",
      "[19]\ttrain-mlogloss:0.29214\tvalid-mlogloss:1.13044\n",
      "[20]\ttrain-mlogloss:0.27706\tvalid-mlogloss:1.13203\n",
      "[21]\ttrain-mlogloss:0.26336\tvalid-mlogloss:1.14269\n",
      "[22]\ttrain-mlogloss:0.25040\tvalid-mlogloss:1.14040\n",
      "[23]\ttrain-mlogloss:0.23829\tvalid-mlogloss:1.15076\n",
      "[24]\ttrain-mlogloss:0.22608\tvalid-mlogloss:1.14787\n",
      "[25]\ttrain-mlogloss:0.21593\tvalid-mlogloss:1.15310\n",
      "[26]\ttrain-mlogloss:0.20562\tvalid-mlogloss:1.16065\n",
      "[27]\ttrain-mlogloss:0.19614\tvalid-mlogloss:1.16007\n",
      "[28]\ttrain-mlogloss:0.18724\tvalid-mlogloss:1.16434\n",
      "[29]\ttrain-mlogloss:0.17868\tvalid-mlogloss:1.16901\n",
      "[30]\ttrain-mlogloss:0.17150\tvalid-mlogloss:1.17039\n",
      "[31]\ttrain-mlogloss:0.16424\tvalid-mlogloss:1.17370\n",
      "[32]\ttrain-mlogloss:0.15735\tvalid-mlogloss:1.16369\n",
      "[33]\ttrain-mlogloss:0.15116\tvalid-mlogloss:1.16941\n",
      "[34]\ttrain-mlogloss:0.14480\tvalid-mlogloss:1.16959\n",
      "[35]\ttrain-mlogloss:0.13911\tvalid-mlogloss:1.16004\n",
      "[36]\ttrain-mlogloss:0.13386\tvalid-mlogloss:1.16420\n",
      "[37]\ttrain-mlogloss:0.12882\tvalid-mlogloss:1.16060\n",
      "[38]\ttrain-mlogloss:0.12416\tvalid-mlogloss:1.16367\n",
      "[39]\ttrain-mlogloss:0.11979\tvalid-mlogloss:1.16120\n",
      "[40]\ttrain-mlogloss:0.11612\tvalid-mlogloss:1.16234\n",
      "[41]\ttrain-mlogloss:0.11204\tvalid-mlogloss:1.16095\n",
      "[42]\ttrain-mlogloss:0.10804\tvalid-mlogloss:1.16549\n",
      "[43]\ttrain-mlogloss:0.10469\tvalid-mlogloss:1.16772\n",
      "[44]\ttrain-mlogloss:0.10150\tvalid-mlogloss:1.16859\n",
      "[45]\ttrain-mlogloss:0.09850\tvalid-mlogloss:1.17127\n",
      "[46]\ttrain-mlogloss:0.09557\tvalid-mlogloss:1.17516\n",
      "[47]\ttrain-mlogloss:0.09263\tvalid-mlogloss:1.17658\n",
      "[48]\ttrain-mlogloss:0.08989\tvalid-mlogloss:1.17991\n",
      "[49]\ttrain-mlogloss:0.08741\tvalid-mlogloss:1.18364\n",
      "[50]\ttrain-mlogloss:0.08516\tvalid-mlogloss:1.18601\n",
      "[51]\ttrain-mlogloss:0.08302\tvalid-mlogloss:1.18793\n",
      "[52]\ttrain-mlogloss:0.08091\tvalid-mlogloss:1.18886\n",
      "[53]\ttrain-mlogloss:0.07875\tvalid-mlogloss:1.18347\n",
      "[54]\ttrain-mlogloss:0.07677\tvalid-mlogloss:1.18830\n",
      "[55]\ttrain-mlogloss:0.07485\tvalid-mlogloss:1.19191\n",
      "[56]\ttrain-mlogloss:0.07296\tvalid-mlogloss:1.19265\n",
      "[57]\ttrain-mlogloss:0.07127\tvalid-mlogloss:1.19316\n",
      "[58]\ttrain-mlogloss:0.06964\tvalid-mlogloss:1.19602\n",
      "[59]\ttrain-mlogloss:0.06823\tvalid-mlogloss:1.19541\n",
      "[60]\ttrain-mlogloss:0.06672\tvalid-mlogloss:1.19764\n",
      "[61]\ttrain-mlogloss:0.06518\tvalid-mlogloss:1.19462\n",
      "[62]\ttrain-mlogloss:0.06373\tvalid-mlogloss:1.19640\n",
      "[63]\ttrain-mlogloss:0.06244\tvalid-mlogloss:1.19726\n",
      "[64]\ttrain-mlogloss:0.06113\tvalid-mlogloss:1.20110\n",
      "[65]\ttrain-mlogloss:0.05989\tvalid-mlogloss:1.20493\n",
      "[66]\ttrain-mlogloss:0.05869\tvalid-mlogloss:1.21451\n",
      "[67]\ttrain-mlogloss:0.05745\tvalid-mlogloss:1.21639\n",
      "[68]\ttrain-mlogloss:0.05633\tvalid-mlogloss:1.21680\n",
      "[69]\ttrain-mlogloss:0.05538\tvalid-mlogloss:1.22165\n",
      "[70]\ttrain-mlogloss:0.05433\tvalid-mlogloss:1.22627\n",
      "[71]\ttrain-mlogloss:0.05337\tvalid-mlogloss:1.22676\n",
      "[72]\ttrain-mlogloss:0.05244\tvalid-mlogloss:1.23303\n",
      "[73]\ttrain-mlogloss:0.05169\tvalid-mlogloss:1.23182\n",
      "[74]\ttrain-mlogloss:0.05084\tvalid-mlogloss:1.23281\n",
      "[75]\ttrain-mlogloss:0.04999\tvalid-mlogloss:1.23510\n",
      "[76]\ttrain-mlogloss:0.04929\tvalid-mlogloss:1.23499\n",
      "[77]\ttrain-mlogloss:0.04850\tvalid-mlogloss:1.23272\n",
      "[78]\ttrain-mlogloss:0.04777\tvalid-mlogloss:1.23735\n",
      "[79]\ttrain-mlogloss:0.04705\tvalid-mlogloss:1.23822\n",
      "[80]\ttrain-mlogloss:0.04635\tvalid-mlogloss:1.23997\n",
      "[81]\ttrain-mlogloss:0.04567\tvalid-mlogloss:1.24002\n",
      "[82]\ttrain-mlogloss:0.04491\tvalid-mlogloss:1.23982\n",
      "[83]\ttrain-mlogloss:0.04425\tvalid-mlogloss:1.23837\n",
      "[84]\ttrain-mlogloss:0.04359\tvalid-mlogloss:1.24308\n",
      "[85]\ttrain-mlogloss:0.04299\tvalid-mlogloss:1.24726\n",
      "[86]\ttrain-mlogloss:0.04243\tvalid-mlogloss:1.24913\n",
      "[87]\ttrain-mlogloss:0.04187\tvalid-mlogloss:1.24588\n",
      "[88]\ttrain-mlogloss:0.04132\tvalid-mlogloss:1.24851\n",
      "[89]\ttrain-mlogloss:0.04077\tvalid-mlogloss:1.24846\n",
      "[90]\ttrain-mlogloss:0.04024\tvalid-mlogloss:1.25087\n",
      "[91]\ttrain-mlogloss:0.03978\tvalid-mlogloss:1.25478\n",
      "[92]\ttrain-mlogloss:0.03929\tvalid-mlogloss:1.25735\n",
      "[93]\ttrain-mlogloss:0.03884\tvalid-mlogloss:1.25500\n",
      "[94]\ttrain-mlogloss:0.03839\tvalid-mlogloss:1.25508\n",
      "[95]\ttrain-mlogloss:0.03797\tvalid-mlogloss:1.25689\n",
      "[96]\ttrain-mlogloss:0.03755\tvalid-mlogloss:1.25924\n",
      "[97]\ttrain-mlogloss:0.03718\tvalid-mlogloss:1.25763\n",
      "[98]\ttrain-mlogloss:0.03678\tvalid-mlogloss:1.26013\n",
      "[99]\ttrain-mlogloss:0.03638\tvalid-mlogloss:1.26227\n",
      "[100]\ttrain-mlogloss:0.03602\tvalid-mlogloss:1.26346\n",
      "[101]\ttrain-mlogloss:0.03564\tvalid-mlogloss:1.26418\n",
      "[102]\ttrain-mlogloss:0.03521\tvalid-mlogloss:1.26518\n",
      "[103]\ttrain-mlogloss:0.03488\tvalid-mlogloss:1.26290\n",
      "[104]\ttrain-mlogloss:0.03449\tvalid-mlogloss:1.26450\n",
      "[105]\ttrain-mlogloss:0.03417\tvalid-mlogloss:1.26439\n",
      "[106]\ttrain-mlogloss:0.03383\tvalid-mlogloss:1.26554\n",
      "[107]\ttrain-mlogloss:0.03351\tvalid-mlogloss:1.26757\n",
      "[108]\ttrain-mlogloss:0.03315\tvalid-mlogloss:1.26715\n",
      "[109]\ttrain-mlogloss:0.03287\tvalid-mlogloss:1.26548\n",
      "[110]\ttrain-mlogloss:0.03257\tvalid-mlogloss:1.26773\n",
      "[111]\ttrain-mlogloss:0.03227\tvalid-mlogloss:1.27206\n",
      "[112]\ttrain-mlogloss:0.03199\tvalid-mlogloss:1.27476\n",
      "[113]\ttrain-mlogloss:0.03172\tvalid-mlogloss:1.27400\n",
      "[114]\ttrain-mlogloss:0.03141\tvalid-mlogloss:1.27484\n",
      "[115]\ttrain-mlogloss:0.03112\tvalid-mlogloss:1.27265\n",
      "[116]\ttrain-mlogloss:0.03086\tvalid-mlogloss:1.27523\n",
      "[117]\ttrain-mlogloss:0.03057\tvalid-mlogloss:1.27858\n",
      "[118]\ttrain-mlogloss:0.03033\tvalid-mlogloss:1.27815\n",
      "[119]\ttrain-mlogloss:0.03008\tvalid-mlogloss:1.27936\n",
      "[120]\ttrain-mlogloss:0.02984\tvalid-mlogloss:1.27818\n",
      "[121]\ttrain-mlogloss:0.02958\tvalid-mlogloss:1.27770\n",
      "[122]\ttrain-mlogloss:0.02934\tvalid-mlogloss:1.27803\n",
      "[123]\ttrain-mlogloss:0.02910\tvalid-mlogloss:1.27694\n",
      "[124]\ttrain-mlogloss:0.02885\tvalid-mlogloss:1.27661\n",
      "[125]\ttrain-mlogloss:0.02863\tvalid-mlogloss:1.27697\n",
      "[126]\ttrain-mlogloss:0.02838\tvalid-mlogloss:1.27609\n",
      "[127]\ttrain-mlogloss:0.02818\tvalid-mlogloss:1.27674\n",
      "[128]\ttrain-mlogloss:0.02798\tvalid-mlogloss:1.27882\n",
      "[129]\ttrain-mlogloss:0.02777\tvalid-mlogloss:1.27744\n",
      "[130]\ttrain-mlogloss:0.02758\tvalid-mlogloss:1.27749\n",
      "[131]\ttrain-mlogloss:0.02740\tvalid-mlogloss:1.27881\n",
      "[132]\ttrain-mlogloss:0.02719\tvalid-mlogloss:1.27980\n",
      "[133]\ttrain-mlogloss:0.02701\tvalid-mlogloss:1.27971\n",
      "[134]\ttrain-mlogloss:0.02682\tvalid-mlogloss:1.28077\n",
      "[135]\ttrain-mlogloss:0.02664\tvalid-mlogloss:1.28215\n",
      "[136]\ttrain-mlogloss:0.02647\tvalid-mlogloss:1.28299\n",
      "[137]\ttrain-mlogloss:0.02629\tvalid-mlogloss:1.28308\n",
      "[138]\ttrain-mlogloss:0.02610\tvalid-mlogloss:1.28118\n",
      "[139]\ttrain-mlogloss:0.02593\tvalid-mlogloss:1.28135\n",
      "[140]\ttrain-mlogloss:0.02576\tvalid-mlogloss:1.28327\n",
      "[141]\ttrain-mlogloss:0.02557\tvalid-mlogloss:1.28197\n",
      "[142]\ttrain-mlogloss:0.02541\tvalid-mlogloss:1.27817\n",
      "[143]\ttrain-mlogloss:0.02525\tvalid-mlogloss:1.27826\n",
      "[144]\ttrain-mlogloss:0.02508\tvalid-mlogloss:1.27752\n",
      "[145]\ttrain-mlogloss:0.02491\tvalid-mlogloss:1.27869\n",
      "[146]\ttrain-mlogloss:0.02475\tvalid-mlogloss:1.27882\n",
      "[147]\ttrain-mlogloss:0.02459\tvalid-mlogloss:1.27653\n",
      "[148]\ttrain-mlogloss:0.02444\tvalid-mlogloss:1.27704\n",
      "[149]\ttrain-mlogloss:0.02430\tvalid-mlogloss:1.27739\n",
      "[150]\ttrain-mlogloss:0.02414\tvalid-mlogloss:1.27557\n",
      "[151]\ttrain-mlogloss:0.02400\tvalid-mlogloss:1.27606\n",
      "[152]\ttrain-mlogloss:0.02385\tvalid-mlogloss:1.27426\n",
      "[153]\ttrain-mlogloss:0.02371\tvalid-mlogloss:1.27539\n",
      "[154]\ttrain-mlogloss:0.02358\tvalid-mlogloss:1.27508\n",
      "[155]\ttrain-mlogloss:0.02346\tvalid-mlogloss:1.27311\n",
      "[156]\ttrain-mlogloss:0.02332\tvalid-mlogloss:1.27513\n",
      "[157]\ttrain-mlogloss:0.02318\tvalid-mlogloss:1.27453\n",
      "[158]\ttrain-mlogloss:0.02306\tvalid-mlogloss:1.27708\n",
      "[159]\ttrain-mlogloss:0.02294\tvalid-mlogloss:1.27827\n",
      "[160]\ttrain-mlogloss:0.02282\tvalid-mlogloss:1.28061\n",
      "[161]\ttrain-mlogloss:0.02269\tvalid-mlogloss:1.27886\n",
      "[162]\ttrain-mlogloss:0.02253\tvalid-mlogloss:1.27963\n",
      "[163]\ttrain-mlogloss:0.02241\tvalid-mlogloss:1.28055\n",
      "[164]\ttrain-mlogloss:0.02230\tvalid-mlogloss:1.28247\n",
      "[165]\ttrain-mlogloss:0.02219\tvalid-mlogloss:1.28267\n",
      "[166]\ttrain-mlogloss:0.02208\tvalid-mlogloss:1.28436\n",
      "[167]\ttrain-mlogloss:0.02196\tvalid-mlogloss:1.28551\n",
      "[168]\ttrain-mlogloss:0.02185\tvalid-mlogloss:1.28624\n",
      "[169]\ttrain-mlogloss:0.02174\tvalid-mlogloss:1.28663\n",
      "[170]\ttrain-mlogloss:0.02163\tvalid-mlogloss:1.28601\n",
      "[171]\ttrain-mlogloss:0.02151\tvalid-mlogloss:1.28463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172]\ttrain-mlogloss:0.02140\tvalid-mlogloss:1.28429\n",
      "[173]\ttrain-mlogloss:0.02131\tvalid-mlogloss:1.28610\n",
      "[174]\ttrain-mlogloss:0.02121\tvalid-mlogloss:1.28447\n",
      "[175]\ttrain-mlogloss:0.02112\tvalid-mlogloss:1.28531\n",
      "[176]\ttrain-mlogloss:0.02102\tvalid-mlogloss:1.28488\n",
      "[177]\ttrain-mlogloss:0.02094\tvalid-mlogloss:1.28493\n",
      "[178]\ttrain-mlogloss:0.02085\tvalid-mlogloss:1.28651\n",
      "[179]\ttrain-mlogloss:0.02076\tvalid-mlogloss:1.28457\n",
      "[180]\ttrain-mlogloss:0.02067\tvalid-mlogloss:1.28459\n",
      "[181]\ttrain-mlogloss:0.02059\tvalid-mlogloss:1.28292\n",
      "[182]\ttrain-mlogloss:0.02049\tvalid-mlogloss:1.28273\n",
      "[183]\ttrain-mlogloss:0.02040\tvalid-mlogloss:1.28199\n",
      "[184]\ttrain-mlogloss:0.02030\tvalid-mlogloss:1.28130\n",
      "[185]\ttrain-mlogloss:0.02021\tvalid-mlogloss:1.28218\n",
      "[186]\ttrain-mlogloss:0.02012\tvalid-mlogloss:1.28229\n",
      "[187]\ttrain-mlogloss:0.02004\tvalid-mlogloss:1.28206\n",
      "[188]\ttrain-mlogloss:0.01996\tvalid-mlogloss:1.28182\n",
      "[189]\ttrain-mlogloss:0.01987\tvalid-mlogloss:1.28414\n",
      "[190]\ttrain-mlogloss:0.01978\tvalid-mlogloss:1.28410\n",
      "[191]\ttrain-mlogloss:0.01970\tvalid-mlogloss:1.28512\n",
      "[192]\ttrain-mlogloss:0.01962\tvalid-mlogloss:1.28691\n",
      "[193]\ttrain-mlogloss:0.01951\tvalid-mlogloss:1.28579\n",
      "[194]\ttrain-mlogloss:0.01943\tvalid-mlogloss:1.28687\n",
      "[195]\ttrain-mlogloss:0.01936\tvalid-mlogloss:1.28796\n",
      "[196]\ttrain-mlogloss:0.01927\tvalid-mlogloss:1.28757\n",
      "[197]\ttrain-mlogloss:0.01919\tvalid-mlogloss:1.28771\n",
      "[198]\ttrain-mlogloss:0.01910\tvalid-mlogloss:1.28783\n",
      "[199]\ttrain-mlogloss:0.01903\tvalid-mlogloss:1.28703\n",
      "[200]\ttrain-mlogloss:0.01895\tvalid-mlogloss:1.28475\n",
      "[201]\ttrain-mlogloss:0.01885\tvalid-mlogloss:1.28419\n",
      "[202]\ttrain-mlogloss:0.01878\tvalid-mlogloss:1.28563\n",
      "[203]\ttrain-mlogloss:0.01871\tvalid-mlogloss:1.28647\n",
      "[204]\ttrain-mlogloss:0.01865\tvalid-mlogloss:1.28664\n",
      "[205]\ttrain-mlogloss:0.01859\tvalid-mlogloss:1.28579\n",
      "[206]\ttrain-mlogloss:0.01852\tvalid-mlogloss:1.28882\n",
      "[Fold 5/10]\n",
      "[Fold 5/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.397101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.397101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.467242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.397101\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.467242\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.009435, \tValidation Accuracy: 93.103448 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:37:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01382\tvalid-mlogloss:1.10473\n",
      "[1]\ttrain-mlogloss:0.93549\tvalid-mlogloss:1.09130\n",
      "[2]\ttrain-mlogloss:0.86545\tvalid-mlogloss:1.09285\n",
      "[3]\ttrain-mlogloss:0.80224\tvalid-mlogloss:1.09129\n",
      "[4]\ttrain-mlogloss:0.74440\tvalid-mlogloss:1.08849\n",
      "[5]\ttrain-mlogloss:0.69137\tvalid-mlogloss:1.08369\n",
      "[6]\ttrain-mlogloss:0.64288\tvalid-mlogloss:1.07886\n",
      "[7]\ttrain-mlogloss:0.59956\tvalid-mlogloss:1.07580\n",
      "[8]\ttrain-mlogloss:0.56018\tvalid-mlogloss:1.08731\n",
      "[9]\ttrain-mlogloss:0.52452\tvalid-mlogloss:1.09474\n",
      "[10]\ttrain-mlogloss:0.49158\tvalid-mlogloss:1.09733\n",
      "[11]\ttrain-mlogloss:0.46343\tvalid-mlogloss:1.09845\n",
      "[12]\ttrain-mlogloss:0.43639\tvalid-mlogloss:1.09789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\ttrain-mlogloss:0.41031\tvalid-mlogloss:1.11058\n",
      "[14]\ttrain-mlogloss:0.38663\tvalid-mlogloss:1.13146\n",
      "[15]\ttrain-mlogloss:0.36465\tvalid-mlogloss:1.13159\n",
      "[16]\ttrain-mlogloss:0.34492\tvalid-mlogloss:1.14161\n",
      "[17]\ttrain-mlogloss:0.32538\tvalid-mlogloss:1.14485\n",
      "[18]\ttrain-mlogloss:0.30663\tvalid-mlogloss:1.15423\n",
      "[19]\ttrain-mlogloss:0.28978\tvalid-mlogloss:1.15638\n",
      "[20]\ttrain-mlogloss:0.27411\tvalid-mlogloss:1.15096\n",
      "[21]\ttrain-mlogloss:0.26104\tvalid-mlogloss:1.15756\n",
      "[22]\ttrain-mlogloss:0.24820\tvalid-mlogloss:1.16762\n",
      "[23]\ttrain-mlogloss:0.23641\tvalid-mlogloss:1.17971\n",
      "[24]\ttrain-mlogloss:0.22568\tvalid-mlogloss:1.17127\n",
      "[25]\ttrain-mlogloss:0.21551\tvalid-mlogloss:1.17008\n",
      "[26]\ttrain-mlogloss:0.20638\tvalid-mlogloss:1.17185\n",
      "[27]\ttrain-mlogloss:0.19726\tvalid-mlogloss:1.17730\n",
      "[28]\ttrain-mlogloss:0.18896\tvalid-mlogloss:1.17511\n",
      "[29]\ttrain-mlogloss:0.18152\tvalid-mlogloss:1.17478\n",
      "[30]\ttrain-mlogloss:0.17397\tvalid-mlogloss:1.17752\n",
      "[31]\ttrain-mlogloss:0.16654\tvalid-mlogloss:1.17677\n",
      "[32]\ttrain-mlogloss:0.16004\tvalid-mlogloss:1.17471\n",
      "[33]\ttrain-mlogloss:0.15366\tvalid-mlogloss:1.17981\n",
      "[34]\ttrain-mlogloss:0.14789\tvalid-mlogloss:1.17765\n",
      "[35]\ttrain-mlogloss:0.14256\tvalid-mlogloss:1.17962\n",
      "[36]\ttrain-mlogloss:0.13717\tvalid-mlogloss:1.18418\n",
      "[37]\ttrain-mlogloss:0.13221\tvalid-mlogloss:1.19014\n",
      "[38]\ttrain-mlogloss:0.12758\tvalid-mlogloss:1.18665\n",
      "[39]\ttrain-mlogloss:0.12337\tvalid-mlogloss:1.19389\n",
      "[40]\ttrain-mlogloss:0.11916\tvalid-mlogloss:1.19788\n",
      "[41]\ttrain-mlogloss:0.11543\tvalid-mlogloss:1.19368\n",
      "[42]\ttrain-mlogloss:0.11173\tvalid-mlogloss:1.19641\n",
      "[43]\ttrain-mlogloss:0.10814\tvalid-mlogloss:1.19723\n",
      "[44]\ttrain-mlogloss:0.10473\tvalid-mlogloss:1.19952\n",
      "[45]\ttrain-mlogloss:0.10138\tvalid-mlogloss:1.20659\n",
      "[46]\ttrain-mlogloss:0.09827\tvalid-mlogloss:1.20256\n",
      "[47]\ttrain-mlogloss:0.09538\tvalid-mlogloss:1.20726\n",
      "[48]\ttrain-mlogloss:0.09254\tvalid-mlogloss:1.20619\n",
      "[49]\ttrain-mlogloss:0.09022\tvalid-mlogloss:1.20976\n",
      "[50]\ttrain-mlogloss:0.08765\tvalid-mlogloss:1.21103\n",
      "[51]\ttrain-mlogloss:0.08519\tvalid-mlogloss:1.21771\n",
      "[52]\ttrain-mlogloss:0.08290\tvalid-mlogloss:1.22058\n",
      "[53]\ttrain-mlogloss:0.08070\tvalid-mlogloss:1.22439\n",
      "[54]\ttrain-mlogloss:0.07859\tvalid-mlogloss:1.21882\n",
      "[55]\ttrain-mlogloss:0.07675\tvalid-mlogloss:1.22001\n",
      "[56]\ttrain-mlogloss:0.07500\tvalid-mlogloss:1.21965\n",
      "[57]\ttrain-mlogloss:0.07322\tvalid-mlogloss:1.22646\n",
      "[58]\ttrain-mlogloss:0.07152\tvalid-mlogloss:1.22097\n",
      "[59]\ttrain-mlogloss:0.06993\tvalid-mlogloss:1.22082\n",
      "[60]\ttrain-mlogloss:0.06838\tvalid-mlogloss:1.22871\n",
      "[61]\ttrain-mlogloss:0.06687\tvalid-mlogloss:1.23660\n",
      "[62]\ttrain-mlogloss:0.06546\tvalid-mlogloss:1.23835\n",
      "[63]\ttrain-mlogloss:0.06412\tvalid-mlogloss:1.23977\n",
      "[64]\ttrain-mlogloss:0.06276\tvalid-mlogloss:1.24350\n",
      "[65]\ttrain-mlogloss:0.06153\tvalid-mlogloss:1.24138\n",
      "[66]\ttrain-mlogloss:0.06041\tvalid-mlogloss:1.24986\n",
      "[67]\ttrain-mlogloss:0.05931\tvalid-mlogloss:1.24889\n",
      "[68]\ttrain-mlogloss:0.05821\tvalid-mlogloss:1.25181\n",
      "[69]\ttrain-mlogloss:0.05718\tvalid-mlogloss:1.25224\n",
      "[70]\ttrain-mlogloss:0.05618\tvalid-mlogloss:1.24913\n",
      "[71]\ttrain-mlogloss:0.05518\tvalid-mlogloss:1.24284\n",
      "[72]\ttrain-mlogloss:0.05427\tvalid-mlogloss:1.24764\n",
      "[73]\ttrain-mlogloss:0.05335\tvalid-mlogloss:1.25710\n",
      "[74]\ttrain-mlogloss:0.05246\tvalid-mlogloss:1.25991\n",
      "[75]\ttrain-mlogloss:0.05155\tvalid-mlogloss:1.26187\n",
      "[76]\ttrain-mlogloss:0.05069\tvalid-mlogloss:1.26510\n",
      "[77]\ttrain-mlogloss:0.04995\tvalid-mlogloss:1.26244\n",
      "[78]\ttrain-mlogloss:0.04918\tvalid-mlogloss:1.26409\n",
      "[79]\ttrain-mlogloss:0.04840\tvalid-mlogloss:1.26458\n",
      "[80]\ttrain-mlogloss:0.04766\tvalid-mlogloss:1.26354\n",
      "[81]\ttrain-mlogloss:0.04696\tvalid-mlogloss:1.26130\n",
      "[82]\ttrain-mlogloss:0.04619\tvalid-mlogloss:1.26162\n",
      "[83]\ttrain-mlogloss:0.04552\tvalid-mlogloss:1.26635\n",
      "[84]\ttrain-mlogloss:0.04491\tvalid-mlogloss:1.26779\n",
      "[85]\ttrain-mlogloss:0.04432\tvalid-mlogloss:1.27271\n",
      "[86]\ttrain-mlogloss:0.04370\tvalid-mlogloss:1.27462\n",
      "[87]\ttrain-mlogloss:0.04312\tvalid-mlogloss:1.27321\n",
      "[88]\ttrain-mlogloss:0.04251\tvalid-mlogloss:1.27877\n",
      "[89]\ttrain-mlogloss:0.04194\tvalid-mlogloss:1.27659\n",
      "[90]\ttrain-mlogloss:0.04136\tvalid-mlogloss:1.28024\n",
      "[91]\ttrain-mlogloss:0.04086\tvalid-mlogloss:1.27716\n",
      "[92]\ttrain-mlogloss:0.04035\tvalid-mlogloss:1.27267\n",
      "[93]\ttrain-mlogloss:0.03984\tvalid-mlogloss:1.27553\n",
      "[94]\ttrain-mlogloss:0.03938\tvalid-mlogloss:1.27688\n",
      "[95]\ttrain-mlogloss:0.03886\tvalid-mlogloss:1.27934\n",
      "[96]\ttrain-mlogloss:0.03845\tvalid-mlogloss:1.28134\n",
      "[97]\ttrain-mlogloss:0.03802\tvalid-mlogloss:1.28308\n",
      "[98]\ttrain-mlogloss:0.03762\tvalid-mlogloss:1.28265\n",
      "[99]\ttrain-mlogloss:0.03718\tvalid-mlogloss:1.28157\n",
      "[100]\ttrain-mlogloss:0.03679\tvalid-mlogloss:1.28083\n",
      "[101]\ttrain-mlogloss:0.03641\tvalid-mlogloss:1.28515\n",
      "[102]\ttrain-mlogloss:0.03598\tvalid-mlogloss:1.28503\n",
      "[103]\ttrain-mlogloss:0.03563\tvalid-mlogloss:1.28699\n",
      "[104]\ttrain-mlogloss:0.03523\tvalid-mlogloss:1.29011\n",
      "[105]\ttrain-mlogloss:0.03488\tvalid-mlogloss:1.29134\n",
      "[106]\ttrain-mlogloss:0.03454\tvalid-mlogloss:1.28882\n",
      "[107]\ttrain-mlogloss:0.03417\tvalid-mlogloss:1.28914\n",
      "[108]\ttrain-mlogloss:0.03385\tvalid-mlogloss:1.29201\n",
      "[109]\ttrain-mlogloss:0.03351\tvalid-mlogloss:1.29404\n",
      "[110]\ttrain-mlogloss:0.03320\tvalid-mlogloss:1.29222\n",
      "[111]\ttrain-mlogloss:0.03293\tvalid-mlogloss:1.29171\n",
      "[112]\ttrain-mlogloss:0.03263\tvalid-mlogloss:1.29077\n",
      "[113]\ttrain-mlogloss:0.03232\tvalid-mlogloss:1.29208\n",
      "[114]\ttrain-mlogloss:0.03201\tvalid-mlogloss:1.29609\n",
      "[115]\ttrain-mlogloss:0.03173\tvalid-mlogloss:1.29894\n",
      "[116]\ttrain-mlogloss:0.03148\tvalid-mlogloss:1.30051\n",
      "[117]\ttrain-mlogloss:0.03123\tvalid-mlogloss:1.30156\n",
      "[118]\ttrain-mlogloss:0.03096\tvalid-mlogloss:1.30226\n",
      "[119]\ttrain-mlogloss:0.03070\tvalid-mlogloss:1.30566\n",
      "[120]\ttrain-mlogloss:0.03046\tvalid-mlogloss:1.30446\n",
      "[121]\ttrain-mlogloss:0.03022\tvalid-mlogloss:1.30690\n",
      "[122]\ttrain-mlogloss:0.02990\tvalid-mlogloss:1.30679\n",
      "[123]\ttrain-mlogloss:0.02967\tvalid-mlogloss:1.30928\n",
      "[124]\ttrain-mlogloss:0.02946\tvalid-mlogloss:1.31218\n",
      "[125]\ttrain-mlogloss:0.02922\tvalid-mlogloss:1.31388\n",
      "[126]\ttrain-mlogloss:0.02899\tvalid-mlogloss:1.31275\n",
      "[127]\ttrain-mlogloss:0.02876\tvalid-mlogloss:1.31590\n",
      "[128]\ttrain-mlogloss:0.02850\tvalid-mlogloss:1.31875\n",
      "[129]\ttrain-mlogloss:0.02829\tvalid-mlogloss:1.32085\n",
      "[130]\ttrain-mlogloss:0.02808\tvalid-mlogloss:1.31950\n",
      "[131]\ttrain-mlogloss:0.02787\tvalid-mlogloss:1.32204\n",
      "[132]\ttrain-mlogloss:0.02766\tvalid-mlogloss:1.32520\n",
      "[133]\ttrain-mlogloss:0.02742\tvalid-mlogloss:1.32846\n",
      "[134]\ttrain-mlogloss:0.02721\tvalid-mlogloss:1.32817\n",
      "[135]\ttrain-mlogloss:0.02700\tvalid-mlogloss:1.33068\n",
      "[136]\ttrain-mlogloss:0.02681\tvalid-mlogloss:1.33292\n",
      "[137]\ttrain-mlogloss:0.02664\tvalid-mlogloss:1.33539\n",
      "[138]\ttrain-mlogloss:0.02644\tvalid-mlogloss:1.33709\n",
      "[139]\ttrain-mlogloss:0.02623\tvalid-mlogloss:1.33700\n",
      "[140]\ttrain-mlogloss:0.02607\tvalid-mlogloss:1.33851\n",
      "[141]\ttrain-mlogloss:0.02589\tvalid-mlogloss:1.33742\n",
      "[142]\ttrain-mlogloss:0.02571\tvalid-mlogloss:1.33938\n",
      "[143]\ttrain-mlogloss:0.02554\tvalid-mlogloss:1.33879\n",
      "[144]\ttrain-mlogloss:0.02539\tvalid-mlogloss:1.33963\n",
      "[145]\ttrain-mlogloss:0.02522\tvalid-mlogloss:1.34122\n",
      "[146]\ttrain-mlogloss:0.02508\tvalid-mlogloss:1.34116\n",
      "[147]\ttrain-mlogloss:0.02494\tvalid-mlogloss:1.34027\n",
      "[148]\ttrain-mlogloss:0.02479\tvalid-mlogloss:1.34222\n",
      "[149]\ttrain-mlogloss:0.02465\tvalid-mlogloss:1.34417\n",
      "[150]\ttrain-mlogloss:0.02452\tvalid-mlogloss:1.34445\n",
      "[151]\ttrain-mlogloss:0.02436\tvalid-mlogloss:1.34445\n",
      "[152]\ttrain-mlogloss:0.02422\tvalid-mlogloss:1.34503\n",
      "[153]\ttrain-mlogloss:0.02408\tvalid-mlogloss:1.34710\n",
      "[154]\ttrain-mlogloss:0.02392\tvalid-mlogloss:1.34536\n",
      "[155]\ttrain-mlogloss:0.02378\tvalid-mlogloss:1.34704\n",
      "[156]\ttrain-mlogloss:0.02367\tvalid-mlogloss:1.34863\n",
      "[157]\ttrain-mlogloss:0.02355\tvalid-mlogloss:1.34940\n",
      "[158]\ttrain-mlogloss:0.02344\tvalid-mlogloss:1.34899\n",
      "[159]\ttrain-mlogloss:0.02329\tvalid-mlogloss:1.34889\n",
      "[160]\ttrain-mlogloss:0.02318\tvalid-mlogloss:1.35105\n",
      "[161]\ttrain-mlogloss:0.02305\tvalid-mlogloss:1.35215\n",
      "[162]\ttrain-mlogloss:0.02293\tvalid-mlogloss:1.35354\n",
      "[163]\ttrain-mlogloss:0.02280\tvalid-mlogloss:1.35504\n",
      "[164]\ttrain-mlogloss:0.02270\tvalid-mlogloss:1.35734\n",
      "[165]\ttrain-mlogloss:0.02259\tvalid-mlogloss:1.35913\n",
      "[166]\ttrain-mlogloss:0.02247\tvalid-mlogloss:1.35881\n",
      "[167]\ttrain-mlogloss:0.02236\tvalid-mlogloss:1.35935\n",
      "[168]\ttrain-mlogloss:0.02224\tvalid-mlogloss:1.35854\n",
      "[169]\ttrain-mlogloss:0.02215\tvalid-mlogloss:1.35987\n",
      "[170]\ttrain-mlogloss:0.02203\tvalid-mlogloss:1.36039\n",
      "[171]\ttrain-mlogloss:0.02192\tvalid-mlogloss:1.36104\n",
      "[172]\ttrain-mlogloss:0.02184\tvalid-mlogloss:1.36170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173]\ttrain-mlogloss:0.02173\tvalid-mlogloss:1.36102\n",
      "[174]\ttrain-mlogloss:0.02163\tvalid-mlogloss:1.36118\n",
      "[175]\ttrain-mlogloss:0.02153\tvalid-mlogloss:1.36165\n",
      "[176]\ttrain-mlogloss:0.02144\tvalid-mlogloss:1.36079\n",
      "[177]\ttrain-mlogloss:0.02135\tvalid-mlogloss:1.36212\n",
      "[178]\ttrain-mlogloss:0.02126\tvalid-mlogloss:1.36320\n",
      "[179]\ttrain-mlogloss:0.02116\tvalid-mlogloss:1.36462\n",
      "[180]\ttrain-mlogloss:0.02105\tvalid-mlogloss:1.36794\n",
      "[181]\ttrain-mlogloss:0.02094\tvalid-mlogloss:1.36904\n",
      "[182]\ttrain-mlogloss:0.02086\tvalid-mlogloss:1.36803\n",
      "[183]\ttrain-mlogloss:0.02076\tvalid-mlogloss:1.37046\n",
      "[184]\ttrain-mlogloss:0.02068\tvalid-mlogloss:1.36974\n",
      "[185]\ttrain-mlogloss:0.02059\tvalid-mlogloss:1.37072\n",
      "[186]\ttrain-mlogloss:0.02050\tvalid-mlogloss:1.37196\n",
      "[187]\ttrain-mlogloss:0.02041\tvalid-mlogloss:1.37158\n",
      "[188]\ttrain-mlogloss:0.02033\tvalid-mlogloss:1.37251\n",
      "[189]\ttrain-mlogloss:0.02025\tvalid-mlogloss:1.37198\n",
      "[190]\ttrain-mlogloss:0.02018\tvalid-mlogloss:1.37426\n",
      "[191]\ttrain-mlogloss:0.02009\tvalid-mlogloss:1.37402\n",
      "[192]\ttrain-mlogloss:0.02000\tvalid-mlogloss:1.37608\n",
      "[193]\ttrain-mlogloss:0.01993\tvalid-mlogloss:1.37623\n",
      "[194]\ttrain-mlogloss:0.01985\tvalid-mlogloss:1.37711\n",
      "[195]\ttrain-mlogloss:0.01977\tvalid-mlogloss:1.37719\n",
      "[196]\ttrain-mlogloss:0.01969\tvalid-mlogloss:1.37739\n",
      "[197]\ttrain-mlogloss:0.01963\tvalid-mlogloss:1.37817\n",
      "[198]\ttrain-mlogloss:0.01955\tvalid-mlogloss:1.38027\n",
      "[199]\ttrain-mlogloss:0.01947\tvalid-mlogloss:1.38038\n",
      "[200]\ttrain-mlogloss:0.01940\tvalid-mlogloss:1.38193\n",
      "[201]\ttrain-mlogloss:0.01932\tvalid-mlogloss:1.38027\n",
      "[202]\ttrain-mlogloss:0.01926\tvalid-mlogloss:1.37991\n",
      "[203]\ttrain-mlogloss:0.01918\tvalid-mlogloss:1.37993\n",
      "[204]\ttrain-mlogloss:0.01911\tvalid-mlogloss:1.38212\n",
      "[205]\ttrain-mlogloss:0.01904\tvalid-mlogloss:1.38250\n",
      "[206]\ttrain-mlogloss:0.01897\tvalid-mlogloss:1.38395\n",
      "[207]\ttrain-mlogloss:0.01891\tvalid-mlogloss:1.38391\n",
      "[Fold 6/10]\n",
      "[Fold 6/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.392386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.392386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.352714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.392386\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.352714\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.014679, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:39:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01307\tvalid-mlogloss:1.08580\n",
      "[1]\ttrain-mlogloss:0.94088\tvalid-mlogloss:1.07110\n",
      "[2]\ttrain-mlogloss:0.87150\tvalid-mlogloss:1.05479\n",
      "[3]\ttrain-mlogloss:0.80878\tvalid-mlogloss:1.03094\n",
      "[4]\ttrain-mlogloss:0.75182\tvalid-mlogloss:1.03352\n",
      "[5]\ttrain-mlogloss:0.70336\tvalid-mlogloss:1.01771\n",
      "[6]\ttrain-mlogloss:0.65572\tvalid-mlogloss:1.01195\n",
      "[7]\ttrain-mlogloss:0.61304\tvalid-mlogloss:1.00931\n",
      "[8]\ttrain-mlogloss:0.57593\tvalid-mlogloss:1.00611\n",
      "[9]\ttrain-mlogloss:0.53804\tvalid-mlogloss:1.00890\n",
      "[10]\ttrain-mlogloss:0.50282\tvalid-mlogloss:1.00156\n",
      "[11]\ttrain-mlogloss:0.47439\tvalid-mlogloss:1.00692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\ttrain-mlogloss:0.44525\tvalid-mlogloss:1.00984\n",
      "[13]\ttrain-mlogloss:0.41841\tvalid-mlogloss:1.00564\n",
      "[14]\ttrain-mlogloss:0.39488\tvalid-mlogloss:1.01559\n",
      "[15]\ttrain-mlogloss:0.37233\tvalid-mlogloss:1.02084\n",
      "[16]\ttrain-mlogloss:0.35265\tvalid-mlogloss:1.01537\n",
      "[17]\ttrain-mlogloss:0.33232\tvalid-mlogloss:1.01811\n",
      "[18]\ttrain-mlogloss:0.31524\tvalid-mlogloss:1.02387\n",
      "[19]\ttrain-mlogloss:0.29754\tvalid-mlogloss:1.02794\n",
      "[20]\ttrain-mlogloss:0.28239\tvalid-mlogloss:1.02293\n",
      "[21]\ttrain-mlogloss:0.26637\tvalid-mlogloss:1.02325\n",
      "[22]\ttrain-mlogloss:0.25312\tvalid-mlogloss:1.03047\n",
      "[23]\ttrain-mlogloss:0.24021\tvalid-mlogloss:1.02665\n",
      "[24]\ttrain-mlogloss:0.22763\tvalid-mlogloss:1.03023\n",
      "[25]\ttrain-mlogloss:0.21681\tvalid-mlogloss:1.03652\n",
      "[26]\ttrain-mlogloss:0.20655\tvalid-mlogloss:1.03887\n",
      "[27]\ttrain-mlogloss:0.19617\tvalid-mlogloss:1.03456\n",
      "[28]\ttrain-mlogloss:0.18757\tvalid-mlogloss:1.04349\n",
      "[29]\ttrain-mlogloss:0.17865\tvalid-mlogloss:1.03982\n",
      "[30]\ttrain-mlogloss:0.17163\tvalid-mlogloss:1.04424\n",
      "[31]\ttrain-mlogloss:0.16395\tvalid-mlogloss:1.05250\n",
      "[32]\ttrain-mlogloss:0.15725\tvalid-mlogloss:1.05932\n",
      "[33]\ttrain-mlogloss:0.15095\tvalid-mlogloss:1.05857\n",
      "[34]\ttrain-mlogloss:0.14466\tvalid-mlogloss:1.06479\n",
      "[35]\ttrain-mlogloss:0.13871\tvalid-mlogloss:1.06632\n",
      "[36]\ttrain-mlogloss:0.13343\tvalid-mlogloss:1.07010\n",
      "[37]\ttrain-mlogloss:0.12854\tvalid-mlogloss:1.07394\n",
      "[38]\ttrain-mlogloss:0.12424\tvalid-mlogloss:1.07723\n",
      "[39]\ttrain-mlogloss:0.11988\tvalid-mlogloss:1.07815\n",
      "[40]\ttrain-mlogloss:0.11555\tvalid-mlogloss:1.07525\n",
      "[41]\ttrain-mlogloss:0.11176\tvalid-mlogloss:1.07586\n",
      "[42]\ttrain-mlogloss:0.10805\tvalid-mlogloss:1.08152\n",
      "[43]\ttrain-mlogloss:0.10449\tvalid-mlogloss:1.08024\n",
      "[44]\ttrain-mlogloss:0.10126\tvalid-mlogloss:1.08421\n",
      "[45]\ttrain-mlogloss:0.09823\tvalid-mlogloss:1.08836\n",
      "[46]\ttrain-mlogloss:0.09517\tvalid-mlogloss:1.09242\n",
      "[47]\ttrain-mlogloss:0.09235\tvalid-mlogloss:1.09814\n",
      "[48]\ttrain-mlogloss:0.08985\tvalid-mlogloss:1.10003\n",
      "[49]\ttrain-mlogloss:0.08737\tvalid-mlogloss:1.09879\n",
      "[50]\ttrain-mlogloss:0.08505\tvalid-mlogloss:1.10278\n",
      "[51]\ttrain-mlogloss:0.08282\tvalid-mlogloss:1.10021\n",
      "[52]\ttrain-mlogloss:0.08068\tvalid-mlogloss:1.10442\n",
      "[53]\ttrain-mlogloss:0.07869\tvalid-mlogloss:1.10773\n",
      "[54]\ttrain-mlogloss:0.07671\tvalid-mlogloss:1.10782\n",
      "[55]\ttrain-mlogloss:0.07488\tvalid-mlogloss:1.10492\n",
      "[56]\ttrain-mlogloss:0.07298\tvalid-mlogloss:1.10728\n",
      "[57]\ttrain-mlogloss:0.07136\tvalid-mlogloss:1.10800\n",
      "[58]\ttrain-mlogloss:0.06966\tvalid-mlogloss:1.11041\n",
      "[59]\ttrain-mlogloss:0.06800\tvalid-mlogloss:1.11170\n",
      "[60]\ttrain-mlogloss:0.06649\tvalid-mlogloss:1.11191\n",
      "[61]\ttrain-mlogloss:0.06498\tvalid-mlogloss:1.11334\n",
      "[62]\ttrain-mlogloss:0.06355\tvalid-mlogloss:1.11384\n",
      "[63]\ttrain-mlogloss:0.06224\tvalid-mlogloss:1.11751\n",
      "[64]\ttrain-mlogloss:0.06096\tvalid-mlogloss:1.12225\n",
      "[65]\ttrain-mlogloss:0.05979\tvalid-mlogloss:1.12831\n",
      "[66]\ttrain-mlogloss:0.05869\tvalid-mlogloss:1.12912\n",
      "[67]\ttrain-mlogloss:0.05757\tvalid-mlogloss:1.13186\n",
      "[68]\ttrain-mlogloss:0.05645\tvalid-mlogloss:1.13507\n",
      "[69]\ttrain-mlogloss:0.05550\tvalid-mlogloss:1.13813\n",
      "[70]\ttrain-mlogloss:0.05452\tvalid-mlogloss:1.14162\n",
      "[71]\ttrain-mlogloss:0.05350\tvalid-mlogloss:1.14175\n",
      "[72]\ttrain-mlogloss:0.05263\tvalid-mlogloss:1.14120\n",
      "[73]\ttrain-mlogloss:0.05177\tvalid-mlogloss:1.13856\n",
      "[74]\ttrain-mlogloss:0.05083\tvalid-mlogloss:1.13997\n",
      "[75]\ttrain-mlogloss:0.05004\tvalid-mlogloss:1.14422\n",
      "[76]\ttrain-mlogloss:0.04926\tvalid-mlogloss:1.14357\n",
      "[77]\ttrain-mlogloss:0.04841\tvalid-mlogloss:1.14286\n",
      "[78]\ttrain-mlogloss:0.04768\tvalid-mlogloss:1.14752\n",
      "[79]\ttrain-mlogloss:0.04701\tvalid-mlogloss:1.15097\n",
      "[80]\ttrain-mlogloss:0.04627\tvalid-mlogloss:1.15340\n",
      "[81]\ttrain-mlogloss:0.04563\tvalid-mlogloss:1.15475\n",
      "[82]\ttrain-mlogloss:0.04498\tvalid-mlogloss:1.15879\n",
      "[83]\ttrain-mlogloss:0.04431\tvalid-mlogloss:1.16057\n",
      "[84]\ttrain-mlogloss:0.04366\tvalid-mlogloss:1.16450\n",
      "[85]\ttrain-mlogloss:0.04311\tvalid-mlogloss:1.16569\n",
      "[86]\ttrain-mlogloss:0.04257\tvalid-mlogloss:1.16570\n",
      "[87]\ttrain-mlogloss:0.04201\tvalid-mlogloss:1.16764\n",
      "[88]\ttrain-mlogloss:0.04153\tvalid-mlogloss:1.17109\n",
      "[89]\ttrain-mlogloss:0.04097\tvalid-mlogloss:1.17433\n",
      "[90]\ttrain-mlogloss:0.04043\tvalid-mlogloss:1.17469\n",
      "[91]\ttrain-mlogloss:0.03985\tvalid-mlogloss:1.17432\n",
      "[92]\ttrain-mlogloss:0.03939\tvalid-mlogloss:1.17608\n",
      "[93]\ttrain-mlogloss:0.03893\tvalid-mlogloss:1.17918\n",
      "[94]\ttrain-mlogloss:0.03848\tvalid-mlogloss:1.17924\n",
      "[95]\ttrain-mlogloss:0.03805\tvalid-mlogloss:1.18090\n",
      "[96]\ttrain-mlogloss:0.03759\tvalid-mlogloss:1.18525\n",
      "[97]\ttrain-mlogloss:0.03709\tvalid-mlogloss:1.18720\n",
      "[98]\ttrain-mlogloss:0.03672\tvalid-mlogloss:1.19031\n",
      "[99]\ttrain-mlogloss:0.03631\tvalid-mlogloss:1.19210\n",
      "[100]\ttrain-mlogloss:0.03586\tvalid-mlogloss:1.19325\n",
      "[101]\ttrain-mlogloss:0.03546\tvalid-mlogloss:1.19650\n",
      "[102]\ttrain-mlogloss:0.03508\tvalid-mlogloss:1.19599\n",
      "[103]\ttrain-mlogloss:0.03473\tvalid-mlogloss:1.19479\n",
      "[104]\ttrain-mlogloss:0.03438\tvalid-mlogloss:1.19466\n",
      "[105]\ttrain-mlogloss:0.03402\tvalid-mlogloss:1.19773\n",
      "[106]\ttrain-mlogloss:0.03365\tvalid-mlogloss:1.20156\n",
      "[107]\ttrain-mlogloss:0.03334\tvalid-mlogloss:1.20264\n",
      "[108]\ttrain-mlogloss:0.03304\tvalid-mlogloss:1.20433\n",
      "[109]\ttrain-mlogloss:0.03271\tvalid-mlogloss:1.20526\n",
      "[110]\ttrain-mlogloss:0.03240\tvalid-mlogloss:1.20541\n",
      "[111]\ttrain-mlogloss:0.03203\tvalid-mlogloss:1.20727\n",
      "[112]\ttrain-mlogloss:0.03172\tvalid-mlogloss:1.20915\n",
      "[113]\ttrain-mlogloss:0.03143\tvalid-mlogloss:1.21149\n",
      "[114]\ttrain-mlogloss:0.03114\tvalid-mlogloss:1.21191\n",
      "[115]\ttrain-mlogloss:0.03085\tvalid-mlogloss:1.21415\n",
      "[116]\ttrain-mlogloss:0.03055\tvalid-mlogloss:1.21680\n",
      "[117]\ttrain-mlogloss:0.03031\tvalid-mlogloss:1.21971\n",
      "[118]\ttrain-mlogloss:0.03004\tvalid-mlogloss:1.22062\n",
      "[119]\ttrain-mlogloss:0.02979\tvalid-mlogloss:1.21934\n",
      "[120]\ttrain-mlogloss:0.02956\tvalid-mlogloss:1.21961\n",
      "[121]\ttrain-mlogloss:0.02931\tvalid-mlogloss:1.22095\n",
      "[122]\ttrain-mlogloss:0.02906\tvalid-mlogloss:1.22024\n",
      "[123]\ttrain-mlogloss:0.02883\tvalid-mlogloss:1.22343\n",
      "[124]\ttrain-mlogloss:0.02861\tvalid-mlogloss:1.22354\n",
      "[125]\ttrain-mlogloss:0.02837\tvalid-mlogloss:1.22388\n",
      "[126]\ttrain-mlogloss:0.02814\tvalid-mlogloss:1.22593\n",
      "[127]\ttrain-mlogloss:0.02796\tvalid-mlogloss:1.22575\n",
      "[128]\ttrain-mlogloss:0.02773\tvalid-mlogloss:1.22754\n",
      "[129]\ttrain-mlogloss:0.02751\tvalid-mlogloss:1.23084\n",
      "[130]\ttrain-mlogloss:0.02728\tvalid-mlogloss:1.23216\n",
      "[131]\ttrain-mlogloss:0.02705\tvalid-mlogloss:1.23391\n",
      "[132]\ttrain-mlogloss:0.02686\tvalid-mlogloss:1.23490\n",
      "[133]\ttrain-mlogloss:0.02667\tvalid-mlogloss:1.23697\n",
      "[134]\ttrain-mlogloss:0.02646\tvalid-mlogloss:1.24047\n",
      "[135]\ttrain-mlogloss:0.02628\tvalid-mlogloss:1.24158\n",
      "[136]\ttrain-mlogloss:0.02612\tvalid-mlogloss:1.24375\n",
      "[137]\ttrain-mlogloss:0.02596\tvalid-mlogloss:1.24437\n",
      "[138]\ttrain-mlogloss:0.02577\tvalid-mlogloss:1.24940\n",
      "[139]\ttrain-mlogloss:0.02560\tvalid-mlogloss:1.25087\n",
      "[140]\ttrain-mlogloss:0.02542\tvalid-mlogloss:1.25391\n",
      "[141]\ttrain-mlogloss:0.02527\tvalid-mlogloss:1.25548\n",
      "[142]\ttrain-mlogloss:0.02511\tvalid-mlogloss:1.25662\n",
      "[143]\ttrain-mlogloss:0.02492\tvalid-mlogloss:1.26214\n",
      "[144]\ttrain-mlogloss:0.02477\tvalid-mlogloss:1.26364\n",
      "[145]\ttrain-mlogloss:0.02460\tvalid-mlogloss:1.26642\n",
      "[146]\ttrain-mlogloss:0.02446\tvalid-mlogloss:1.26733\n",
      "[147]\ttrain-mlogloss:0.02429\tvalid-mlogloss:1.26950\n",
      "[148]\ttrain-mlogloss:0.02416\tvalid-mlogloss:1.27000\n",
      "[149]\ttrain-mlogloss:0.02402\tvalid-mlogloss:1.27329\n",
      "[150]\ttrain-mlogloss:0.02388\tvalid-mlogloss:1.27179\n",
      "[151]\ttrain-mlogloss:0.02372\tvalid-mlogloss:1.27329\n",
      "[152]\ttrain-mlogloss:0.02360\tvalid-mlogloss:1.27465\n",
      "[153]\ttrain-mlogloss:0.02345\tvalid-mlogloss:1.27717\n",
      "[154]\ttrain-mlogloss:0.02332\tvalid-mlogloss:1.27733\n",
      "[155]\ttrain-mlogloss:0.02318\tvalid-mlogloss:1.27813\n",
      "[156]\ttrain-mlogloss:0.02306\tvalid-mlogloss:1.27813\n",
      "[157]\ttrain-mlogloss:0.02294\tvalid-mlogloss:1.28153\n",
      "[158]\ttrain-mlogloss:0.02279\tvalid-mlogloss:1.28440\n",
      "[159]\ttrain-mlogloss:0.02270\tvalid-mlogloss:1.28549\n",
      "[160]\ttrain-mlogloss:0.02257\tvalid-mlogloss:1.28602\n",
      "[161]\ttrain-mlogloss:0.02243\tvalid-mlogloss:1.28377\n",
      "[162]\ttrain-mlogloss:0.02230\tvalid-mlogloss:1.28660\n",
      "[163]\ttrain-mlogloss:0.02219\tvalid-mlogloss:1.28655\n",
      "[164]\ttrain-mlogloss:0.02207\tvalid-mlogloss:1.28564\n",
      "[165]\ttrain-mlogloss:0.02196\tvalid-mlogloss:1.28656\n",
      "[166]\ttrain-mlogloss:0.02185\tvalid-mlogloss:1.28799\n",
      "[167]\ttrain-mlogloss:0.02173\tvalid-mlogloss:1.28739\n",
      "[168]\ttrain-mlogloss:0.02162\tvalid-mlogloss:1.29024\n",
      "[169]\ttrain-mlogloss:0.02150\tvalid-mlogloss:1.29276\n",
      "[170]\ttrain-mlogloss:0.02138\tvalid-mlogloss:1.29348\n",
      "[171]\ttrain-mlogloss:0.02128\tvalid-mlogloss:1.29422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172]\ttrain-mlogloss:0.02119\tvalid-mlogloss:1.29218\n",
      "[173]\ttrain-mlogloss:0.02109\tvalid-mlogloss:1.29469\n",
      "[174]\ttrain-mlogloss:0.02099\tvalid-mlogloss:1.29470\n",
      "[175]\ttrain-mlogloss:0.02090\tvalid-mlogloss:1.29839\n",
      "[176]\ttrain-mlogloss:0.02081\tvalid-mlogloss:1.30028\n",
      "[177]\ttrain-mlogloss:0.02070\tvalid-mlogloss:1.30132\n",
      "[178]\ttrain-mlogloss:0.02059\tvalid-mlogloss:1.30467\n",
      "[179]\ttrain-mlogloss:0.02049\tvalid-mlogloss:1.30585\n",
      "[180]\ttrain-mlogloss:0.02038\tvalid-mlogloss:1.30880\n",
      "[181]\ttrain-mlogloss:0.02029\tvalid-mlogloss:1.31173\n",
      "[182]\ttrain-mlogloss:0.02021\tvalid-mlogloss:1.31351\n",
      "[183]\ttrain-mlogloss:0.02013\tvalid-mlogloss:1.31190\n",
      "[184]\ttrain-mlogloss:0.02004\tvalid-mlogloss:1.31145\n",
      "[185]\ttrain-mlogloss:0.01995\tvalid-mlogloss:1.31255\n",
      "[186]\ttrain-mlogloss:0.01986\tvalid-mlogloss:1.31424\n",
      "[187]\ttrain-mlogloss:0.01979\tvalid-mlogloss:1.31326\n",
      "[188]\ttrain-mlogloss:0.01970\tvalid-mlogloss:1.31512\n",
      "[189]\ttrain-mlogloss:0.01963\tvalid-mlogloss:1.31546\n",
      "[190]\ttrain-mlogloss:0.01955\tvalid-mlogloss:1.31624\n",
      "[191]\ttrain-mlogloss:0.01948\tvalid-mlogloss:1.31709\n",
      "[192]\ttrain-mlogloss:0.01941\tvalid-mlogloss:1.31669\n",
      "[193]\ttrain-mlogloss:0.01934\tvalid-mlogloss:1.31660\n",
      "[194]\ttrain-mlogloss:0.01927\tvalid-mlogloss:1.31774\n",
      "[195]\ttrain-mlogloss:0.01920\tvalid-mlogloss:1.31820\n",
      "[196]\ttrain-mlogloss:0.01913\tvalid-mlogloss:1.31969\n",
      "[197]\ttrain-mlogloss:0.01904\tvalid-mlogloss:1.32179\n",
      "[198]\ttrain-mlogloss:0.01898\tvalid-mlogloss:1.32163\n",
      "[199]\ttrain-mlogloss:0.01892\tvalid-mlogloss:1.32129\n",
      "[200]\ttrain-mlogloss:0.01885\tvalid-mlogloss:1.32014\n",
      "[201]\ttrain-mlogloss:0.01879\tvalid-mlogloss:1.32068\n",
      "[202]\ttrain-mlogloss:0.01873\tvalid-mlogloss:1.32115\n",
      "[203]\ttrain-mlogloss:0.01866\tvalid-mlogloss:1.32272\n",
      "[204]\ttrain-mlogloss:0.01859\tvalid-mlogloss:1.32517\n",
      "[205]\ttrain-mlogloss:0.01853\tvalid-mlogloss:1.32581\n",
      "[206]\ttrain-mlogloss:0.01847\tvalid-mlogloss:1.32724\n",
      "[207]\ttrain-mlogloss:0.01840\tvalid-mlogloss:1.32942\n",
      "[208]\ttrain-mlogloss:0.01833\tvalid-mlogloss:1.33017\n",
      "[209]\ttrain-mlogloss:0.01826\tvalid-mlogloss:1.32973\n",
      "[210]\ttrain-mlogloss:0.01820\tvalid-mlogloss:1.32918\n",
      "[Fold 7/10]\n",
      "[Fold 7/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.390131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.390131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.432609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.390131\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.432609\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.013529, \tValidation Accuracy: 86.206897 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:41:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01635\tvalid-mlogloss:1.10325\n",
      "[1]\ttrain-mlogloss:0.94251\tvalid-mlogloss:1.10194\n",
      "[2]\ttrain-mlogloss:0.86930\tvalid-mlogloss:1.10708\n",
      "[3]\ttrain-mlogloss:0.81059\tvalid-mlogloss:1.12365\n",
      "[4]\ttrain-mlogloss:0.75194\tvalid-mlogloss:1.11579\n",
      "[5]\ttrain-mlogloss:0.69690\tvalid-mlogloss:1.12670\n",
      "[6]\ttrain-mlogloss:0.64770\tvalid-mlogloss:1.13956\n",
      "[7]\ttrain-mlogloss:0.60491\tvalid-mlogloss:1.13941\n",
      "[8]\ttrain-mlogloss:0.56606\tvalid-mlogloss:1.15102\n",
      "[9]\ttrain-mlogloss:0.52928\tvalid-mlogloss:1.15561\n",
      "[10]\ttrain-mlogloss:0.49713\tvalid-mlogloss:1.15380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\ttrain-mlogloss:0.46760\tvalid-mlogloss:1.16416\n",
      "[12]\ttrain-mlogloss:0.43857\tvalid-mlogloss:1.16188\n",
      "[13]\ttrain-mlogloss:0.41298\tvalid-mlogloss:1.16873\n",
      "[14]\ttrain-mlogloss:0.38900\tvalid-mlogloss:1.16397\n",
      "[15]\ttrain-mlogloss:0.36715\tvalid-mlogloss:1.17148\n",
      "[16]\ttrain-mlogloss:0.34443\tvalid-mlogloss:1.17701\n",
      "[17]\ttrain-mlogloss:0.32584\tvalid-mlogloss:1.18629\n",
      "[18]\ttrain-mlogloss:0.30882\tvalid-mlogloss:1.18551\n",
      "[19]\ttrain-mlogloss:0.29165\tvalid-mlogloss:1.19138\n",
      "[20]\ttrain-mlogloss:0.27629\tvalid-mlogloss:1.20404\n",
      "[21]\ttrain-mlogloss:0.26114\tvalid-mlogloss:1.22687\n",
      "[22]\ttrain-mlogloss:0.24855\tvalid-mlogloss:1.22612\n",
      "[23]\ttrain-mlogloss:0.23689\tvalid-mlogloss:1.23616\n",
      "[24]\ttrain-mlogloss:0.22604\tvalid-mlogloss:1.24637\n",
      "[25]\ttrain-mlogloss:0.21490\tvalid-mlogloss:1.24910\n",
      "[26]\ttrain-mlogloss:0.20524\tvalid-mlogloss:1.26000\n",
      "[27]\ttrain-mlogloss:0.19558\tvalid-mlogloss:1.26569\n",
      "[28]\ttrain-mlogloss:0.18728\tvalid-mlogloss:1.27942\n",
      "[29]\ttrain-mlogloss:0.17992\tvalid-mlogloss:1.28649\n",
      "[30]\ttrain-mlogloss:0.17198\tvalid-mlogloss:1.30037\n",
      "[31]\ttrain-mlogloss:0.16506\tvalid-mlogloss:1.30456\n",
      "[32]\ttrain-mlogloss:0.15811\tvalid-mlogloss:1.31378\n",
      "[33]\ttrain-mlogloss:0.15156\tvalid-mlogloss:1.31814\n",
      "[34]\ttrain-mlogloss:0.14536\tvalid-mlogloss:1.32128\n",
      "[35]\ttrain-mlogloss:0.13945\tvalid-mlogloss:1.32553\n",
      "[36]\ttrain-mlogloss:0.13448\tvalid-mlogloss:1.32699\n",
      "[37]\ttrain-mlogloss:0.12932\tvalid-mlogloss:1.33694\n",
      "[38]\ttrain-mlogloss:0.12473\tvalid-mlogloss:1.34178\n",
      "[39]\ttrain-mlogloss:0.12027\tvalid-mlogloss:1.34736\n",
      "[40]\ttrain-mlogloss:0.11643\tvalid-mlogloss:1.35730\n",
      "[41]\ttrain-mlogloss:0.11221\tvalid-mlogloss:1.37160\n",
      "[42]\ttrain-mlogloss:0.10830\tvalid-mlogloss:1.38399\n",
      "[43]\ttrain-mlogloss:0.10513\tvalid-mlogloss:1.38306\n",
      "[44]\ttrain-mlogloss:0.10198\tvalid-mlogloss:1.39156\n",
      "[45]\ttrain-mlogloss:0.09887\tvalid-mlogloss:1.39797\n",
      "[46]\ttrain-mlogloss:0.09584\tvalid-mlogloss:1.40194\n",
      "[47]\ttrain-mlogloss:0.09282\tvalid-mlogloss:1.40509\n",
      "[48]\ttrain-mlogloss:0.09026\tvalid-mlogloss:1.41508\n",
      "[49]\ttrain-mlogloss:0.08788\tvalid-mlogloss:1.41821\n",
      "[50]\ttrain-mlogloss:0.08554\tvalid-mlogloss:1.42634\n",
      "[51]\ttrain-mlogloss:0.08323\tvalid-mlogloss:1.42884\n",
      "[52]\ttrain-mlogloss:0.08095\tvalid-mlogloss:1.43451\n",
      "[53]\ttrain-mlogloss:0.07897\tvalid-mlogloss:1.43577\n",
      "[54]\ttrain-mlogloss:0.07695\tvalid-mlogloss:1.44141\n",
      "[55]\ttrain-mlogloss:0.07496\tvalid-mlogloss:1.44417\n",
      "[56]\ttrain-mlogloss:0.07328\tvalid-mlogloss:1.45257\n",
      "[57]\ttrain-mlogloss:0.07159\tvalid-mlogloss:1.45734\n",
      "[58]\ttrain-mlogloss:0.07001\tvalid-mlogloss:1.46328\n",
      "[59]\ttrain-mlogloss:0.06834\tvalid-mlogloss:1.46865\n",
      "[60]\ttrain-mlogloss:0.06681\tvalid-mlogloss:1.47247\n",
      "[61]\ttrain-mlogloss:0.06525\tvalid-mlogloss:1.47367\n",
      "[62]\ttrain-mlogloss:0.06379\tvalid-mlogloss:1.47444\n",
      "[63]\ttrain-mlogloss:0.06242\tvalid-mlogloss:1.48011\n",
      "[64]\ttrain-mlogloss:0.06116\tvalid-mlogloss:1.48089\n",
      "[65]\ttrain-mlogloss:0.05988\tvalid-mlogloss:1.48376\n",
      "[66]\ttrain-mlogloss:0.05873\tvalid-mlogloss:1.48600\n",
      "[67]\ttrain-mlogloss:0.05758\tvalid-mlogloss:1.49062\n",
      "[68]\ttrain-mlogloss:0.05648\tvalid-mlogloss:1.49625\n",
      "[69]\ttrain-mlogloss:0.05540\tvalid-mlogloss:1.49715\n",
      "[70]\ttrain-mlogloss:0.05430\tvalid-mlogloss:1.49804\n",
      "[71]\ttrain-mlogloss:0.05340\tvalid-mlogloss:1.50078\n",
      "[72]\ttrain-mlogloss:0.05242\tvalid-mlogloss:1.50717\n",
      "[73]\ttrain-mlogloss:0.05142\tvalid-mlogloss:1.50710\n",
      "[74]\ttrain-mlogloss:0.05059\tvalid-mlogloss:1.51479\n",
      "[75]\ttrain-mlogloss:0.04964\tvalid-mlogloss:1.51980\n",
      "[76]\ttrain-mlogloss:0.04884\tvalid-mlogloss:1.52067\n",
      "[77]\ttrain-mlogloss:0.04797\tvalid-mlogloss:1.52461\n",
      "[78]\ttrain-mlogloss:0.04717\tvalid-mlogloss:1.52813\n",
      "[79]\ttrain-mlogloss:0.04643\tvalid-mlogloss:1.53319\n",
      "[80]\ttrain-mlogloss:0.04575\tvalid-mlogloss:1.53703\n",
      "[81]\ttrain-mlogloss:0.04504\tvalid-mlogloss:1.54176\n",
      "[82]\ttrain-mlogloss:0.04441\tvalid-mlogloss:1.54413\n",
      "[83]\ttrain-mlogloss:0.04377\tvalid-mlogloss:1.54787\n",
      "[84]\ttrain-mlogloss:0.04310\tvalid-mlogloss:1.55278\n",
      "[85]\ttrain-mlogloss:0.04251\tvalid-mlogloss:1.55765\n",
      "[86]\ttrain-mlogloss:0.04196\tvalid-mlogloss:1.55679\n",
      "[87]\ttrain-mlogloss:0.04147\tvalid-mlogloss:1.56033\n",
      "[88]\ttrain-mlogloss:0.04094\tvalid-mlogloss:1.56672\n",
      "[89]\ttrain-mlogloss:0.04041\tvalid-mlogloss:1.57030\n",
      "[90]\ttrain-mlogloss:0.03984\tvalid-mlogloss:1.57326\n",
      "[91]\ttrain-mlogloss:0.03937\tvalid-mlogloss:1.57777\n",
      "[92]\ttrain-mlogloss:0.03886\tvalid-mlogloss:1.57950\n",
      "[93]\ttrain-mlogloss:0.03842\tvalid-mlogloss:1.58384\n",
      "[94]\ttrain-mlogloss:0.03801\tvalid-mlogloss:1.58765\n",
      "[95]\ttrain-mlogloss:0.03757\tvalid-mlogloss:1.59584\n",
      "[96]\ttrain-mlogloss:0.03717\tvalid-mlogloss:1.59529\n",
      "[97]\ttrain-mlogloss:0.03672\tvalid-mlogloss:1.59910\n",
      "[98]\ttrain-mlogloss:0.03627\tvalid-mlogloss:1.60109\n",
      "[99]\ttrain-mlogloss:0.03582\tvalid-mlogloss:1.60305\n",
      "[100]\ttrain-mlogloss:0.03543\tvalid-mlogloss:1.60296\n",
      "[101]\ttrain-mlogloss:0.03502\tvalid-mlogloss:1.60657\n",
      "[102]\ttrain-mlogloss:0.03465\tvalid-mlogloss:1.61055\n",
      "[103]\ttrain-mlogloss:0.03432\tvalid-mlogloss:1.61164\n",
      "[104]\ttrain-mlogloss:0.03396\tvalid-mlogloss:1.61308\n",
      "[105]\ttrain-mlogloss:0.03360\tvalid-mlogloss:1.61733\n",
      "[106]\ttrain-mlogloss:0.03325\tvalid-mlogloss:1.61993\n",
      "[107]\ttrain-mlogloss:0.03290\tvalid-mlogloss:1.62082\n",
      "[108]\ttrain-mlogloss:0.03258\tvalid-mlogloss:1.62285\n",
      "[109]\ttrain-mlogloss:0.03227\tvalid-mlogloss:1.62323\n",
      "[110]\ttrain-mlogloss:0.03194\tvalid-mlogloss:1.62600\n",
      "[111]\ttrain-mlogloss:0.03162\tvalid-mlogloss:1.62619\n",
      "[112]\ttrain-mlogloss:0.03129\tvalid-mlogloss:1.62511\n",
      "[113]\ttrain-mlogloss:0.03098\tvalid-mlogloss:1.63135\n",
      "[114]\ttrain-mlogloss:0.03072\tvalid-mlogloss:1.63146\n",
      "[115]\ttrain-mlogloss:0.03042\tvalid-mlogloss:1.63000\n",
      "[116]\ttrain-mlogloss:0.03019\tvalid-mlogloss:1.63357\n",
      "[117]\ttrain-mlogloss:0.02992\tvalid-mlogloss:1.63690\n",
      "[118]\ttrain-mlogloss:0.02966\tvalid-mlogloss:1.64171\n",
      "[119]\ttrain-mlogloss:0.02937\tvalid-mlogloss:1.64611\n",
      "[120]\ttrain-mlogloss:0.02916\tvalid-mlogloss:1.64735\n",
      "[121]\ttrain-mlogloss:0.02892\tvalid-mlogloss:1.64986\n",
      "[122]\ttrain-mlogloss:0.02870\tvalid-mlogloss:1.65246\n",
      "[123]\ttrain-mlogloss:0.02848\tvalid-mlogloss:1.65261\n",
      "[124]\ttrain-mlogloss:0.02826\tvalid-mlogloss:1.65447\n",
      "[125]\ttrain-mlogloss:0.02805\tvalid-mlogloss:1.65819\n",
      "[126]\ttrain-mlogloss:0.02784\tvalid-mlogloss:1.66038\n",
      "[127]\ttrain-mlogloss:0.02757\tvalid-mlogloss:1.66010\n",
      "[128]\ttrain-mlogloss:0.02734\tvalid-mlogloss:1.66335\n",
      "[129]\ttrain-mlogloss:0.02711\tvalid-mlogloss:1.66467\n",
      "[130]\ttrain-mlogloss:0.02691\tvalid-mlogloss:1.66724\n",
      "[131]\ttrain-mlogloss:0.02674\tvalid-mlogloss:1.66791\n",
      "[132]\ttrain-mlogloss:0.02656\tvalid-mlogloss:1.66865\n",
      "[133]\ttrain-mlogloss:0.02635\tvalid-mlogloss:1.67150\n",
      "[134]\ttrain-mlogloss:0.02616\tvalid-mlogloss:1.67318\n",
      "[135]\ttrain-mlogloss:0.02599\tvalid-mlogloss:1.67596\n",
      "[136]\ttrain-mlogloss:0.02581\tvalid-mlogloss:1.67871\n",
      "[137]\ttrain-mlogloss:0.02567\tvalid-mlogloss:1.68081\n",
      "[138]\ttrain-mlogloss:0.02549\tvalid-mlogloss:1.68452\n",
      "[139]\ttrain-mlogloss:0.02530\tvalid-mlogloss:1.68691\n",
      "[140]\ttrain-mlogloss:0.02516\tvalid-mlogloss:1.69471\n",
      "[141]\ttrain-mlogloss:0.02500\tvalid-mlogloss:1.69793\n",
      "[142]\ttrain-mlogloss:0.02484\tvalid-mlogloss:1.69586\n",
      "[143]\ttrain-mlogloss:0.02468\tvalid-mlogloss:1.69686\n",
      "[144]\ttrain-mlogloss:0.02453\tvalid-mlogloss:1.70117\n",
      "[145]\ttrain-mlogloss:0.02436\tvalid-mlogloss:1.70286\n",
      "[146]\ttrain-mlogloss:0.02421\tvalid-mlogloss:1.70337\n",
      "[147]\ttrain-mlogloss:0.02405\tvalid-mlogloss:1.70759\n",
      "[148]\ttrain-mlogloss:0.02392\tvalid-mlogloss:1.70830\n",
      "[149]\ttrain-mlogloss:0.02375\tvalid-mlogloss:1.71214\n",
      "[150]\ttrain-mlogloss:0.02360\tvalid-mlogloss:1.71383\n",
      "[151]\ttrain-mlogloss:0.02344\tvalid-mlogloss:1.71841\n",
      "[152]\ttrain-mlogloss:0.02330\tvalid-mlogloss:1.71819\n",
      "[153]\ttrain-mlogloss:0.02314\tvalid-mlogloss:1.71951\n",
      "[154]\ttrain-mlogloss:0.02303\tvalid-mlogloss:1.72407\n",
      "[155]\ttrain-mlogloss:0.02290\tvalid-mlogloss:1.72636\n",
      "[156]\ttrain-mlogloss:0.02278\tvalid-mlogloss:1.72589\n",
      "[157]\ttrain-mlogloss:0.02265\tvalid-mlogloss:1.72715\n",
      "[158]\ttrain-mlogloss:0.02254\tvalid-mlogloss:1.73011\n",
      "[159]\ttrain-mlogloss:0.02241\tvalid-mlogloss:1.72853\n",
      "[160]\ttrain-mlogloss:0.02230\tvalid-mlogloss:1.73025\n",
      "[161]\ttrain-mlogloss:0.02218\tvalid-mlogloss:1.73229\n",
      "[162]\ttrain-mlogloss:0.02205\tvalid-mlogloss:1.73325\n",
      "[163]\ttrain-mlogloss:0.02193\tvalid-mlogloss:1.73446\n",
      "[164]\ttrain-mlogloss:0.02183\tvalid-mlogloss:1.73454\n",
      "[165]\ttrain-mlogloss:0.02172\tvalid-mlogloss:1.73408\n",
      "[166]\ttrain-mlogloss:0.02162\tvalid-mlogloss:1.73557\n",
      "[167]\ttrain-mlogloss:0.02151\tvalid-mlogloss:1.73815\n",
      "[168]\ttrain-mlogloss:0.02143\tvalid-mlogloss:1.73917\n",
      "[169]\ttrain-mlogloss:0.02133\tvalid-mlogloss:1.73965\n",
      "[170]\ttrain-mlogloss:0.02123\tvalid-mlogloss:1.73949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171]\ttrain-mlogloss:0.02113\tvalid-mlogloss:1.74045\n",
      "[172]\ttrain-mlogloss:0.02103\tvalid-mlogloss:1.74240\n",
      "[173]\ttrain-mlogloss:0.02094\tvalid-mlogloss:1.74333\n",
      "[174]\ttrain-mlogloss:0.02084\tvalid-mlogloss:1.74380\n",
      "[175]\ttrain-mlogloss:0.02075\tvalid-mlogloss:1.74436\n",
      "[176]\ttrain-mlogloss:0.02067\tvalid-mlogloss:1.74404\n",
      "[177]\ttrain-mlogloss:0.02058\tvalid-mlogloss:1.74532\n",
      "[178]\ttrain-mlogloss:0.02049\tvalid-mlogloss:1.74835\n",
      "[179]\ttrain-mlogloss:0.02040\tvalid-mlogloss:1.75126\n",
      "[180]\ttrain-mlogloss:0.02031\tvalid-mlogloss:1.75264\n",
      "[181]\ttrain-mlogloss:0.02022\tvalid-mlogloss:1.75210\n",
      "[182]\ttrain-mlogloss:0.02013\tvalid-mlogloss:1.75501\n",
      "[183]\ttrain-mlogloss:0.02003\tvalid-mlogloss:1.75731\n",
      "[184]\ttrain-mlogloss:0.01995\tvalid-mlogloss:1.75802\n",
      "[185]\ttrain-mlogloss:0.01986\tvalid-mlogloss:1.75954\n",
      "[186]\ttrain-mlogloss:0.01977\tvalid-mlogloss:1.76010\n",
      "[187]\ttrain-mlogloss:0.01969\tvalid-mlogloss:1.76008\n",
      "[188]\ttrain-mlogloss:0.01959\tvalid-mlogloss:1.75968\n",
      "[189]\ttrain-mlogloss:0.01950\tvalid-mlogloss:1.76317\n",
      "[190]\ttrain-mlogloss:0.01943\tvalid-mlogloss:1.76418\n",
      "[191]\ttrain-mlogloss:0.01935\tvalid-mlogloss:1.76443\n",
      "[192]\ttrain-mlogloss:0.01927\tvalid-mlogloss:1.76618\n",
      "[193]\ttrain-mlogloss:0.01919\tvalid-mlogloss:1.76799\n",
      "[194]\ttrain-mlogloss:0.01910\tvalid-mlogloss:1.77052\n",
      "[195]\ttrain-mlogloss:0.01902\tvalid-mlogloss:1.76964\n",
      "[196]\ttrain-mlogloss:0.01894\tvalid-mlogloss:1.76931\n",
      "[197]\ttrain-mlogloss:0.01886\tvalid-mlogloss:1.77234\n",
      "[198]\ttrain-mlogloss:0.01881\tvalid-mlogloss:1.77125\n",
      "[199]\ttrain-mlogloss:0.01874\tvalid-mlogloss:1.77182\n",
      "[200]\ttrain-mlogloss:0.01866\tvalid-mlogloss:1.77188\n",
      "[201]\ttrain-mlogloss:0.01860\tvalid-mlogloss:1.77279\n",
      "[Fold 8/10]\n",
      "[Fold 8/10 Prediciton:]\n",
      "Train Epoch: 1 [0/259 (0%)]\tTrain Loss: 0.407691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 2 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 2 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 3 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 3 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 4 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 4 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 5 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 5 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 6 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 6 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 7 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 7 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 8 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 8 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 9 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 9 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 10 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 10 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 11 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 11 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 12 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 12 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 13 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 13 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 14 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 14 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 15 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 15 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 16 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 16 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 17 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 17 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 18 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 18 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 19 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 19 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 20 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 20 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 21 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 21 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 22 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 22 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 23 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 23 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 24 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 24 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 25 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 25 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 26 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 26 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 27 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 27 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 28 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 28 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 29 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 29 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 30 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 30 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 31 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 31 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 32 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 32 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 33 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 33 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 34 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 34 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 35 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 35 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 36 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 36 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 37 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 37 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 38 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 38 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 39 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 39 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 40 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 40 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 41 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 41 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 42 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 42 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 43 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 43 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 44 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 44 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 45 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 45 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 46 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 46 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 47 [0/259 (0%)]\tTrain Loss: 0.407691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 48 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 48 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 49 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 49 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 50 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 50 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 51 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 51 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 52 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 52 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 53 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 53 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 54 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 54 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 55 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 55 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 56 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 56 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 57 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 57 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 58 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 58 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 59 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 59 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 60 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 60 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 61 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 61 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 62 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 62 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 63 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 63 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 64 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 64 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 65 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 65 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 66 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 66 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 67 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 67 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 68 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 68 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 69 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 69 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 70 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 70 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 71 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 71 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 72 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 72 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 73 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 73 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 74 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 74 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 75 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 75 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 76 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 76 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 77 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 77 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 78 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 78 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 79 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 79 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 80 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 80 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 81 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 81 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 82 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 82 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 83 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 83 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 84 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 84 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 85 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 85 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 86 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 86 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 87 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 87 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 88 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 88 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 89 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 89 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 90 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 90 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 91 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 91 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 92 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 92 [160/259 (56%)]\tTrain Loss: 0.422032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 93 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 93 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 94 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 94 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 95 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 95 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 96 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 96 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 97 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 97 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 98 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 98 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 99 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 99 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "Train Epoch: 100 [0/259 (0%)]\tTrain Loss: 0.407691\n",
      "Train Epoch: 100 [160/259 (56%)]\tTrain Loss: 0.422032\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.014355, \tValidation Accuracy: 82.758621 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:43:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.00661\tvalid-mlogloss:1.10038\n",
      "[1]\ttrain-mlogloss:0.92871\tvalid-mlogloss:1.09370\n",
      "[2]\ttrain-mlogloss:0.86286\tvalid-mlogloss:1.09787\n",
      "[3]\ttrain-mlogloss:0.79925\tvalid-mlogloss:1.10546\n",
      "[4]\ttrain-mlogloss:0.73912\tvalid-mlogloss:1.11962\n",
      "[5]\ttrain-mlogloss:0.68722\tvalid-mlogloss:1.12735\n",
      "[6]\ttrain-mlogloss:0.63952\tvalid-mlogloss:1.14189\n",
      "[7]\ttrain-mlogloss:0.59534\tvalid-mlogloss:1.16636\n",
      "[8]\ttrain-mlogloss:0.55404\tvalid-mlogloss:1.17805\n",
      "[9]\ttrain-mlogloss:0.51541\tvalid-mlogloss:1.19719\n",
      "[10]\ttrain-mlogloss:0.48260\tvalid-mlogloss:1.20177\n",
      "[11]\ttrain-mlogloss:0.45061\tvalid-mlogloss:1.20245\n",
      "[12]\ttrain-mlogloss:0.42093\tvalid-mlogloss:1.20909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\ttrain-mlogloss:0.39382\tvalid-mlogloss:1.22491\n",
      "[14]\ttrain-mlogloss:0.36980\tvalid-mlogloss:1.22109\n",
      "[15]\ttrain-mlogloss:0.34929\tvalid-mlogloss:1.22515\n",
      "[16]\ttrain-mlogloss:0.32958\tvalid-mlogloss:1.22724\n",
      "[17]\ttrain-mlogloss:0.31254\tvalid-mlogloss:1.24157\n",
      "[18]\ttrain-mlogloss:0.29425\tvalid-mlogloss:1.25277\n",
      "[19]\ttrain-mlogloss:0.27793\tvalid-mlogloss:1.25840\n",
      "[20]\ttrain-mlogloss:0.26266\tvalid-mlogloss:1.27374\n",
      "[21]\ttrain-mlogloss:0.24972\tvalid-mlogloss:1.28239\n",
      "[22]\ttrain-mlogloss:0.23660\tvalid-mlogloss:1.28507\n",
      "[23]\ttrain-mlogloss:0.22411\tvalid-mlogloss:1.30201\n",
      "[24]\ttrain-mlogloss:0.21369\tvalid-mlogloss:1.32246\n",
      "[25]\ttrain-mlogloss:0.20277\tvalid-mlogloss:1.33140\n",
      "[26]\ttrain-mlogloss:0.19373\tvalid-mlogloss:1.34730\n",
      "[27]\ttrain-mlogloss:0.18473\tvalid-mlogloss:1.35516\n",
      "[28]\ttrain-mlogloss:0.17702\tvalid-mlogloss:1.37070\n",
      "[29]\ttrain-mlogloss:0.16918\tvalid-mlogloss:1.38299\n",
      "[30]\ttrain-mlogloss:0.16166\tvalid-mlogloss:1.39003\n",
      "[31]\ttrain-mlogloss:0.15518\tvalid-mlogloss:1.40060\n",
      "[32]\ttrain-mlogloss:0.14841\tvalid-mlogloss:1.41666\n",
      "[33]\ttrain-mlogloss:0.14243\tvalid-mlogloss:1.43082\n",
      "[34]\ttrain-mlogloss:0.13654\tvalid-mlogloss:1.44360\n",
      "[35]\ttrain-mlogloss:0.13154\tvalid-mlogloss:1.45541\n",
      "[36]\ttrain-mlogloss:0.12639\tvalid-mlogloss:1.47217\n",
      "[37]\ttrain-mlogloss:0.12196\tvalid-mlogloss:1.48884\n",
      "[38]\ttrain-mlogloss:0.11756\tvalid-mlogloss:1.49385\n",
      "[39]\ttrain-mlogloss:0.11362\tvalid-mlogloss:1.50843\n",
      "[40]\ttrain-mlogloss:0.10960\tvalid-mlogloss:1.52390\n",
      "[41]\ttrain-mlogloss:0.10591\tvalid-mlogloss:1.53349\n",
      "[42]\ttrain-mlogloss:0.10241\tvalid-mlogloss:1.53886\n",
      "[43]\ttrain-mlogloss:0.09930\tvalid-mlogloss:1.54958\n",
      "[44]\ttrain-mlogloss:0.09637\tvalid-mlogloss:1.55639\n",
      "[45]\ttrain-mlogloss:0.09332\tvalid-mlogloss:1.56002\n",
      "[46]\ttrain-mlogloss:0.09061\tvalid-mlogloss:1.57196\n",
      "[47]\ttrain-mlogloss:0.08807\tvalid-mlogloss:1.57294\n",
      "[48]\ttrain-mlogloss:0.08567\tvalid-mlogloss:1.58368\n",
      "[49]\ttrain-mlogloss:0.08335\tvalid-mlogloss:1.59684\n",
      "[50]\ttrain-mlogloss:0.08138\tvalid-mlogloss:1.60029\n",
      "[51]\ttrain-mlogloss:0.07925\tvalid-mlogloss:1.60049\n",
      "[52]\ttrain-mlogloss:0.07746\tvalid-mlogloss:1.59815\n",
      "[53]\ttrain-mlogloss:0.07573\tvalid-mlogloss:1.60126\n",
      "[54]\ttrain-mlogloss:0.07394\tvalid-mlogloss:1.59959\n",
      "[55]\ttrain-mlogloss:0.07222\tvalid-mlogloss:1.61084\n",
      "[56]\ttrain-mlogloss:0.07057\tvalid-mlogloss:1.61076\n",
      "[57]\ttrain-mlogloss:0.06896\tvalid-mlogloss:1.62277\n",
      "[58]\ttrain-mlogloss:0.06757\tvalid-mlogloss:1.62852\n",
      "[59]\ttrain-mlogloss:0.06613\tvalid-mlogloss:1.63688\n",
      "[60]\ttrain-mlogloss:0.06470\tvalid-mlogloss:1.64284\n",
      "[61]\ttrain-mlogloss:0.06338\tvalid-mlogloss:1.64969\n",
      "[62]\ttrain-mlogloss:0.06223\tvalid-mlogloss:1.65759\n",
      "[63]\ttrain-mlogloss:0.06095\tvalid-mlogloss:1.66166\n",
      "[64]\ttrain-mlogloss:0.05990\tvalid-mlogloss:1.66723\n",
      "[65]\ttrain-mlogloss:0.05877\tvalid-mlogloss:1.67392\n",
      "[66]\ttrain-mlogloss:0.05769\tvalid-mlogloss:1.68014\n",
      "[67]\ttrain-mlogloss:0.05648\tvalid-mlogloss:1.68624\n",
      "[68]\ttrain-mlogloss:0.05540\tvalid-mlogloss:1.69021\n",
      "[69]\ttrain-mlogloss:0.05441\tvalid-mlogloss:1.69630\n",
      "[70]\ttrain-mlogloss:0.05337\tvalid-mlogloss:1.69883\n",
      "[71]\ttrain-mlogloss:0.05251\tvalid-mlogloss:1.70068\n",
      "[72]\ttrain-mlogloss:0.05162\tvalid-mlogloss:1.70987\n",
      "[73]\ttrain-mlogloss:0.05080\tvalid-mlogloss:1.71474\n",
      "[74]\ttrain-mlogloss:0.04995\tvalid-mlogloss:1.71210\n",
      "[75]\ttrain-mlogloss:0.04921\tvalid-mlogloss:1.71754\n",
      "[76]\ttrain-mlogloss:0.04847\tvalid-mlogloss:1.72397\n",
      "[77]\ttrain-mlogloss:0.04774\tvalid-mlogloss:1.73681\n",
      "[78]\ttrain-mlogloss:0.04706\tvalid-mlogloss:1.73894\n",
      "[79]\ttrain-mlogloss:0.04635\tvalid-mlogloss:1.73725\n",
      "[80]\ttrain-mlogloss:0.04571\tvalid-mlogloss:1.74535\n",
      "[81]\ttrain-mlogloss:0.04499\tvalid-mlogloss:1.74960\n",
      "[82]\ttrain-mlogloss:0.04437\tvalid-mlogloss:1.75735\n",
      "[83]\ttrain-mlogloss:0.04375\tvalid-mlogloss:1.75923\n",
      "[84]\ttrain-mlogloss:0.04319\tvalid-mlogloss:1.76223\n",
      "[85]\ttrain-mlogloss:0.04260\tvalid-mlogloss:1.75852\n",
      "[86]\ttrain-mlogloss:0.04203\tvalid-mlogloss:1.76306\n",
      "[87]\ttrain-mlogloss:0.04148\tvalid-mlogloss:1.77003\n",
      "[88]\ttrain-mlogloss:0.04091\tvalid-mlogloss:1.76999\n",
      "[89]\ttrain-mlogloss:0.04041\tvalid-mlogloss:1.77107\n",
      "[90]\ttrain-mlogloss:0.03992\tvalid-mlogloss:1.77564\n",
      "[91]\ttrain-mlogloss:0.03942\tvalid-mlogloss:1.78240\n",
      "[92]\ttrain-mlogloss:0.03893\tvalid-mlogloss:1.78355\n",
      "[93]\ttrain-mlogloss:0.03848\tvalid-mlogloss:1.78287\n",
      "[94]\ttrain-mlogloss:0.03803\tvalid-mlogloss:1.78658\n",
      "[95]\ttrain-mlogloss:0.03757\tvalid-mlogloss:1.79099\n",
      "[96]\ttrain-mlogloss:0.03718\tvalid-mlogloss:1.79788\n",
      "[97]\ttrain-mlogloss:0.03669\tvalid-mlogloss:1.79811\n",
      "[98]\ttrain-mlogloss:0.03630\tvalid-mlogloss:1.80082\n",
      "[99]\ttrain-mlogloss:0.03589\tvalid-mlogloss:1.80430\n",
      "[100]\ttrain-mlogloss:0.03552\tvalid-mlogloss:1.80510\n",
      "[101]\ttrain-mlogloss:0.03521\tvalid-mlogloss:1.80801\n",
      "[102]\ttrain-mlogloss:0.03487\tvalid-mlogloss:1.81730\n",
      "[103]\ttrain-mlogloss:0.03450\tvalid-mlogloss:1.82178\n",
      "[104]\ttrain-mlogloss:0.03415\tvalid-mlogloss:1.82763\n",
      "[105]\ttrain-mlogloss:0.03388\tvalid-mlogloss:1.82961\n",
      "[106]\ttrain-mlogloss:0.03357\tvalid-mlogloss:1.83346\n",
      "[107]\ttrain-mlogloss:0.03326\tvalid-mlogloss:1.83739\n",
      "[108]\ttrain-mlogloss:0.03295\tvalid-mlogloss:1.83650\n",
      "[109]\ttrain-mlogloss:0.03268\tvalid-mlogloss:1.83735\n",
      "[110]\ttrain-mlogloss:0.03238\tvalid-mlogloss:1.84116\n",
      "[111]\ttrain-mlogloss:0.03207\tvalid-mlogloss:1.84399\n",
      "[112]\ttrain-mlogloss:0.03180\tvalid-mlogloss:1.84712\n",
      "[113]\ttrain-mlogloss:0.03155\tvalid-mlogloss:1.84959\n",
      "[114]\ttrain-mlogloss:0.03125\tvalid-mlogloss:1.85152\n",
      "[115]\ttrain-mlogloss:0.03097\tvalid-mlogloss:1.85261\n",
      "[116]\ttrain-mlogloss:0.03070\tvalid-mlogloss:1.85625\n",
      "[117]\ttrain-mlogloss:0.03044\tvalid-mlogloss:1.85963\n",
      "[118]\ttrain-mlogloss:0.03014\tvalid-mlogloss:1.86151\n",
      "[119]\ttrain-mlogloss:0.02988\tvalid-mlogloss:1.86490\n",
      "[120]\ttrain-mlogloss:0.02963\tvalid-mlogloss:1.86774\n",
      "[121]\ttrain-mlogloss:0.02942\tvalid-mlogloss:1.87054\n",
      "[122]\ttrain-mlogloss:0.02920\tvalid-mlogloss:1.87581\n",
      "[123]\ttrain-mlogloss:0.02896\tvalid-mlogloss:1.87785\n",
      "[124]\ttrain-mlogloss:0.02871\tvalid-mlogloss:1.88019\n",
      "[125]\ttrain-mlogloss:0.02849\tvalid-mlogloss:1.88146\n",
      "[126]\ttrain-mlogloss:0.02827\tvalid-mlogloss:1.88426\n",
      "[127]\ttrain-mlogloss:0.02803\tvalid-mlogloss:1.88960\n",
      "[128]\ttrain-mlogloss:0.02780\tvalid-mlogloss:1.89109\n",
      "[129]\ttrain-mlogloss:0.02759\tvalid-mlogloss:1.89423\n",
      "[130]\ttrain-mlogloss:0.02740\tvalid-mlogloss:1.89485\n",
      "[131]\ttrain-mlogloss:0.02718\tvalid-mlogloss:1.89594\n",
      "[132]\ttrain-mlogloss:0.02699\tvalid-mlogloss:1.89729\n",
      "[133]\ttrain-mlogloss:0.02680\tvalid-mlogloss:1.90173\n",
      "[134]\ttrain-mlogloss:0.02662\tvalid-mlogloss:1.90261\n",
      "[135]\ttrain-mlogloss:0.02642\tvalid-mlogloss:1.90398\n",
      "[136]\ttrain-mlogloss:0.02621\tvalid-mlogloss:1.90563\n",
      "[137]\ttrain-mlogloss:0.02602\tvalid-mlogloss:1.90637\n",
      "[138]\ttrain-mlogloss:0.02587\tvalid-mlogloss:1.90936\n",
      "[139]\ttrain-mlogloss:0.02570\tvalid-mlogloss:1.91265\n",
      "[140]\ttrain-mlogloss:0.02550\tvalid-mlogloss:1.91133\n",
      "[141]\ttrain-mlogloss:0.02535\tvalid-mlogloss:1.91199\n",
      "[142]\ttrain-mlogloss:0.02518\tvalid-mlogloss:1.91670\n",
      "[143]\ttrain-mlogloss:0.02498\tvalid-mlogloss:1.91875\n",
      "[144]\ttrain-mlogloss:0.02483\tvalid-mlogloss:1.91790\n",
      "[145]\ttrain-mlogloss:0.02469\tvalid-mlogloss:1.91777\n",
      "[146]\ttrain-mlogloss:0.02453\tvalid-mlogloss:1.91766\n",
      "[147]\ttrain-mlogloss:0.02438\tvalid-mlogloss:1.92249\n",
      "[148]\ttrain-mlogloss:0.02419\tvalid-mlogloss:1.92310\n",
      "[149]\ttrain-mlogloss:0.02404\tvalid-mlogloss:1.92497\n",
      "[150]\ttrain-mlogloss:0.02391\tvalid-mlogloss:1.92638\n",
      "[151]\ttrain-mlogloss:0.02378\tvalid-mlogloss:1.92667\n",
      "[152]\ttrain-mlogloss:0.02365\tvalid-mlogloss:1.92906\n",
      "[153]\ttrain-mlogloss:0.02349\tvalid-mlogloss:1.93044\n",
      "[154]\ttrain-mlogloss:0.02335\tvalid-mlogloss:1.93163\n",
      "[155]\ttrain-mlogloss:0.02322\tvalid-mlogloss:1.93381\n",
      "[156]\ttrain-mlogloss:0.02309\tvalid-mlogloss:1.93393\n",
      "[157]\ttrain-mlogloss:0.02297\tvalid-mlogloss:1.93527\n",
      "[158]\ttrain-mlogloss:0.02283\tvalid-mlogloss:1.93747\n",
      "[159]\ttrain-mlogloss:0.02270\tvalid-mlogloss:1.93965\n",
      "[160]\ttrain-mlogloss:0.02258\tvalid-mlogloss:1.94077\n",
      "[161]\ttrain-mlogloss:0.02245\tvalid-mlogloss:1.94393\n",
      "[162]\ttrain-mlogloss:0.02233\tvalid-mlogloss:1.94466\n",
      "[163]\ttrain-mlogloss:0.02222\tvalid-mlogloss:1.94454\n",
      "[164]\ttrain-mlogloss:0.02210\tvalid-mlogloss:1.94980\n",
      "[165]\ttrain-mlogloss:0.02199\tvalid-mlogloss:1.95093\n",
      "[166]\ttrain-mlogloss:0.02187\tvalid-mlogloss:1.95123\n",
      "[167]\ttrain-mlogloss:0.02177\tvalid-mlogloss:1.95279\n",
      "[168]\ttrain-mlogloss:0.02167\tvalid-mlogloss:1.95449\n",
      "[169]\ttrain-mlogloss:0.02155\tvalid-mlogloss:1.95425\n",
      "[170]\ttrain-mlogloss:0.02146\tvalid-mlogloss:1.95746\n",
      "[171]\ttrain-mlogloss:0.02137\tvalid-mlogloss:1.95858\n",
      "[172]\ttrain-mlogloss:0.02127\tvalid-mlogloss:1.95929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173]\ttrain-mlogloss:0.02115\tvalid-mlogloss:1.96005\n",
      "[174]\ttrain-mlogloss:0.02104\tvalid-mlogloss:1.96138\n",
      "[175]\ttrain-mlogloss:0.02095\tvalid-mlogloss:1.95884\n",
      "[176]\ttrain-mlogloss:0.02086\tvalid-mlogloss:1.95988\n",
      "[177]\ttrain-mlogloss:0.02077\tvalid-mlogloss:1.95918\n",
      "[178]\ttrain-mlogloss:0.02068\tvalid-mlogloss:1.96110\n",
      "[179]\ttrain-mlogloss:0.02059\tvalid-mlogloss:1.96336\n",
      "[180]\ttrain-mlogloss:0.02051\tvalid-mlogloss:1.96283\n",
      "[181]\ttrain-mlogloss:0.02041\tvalid-mlogloss:1.96331\n",
      "[182]\ttrain-mlogloss:0.02032\tvalid-mlogloss:1.96277\n",
      "[183]\ttrain-mlogloss:0.02024\tvalid-mlogloss:1.96184\n",
      "[184]\ttrain-mlogloss:0.02016\tvalid-mlogloss:1.96248\n",
      "[185]\ttrain-mlogloss:0.02008\tvalid-mlogloss:1.96122\n",
      "[186]\ttrain-mlogloss:0.02000\tvalid-mlogloss:1.96464\n",
      "[187]\ttrain-mlogloss:0.01992\tvalid-mlogloss:1.96615\n",
      "[188]\ttrain-mlogloss:0.01984\tvalid-mlogloss:1.96807\n",
      "[189]\ttrain-mlogloss:0.01977\tvalid-mlogloss:1.96688\n",
      "[190]\ttrain-mlogloss:0.01967\tvalid-mlogloss:1.96873\n",
      "[191]\ttrain-mlogloss:0.01959\tvalid-mlogloss:1.96958\n",
      "[192]\ttrain-mlogloss:0.01951\tvalid-mlogloss:1.97142\n",
      "[193]\ttrain-mlogloss:0.01943\tvalid-mlogloss:1.97109\n",
      "[194]\ttrain-mlogloss:0.01936\tvalid-mlogloss:1.97209\n",
      "[195]\ttrain-mlogloss:0.01928\tvalid-mlogloss:1.97401\n",
      "[196]\ttrain-mlogloss:0.01921\tvalid-mlogloss:1.97876\n",
      "[197]\ttrain-mlogloss:0.01914\tvalid-mlogloss:1.97881\n",
      "[198]\ttrain-mlogloss:0.01907\tvalid-mlogloss:1.97800\n",
      "[199]\ttrain-mlogloss:0.01899\tvalid-mlogloss:1.97910\n",
      "[200]\ttrain-mlogloss:0.01893\tvalid-mlogloss:1.97793\n",
      "[Fold 9/10]\n",
      "[Fold 9/10 Prediciton:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 1 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 2 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 2 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 3 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 3 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 4 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 4 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 5 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 5 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 6 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 6 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 7 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 7 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 8 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 8 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 9 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 9 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 10 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 10 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 11 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 11 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 12 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 12 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 13 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 13 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 14 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 14 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 15 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 15 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 16 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 16 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 17 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 17 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 18 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 18 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 19 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 19 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 20 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 20 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 21 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 21 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 22 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 22 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 23 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 23 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 24 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 24 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 25 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 25 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 26 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 26 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 27 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 27 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 28 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 28 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 29 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 29 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 30 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 30 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 31 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 31 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 32 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 32 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 33 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 33 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 34 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 34 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 35 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 35 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 36 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 36 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 37 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 37 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 38 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 38 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 39 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 39 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 40 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 40 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 41 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 41 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 42 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 42 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 43 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 43 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 44 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 44 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 45 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 45 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 46 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 46 [160/260 (56%)]\tTrain Loss: 0.472693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 47 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 47 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 48 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 48 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 49 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 49 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 50 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 50 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 51 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 51 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 52 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 52 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 53 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 53 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 54 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 54 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 55 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 55 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 56 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 56 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 57 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 57 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 58 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 58 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 59 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 59 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 60 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 60 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 61 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 61 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 62 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 62 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 63 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 63 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 64 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 64 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 65 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 65 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 66 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 66 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 67 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 67 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 68 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 68 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 69 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 69 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 70 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 70 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 71 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 71 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 72 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 72 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 73 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 73 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 74 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 74 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 75 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 75 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 76 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 76 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 77 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 77 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 78 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 78 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 79 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 79 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 80 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 80 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 81 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 81 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 82 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 82 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 83 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 83 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 84 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 84 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 85 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 85 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 86 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 86 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 87 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 87 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 88 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 88 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 89 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 89 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 90 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 90 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 91 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 91 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 92 [0/260 (0%)]\tTrain Loss: 0.421261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 92 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 93 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 93 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 94 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 94 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 95 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 95 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 96 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 96 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 97 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 97 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 98 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 98 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 99 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 99 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 100 [0/260 (0%)]\tTrain Loss: 0.421261\n",
      "Train Epoch: 100 [160/260 (56%)]\tTrain Loss: 0.472693\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.012891, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:45:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01494\tvalid-mlogloss:1.08834\n",
      "[1]\ttrain-mlogloss:0.93374\tvalid-mlogloss:1.07483\n",
      "[2]\ttrain-mlogloss:0.86263\tvalid-mlogloss:1.06309\n",
      "[3]\ttrain-mlogloss:0.79850\tvalid-mlogloss:1.05494\n",
      "[4]\ttrain-mlogloss:0.74094\tvalid-mlogloss:1.04462\n",
      "[5]\ttrain-mlogloss:0.68831\tvalid-mlogloss:1.01820\n",
      "[6]\ttrain-mlogloss:0.64268\tvalid-mlogloss:1.02023\n",
      "[7]\ttrain-mlogloss:0.60109\tvalid-mlogloss:1.00631\n",
      "[8]\ttrain-mlogloss:0.56494\tvalid-mlogloss:0.99857\n",
      "[9]\ttrain-mlogloss:0.52736\tvalid-mlogloss:0.98942\n",
      "[10]\ttrain-mlogloss:0.49382\tvalid-mlogloss:0.99159\n",
      "[11]\ttrain-mlogloss:0.46233\tvalid-mlogloss:0.99012\n",
      "[12]\ttrain-mlogloss:0.43320\tvalid-mlogloss:0.98782\n",
      "[13]\ttrain-mlogloss:0.40651\tvalid-mlogloss:0.99449\n",
      "[14]\ttrain-mlogloss:0.37992\tvalid-mlogloss:1.00028\n",
      "[15]\ttrain-mlogloss:0.35827\tvalid-mlogloss:1.01343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16]\ttrain-mlogloss:0.33615\tvalid-mlogloss:1.02171\n",
      "[17]\ttrain-mlogloss:0.31562\tvalid-mlogloss:1.02536\n",
      "[18]\ttrain-mlogloss:0.29775\tvalid-mlogloss:1.02766\n",
      "[19]\ttrain-mlogloss:0.28141\tvalid-mlogloss:1.02770\n",
      "[20]\ttrain-mlogloss:0.26603\tvalid-mlogloss:1.03655\n",
      "[21]\ttrain-mlogloss:0.25142\tvalid-mlogloss:1.04132\n",
      "[22]\ttrain-mlogloss:0.23918\tvalid-mlogloss:1.05068\n",
      "[23]\ttrain-mlogloss:0.22757\tvalid-mlogloss:1.05559\n",
      "[24]\ttrain-mlogloss:0.21595\tvalid-mlogloss:1.05409\n",
      "[25]\ttrain-mlogloss:0.20581\tvalid-mlogloss:1.05975\n",
      "[26]\ttrain-mlogloss:0.19570\tvalid-mlogloss:1.06561\n",
      "[27]\ttrain-mlogloss:0.18692\tvalid-mlogloss:1.06398\n",
      "[28]\ttrain-mlogloss:0.17868\tvalid-mlogloss:1.06597\n",
      "[29]\ttrain-mlogloss:0.17099\tvalid-mlogloss:1.06505\n",
      "[30]\ttrain-mlogloss:0.16367\tvalid-mlogloss:1.06087\n",
      "[31]\ttrain-mlogloss:0.15676\tvalid-mlogloss:1.06550\n",
      "[32]\ttrain-mlogloss:0.15052\tvalid-mlogloss:1.06917\n",
      "[33]\ttrain-mlogloss:0.14418\tvalid-mlogloss:1.06959\n",
      "[34]\ttrain-mlogloss:0.13878\tvalid-mlogloss:1.06506\n",
      "[35]\ttrain-mlogloss:0.13330\tvalid-mlogloss:1.06806\n",
      "[36]\ttrain-mlogloss:0.12859\tvalid-mlogloss:1.06651\n",
      "[37]\ttrain-mlogloss:0.12379\tvalid-mlogloss:1.06940\n",
      "[38]\ttrain-mlogloss:0.11946\tvalid-mlogloss:1.08138\n",
      "[39]\ttrain-mlogloss:0.11552\tvalid-mlogloss:1.07972\n",
      "[40]\ttrain-mlogloss:0.11197\tvalid-mlogloss:1.08110\n",
      "[41]\ttrain-mlogloss:0.10829\tvalid-mlogloss:1.08550\n",
      "[42]\ttrain-mlogloss:0.10494\tvalid-mlogloss:1.08716\n",
      "[43]\ttrain-mlogloss:0.10151\tvalid-mlogloss:1.08927\n",
      "[44]\ttrain-mlogloss:0.09842\tvalid-mlogloss:1.09183\n",
      "[45]\ttrain-mlogloss:0.09556\tvalid-mlogloss:1.09628\n",
      "[46]\ttrain-mlogloss:0.09258\tvalid-mlogloss:1.09673\n",
      "[47]\ttrain-mlogloss:0.08993\tvalid-mlogloss:1.09595\n",
      "[48]\ttrain-mlogloss:0.08741\tvalid-mlogloss:1.10032\n",
      "[49]\ttrain-mlogloss:0.08498\tvalid-mlogloss:1.10236\n",
      "[50]\ttrain-mlogloss:0.08272\tvalid-mlogloss:1.10202\n",
      "[51]\ttrain-mlogloss:0.08061\tvalid-mlogloss:1.10354\n",
      "[52]\ttrain-mlogloss:0.07848\tvalid-mlogloss:1.10041\n",
      "[53]\ttrain-mlogloss:0.07670\tvalid-mlogloss:1.10540\n",
      "[54]\ttrain-mlogloss:0.07472\tvalid-mlogloss:1.09747\n",
      "[55]\ttrain-mlogloss:0.07286\tvalid-mlogloss:1.09762\n",
      "[56]\ttrain-mlogloss:0.07115\tvalid-mlogloss:1.09903\n",
      "[57]\ttrain-mlogloss:0.06949\tvalid-mlogloss:1.10360\n",
      "[58]\ttrain-mlogloss:0.06795\tvalid-mlogloss:1.10549\n",
      "[59]\ttrain-mlogloss:0.06646\tvalid-mlogloss:1.10493\n",
      "[60]\ttrain-mlogloss:0.06508\tvalid-mlogloss:1.10143\n",
      "[61]\ttrain-mlogloss:0.06372\tvalid-mlogloss:1.10215\n",
      "[62]\ttrain-mlogloss:0.06247\tvalid-mlogloss:1.10427\n",
      "[63]\ttrain-mlogloss:0.06119\tvalid-mlogloss:1.10662\n",
      "[64]\ttrain-mlogloss:0.06002\tvalid-mlogloss:1.10666\n",
      "[65]\ttrain-mlogloss:0.05891\tvalid-mlogloss:1.10758\n",
      "[66]\ttrain-mlogloss:0.05780\tvalid-mlogloss:1.10759\n",
      "[67]\ttrain-mlogloss:0.05665\tvalid-mlogloss:1.11425\n",
      "[68]\ttrain-mlogloss:0.05562\tvalid-mlogloss:1.11560\n",
      "[69]\ttrain-mlogloss:0.05448\tvalid-mlogloss:1.11628\n",
      "[70]\ttrain-mlogloss:0.05356\tvalid-mlogloss:1.11952\n",
      "[71]\ttrain-mlogloss:0.05272\tvalid-mlogloss:1.11978\n",
      "[72]\ttrain-mlogloss:0.05171\tvalid-mlogloss:1.12383\n",
      "[73]\ttrain-mlogloss:0.05078\tvalid-mlogloss:1.12668\n",
      "[74]\ttrain-mlogloss:0.04995\tvalid-mlogloss:1.13085\n",
      "[75]\ttrain-mlogloss:0.04913\tvalid-mlogloss:1.13093\n",
      "[76]\ttrain-mlogloss:0.04835\tvalid-mlogloss:1.13144\n",
      "[77]\ttrain-mlogloss:0.04761\tvalid-mlogloss:1.13190\n",
      "[78]\ttrain-mlogloss:0.04682\tvalid-mlogloss:1.13398\n",
      "[79]\ttrain-mlogloss:0.04610\tvalid-mlogloss:1.13539\n",
      "[80]\ttrain-mlogloss:0.04538\tvalid-mlogloss:1.13889\n",
      "[81]\ttrain-mlogloss:0.04477\tvalid-mlogloss:1.14159\n",
      "[82]\ttrain-mlogloss:0.04413\tvalid-mlogloss:1.14318\n",
      "[83]\ttrain-mlogloss:0.04348\tvalid-mlogloss:1.14209\n",
      "[84]\ttrain-mlogloss:0.04285\tvalid-mlogloss:1.14246\n",
      "[85]\ttrain-mlogloss:0.04232\tvalid-mlogloss:1.14385\n",
      "[86]\ttrain-mlogloss:0.04173\tvalid-mlogloss:1.14382\n",
      "[87]\ttrain-mlogloss:0.04119\tvalid-mlogloss:1.14520\n",
      "[88]\ttrain-mlogloss:0.04067\tvalid-mlogloss:1.14582\n",
      "[89]\ttrain-mlogloss:0.04018\tvalid-mlogloss:1.14917\n",
      "[90]\ttrain-mlogloss:0.03963\tvalid-mlogloss:1.15152\n",
      "[91]\ttrain-mlogloss:0.03910\tvalid-mlogloss:1.15549\n",
      "[92]\ttrain-mlogloss:0.03868\tvalid-mlogloss:1.15788\n",
      "[93]\ttrain-mlogloss:0.03818\tvalid-mlogloss:1.16047\n",
      "[94]\ttrain-mlogloss:0.03771\tvalid-mlogloss:1.16407\n",
      "[95]\ttrain-mlogloss:0.03725\tvalid-mlogloss:1.16693\n",
      "[96]\ttrain-mlogloss:0.03681\tvalid-mlogloss:1.17149\n",
      "[97]\ttrain-mlogloss:0.03641\tvalid-mlogloss:1.17340\n",
      "[98]\ttrain-mlogloss:0.03603\tvalid-mlogloss:1.17631\n",
      "[99]\ttrain-mlogloss:0.03562\tvalid-mlogloss:1.18008\n",
      "[100]\ttrain-mlogloss:0.03527\tvalid-mlogloss:1.18040\n",
      "[101]\ttrain-mlogloss:0.03488\tvalid-mlogloss:1.18197\n",
      "[102]\ttrain-mlogloss:0.03454\tvalid-mlogloss:1.18654\n",
      "[103]\ttrain-mlogloss:0.03421\tvalid-mlogloss:1.18762\n",
      "[104]\ttrain-mlogloss:0.03386\tvalid-mlogloss:1.18636\n",
      "[105]\ttrain-mlogloss:0.03355\tvalid-mlogloss:1.19015\n",
      "[106]\ttrain-mlogloss:0.03325\tvalid-mlogloss:1.19010\n",
      "[107]\ttrain-mlogloss:0.03291\tvalid-mlogloss:1.18801\n",
      "[108]\ttrain-mlogloss:0.03259\tvalid-mlogloss:1.18767\n",
      "[109]\ttrain-mlogloss:0.03229\tvalid-mlogloss:1.19246\n",
      "[110]\ttrain-mlogloss:0.03196\tvalid-mlogloss:1.19396\n",
      "[111]\ttrain-mlogloss:0.03164\tvalid-mlogloss:1.19508\n",
      "[112]\ttrain-mlogloss:0.03133\tvalid-mlogloss:1.19676\n",
      "[113]\ttrain-mlogloss:0.03103\tvalid-mlogloss:1.19979\n",
      "[114]\ttrain-mlogloss:0.03078\tvalid-mlogloss:1.20015\n",
      "[115]\ttrain-mlogloss:0.03048\tvalid-mlogloss:1.20412\n",
      "[116]\ttrain-mlogloss:0.03022\tvalid-mlogloss:1.20675\n",
      "[117]\ttrain-mlogloss:0.02993\tvalid-mlogloss:1.20867\n",
      "[118]\ttrain-mlogloss:0.02964\tvalid-mlogloss:1.21402\n",
      "[119]\ttrain-mlogloss:0.02940\tvalid-mlogloss:1.21473\n",
      "[120]\ttrain-mlogloss:0.02917\tvalid-mlogloss:1.21772\n",
      "[121]\ttrain-mlogloss:0.02896\tvalid-mlogloss:1.21634\n",
      "[122]\ttrain-mlogloss:0.02872\tvalid-mlogloss:1.21645\n",
      "[123]\ttrain-mlogloss:0.02851\tvalid-mlogloss:1.22035\n",
      "[124]\ttrain-mlogloss:0.02830\tvalid-mlogloss:1.22021\n",
      "[125]\ttrain-mlogloss:0.02806\tvalid-mlogloss:1.22375\n",
      "[126]\ttrain-mlogloss:0.02784\tvalid-mlogloss:1.22347\n",
      "[127]\ttrain-mlogloss:0.02762\tvalid-mlogloss:1.22607\n",
      "[128]\ttrain-mlogloss:0.02738\tvalid-mlogloss:1.22850\n",
      "[129]\ttrain-mlogloss:0.02717\tvalid-mlogloss:1.22923\n",
      "[130]\ttrain-mlogloss:0.02696\tvalid-mlogloss:1.23081\n",
      "[131]\ttrain-mlogloss:0.02675\tvalid-mlogloss:1.23184\n",
      "[132]\ttrain-mlogloss:0.02658\tvalid-mlogloss:1.23321\n",
      "[133]\ttrain-mlogloss:0.02640\tvalid-mlogloss:1.23644\n",
      "[134]\ttrain-mlogloss:0.02621\tvalid-mlogloss:1.23565\n",
      "[135]\ttrain-mlogloss:0.02603\tvalid-mlogloss:1.23458\n",
      "[136]\ttrain-mlogloss:0.02584\tvalid-mlogloss:1.23789\n",
      "[137]\ttrain-mlogloss:0.02566\tvalid-mlogloss:1.24028\n",
      "[138]\ttrain-mlogloss:0.02549\tvalid-mlogloss:1.24185\n",
      "[139]\ttrain-mlogloss:0.02531\tvalid-mlogloss:1.24085\n",
      "[140]\ttrain-mlogloss:0.02516\tvalid-mlogloss:1.24174\n",
      "[141]\ttrain-mlogloss:0.02499\tvalid-mlogloss:1.24392\n",
      "[142]\ttrain-mlogloss:0.02482\tvalid-mlogloss:1.24376\n",
      "[143]\ttrain-mlogloss:0.02468\tvalid-mlogloss:1.24425\n",
      "[144]\ttrain-mlogloss:0.02453\tvalid-mlogloss:1.24475\n",
      "[145]\ttrain-mlogloss:0.02438\tvalid-mlogloss:1.24779\n",
      "[146]\ttrain-mlogloss:0.02425\tvalid-mlogloss:1.24662\n",
      "[147]\ttrain-mlogloss:0.02409\tvalid-mlogloss:1.24737\n",
      "[148]\ttrain-mlogloss:0.02392\tvalid-mlogloss:1.24806\n",
      "[149]\ttrain-mlogloss:0.02376\tvalid-mlogloss:1.25047\n",
      "[150]\ttrain-mlogloss:0.02362\tvalid-mlogloss:1.24911\n",
      "[151]\ttrain-mlogloss:0.02347\tvalid-mlogloss:1.24822\n",
      "[152]\ttrain-mlogloss:0.02334\tvalid-mlogloss:1.24924\n",
      "[153]\ttrain-mlogloss:0.02323\tvalid-mlogloss:1.25024\n",
      "[154]\ttrain-mlogloss:0.02307\tvalid-mlogloss:1.25368\n",
      "[155]\ttrain-mlogloss:0.02295\tvalid-mlogloss:1.25453\n",
      "[156]\ttrain-mlogloss:0.02280\tvalid-mlogloss:1.25558\n",
      "[157]\ttrain-mlogloss:0.02269\tvalid-mlogloss:1.25462\n",
      "[158]\ttrain-mlogloss:0.02255\tvalid-mlogloss:1.25673\n",
      "[159]\ttrain-mlogloss:0.02244\tvalid-mlogloss:1.25564\n",
      "[160]\ttrain-mlogloss:0.02231\tvalid-mlogloss:1.25810\n",
      "[161]\ttrain-mlogloss:0.02218\tvalid-mlogloss:1.25862\n",
      "[162]\ttrain-mlogloss:0.02208\tvalid-mlogloss:1.25997\n",
      "[163]\ttrain-mlogloss:0.02196\tvalid-mlogloss:1.26093\n",
      "[164]\ttrain-mlogloss:0.02183\tvalid-mlogloss:1.26347\n",
      "[165]\ttrain-mlogloss:0.02171\tvalid-mlogloss:1.26304\n",
      "[166]\ttrain-mlogloss:0.02159\tvalid-mlogloss:1.26483\n",
      "[167]\ttrain-mlogloss:0.02148\tvalid-mlogloss:1.26502\n",
      "[168]\ttrain-mlogloss:0.02138\tvalid-mlogloss:1.26713\n",
      "[169]\ttrain-mlogloss:0.02127\tvalid-mlogloss:1.26682\n",
      "[170]\ttrain-mlogloss:0.02116\tvalid-mlogloss:1.27051\n",
      "[171]\ttrain-mlogloss:0.02105\tvalid-mlogloss:1.26928\n",
      "[172]\ttrain-mlogloss:0.02096\tvalid-mlogloss:1.26916\n",
      "[173]\ttrain-mlogloss:0.02086\tvalid-mlogloss:1.26683\n",
      "[174]\ttrain-mlogloss:0.02076\tvalid-mlogloss:1.26532\n",
      "[175]\ttrain-mlogloss:0.02068\tvalid-mlogloss:1.26770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176]\ttrain-mlogloss:0.02058\tvalid-mlogloss:1.26811\n",
      "[177]\ttrain-mlogloss:0.02048\tvalid-mlogloss:1.26861\n",
      "[178]\ttrain-mlogloss:0.02038\tvalid-mlogloss:1.27012\n",
      "[179]\ttrain-mlogloss:0.02029\tvalid-mlogloss:1.26959\n",
      "[180]\ttrain-mlogloss:0.02019\tvalid-mlogloss:1.26892\n",
      "[181]\ttrain-mlogloss:0.02011\tvalid-mlogloss:1.26963\n",
      "[182]\ttrain-mlogloss:0.02002\tvalid-mlogloss:1.27117\n",
      "[183]\ttrain-mlogloss:0.01994\tvalid-mlogloss:1.27230\n",
      "[184]\ttrain-mlogloss:0.01985\tvalid-mlogloss:1.27503\n",
      "[185]\ttrain-mlogloss:0.01978\tvalid-mlogloss:1.27672\n",
      "[186]\ttrain-mlogloss:0.01968\tvalid-mlogloss:1.27810\n",
      "[187]\ttrain-mlogloss:0.01959\tvalid-mlogloss:1.27744\n",
      "[188]\ttrain-mlogloss:0.01952\tvalid-mlogloss:1.27710\n",
      "[189]\ttrain-mlogloss:0.01944\tvalid-mlogloss:1.27914\n",
      "[190]\ttrain-mlogloss:0.01936\tvalid-mlogloss:1.28052\n",
      "[191]\ttrain-mlogloss:0.01928\tvalid-mlogloss:1.28012\n",
      "[192]\ttrain-mlogloss:0.01920\tvalid-mlogloss:1.28008\n",
      "[193]\ttrain-mlogloss:0.01911\tvalid-mlogloss:1.28088\n",
      "[194]\ttrain-mlogloss:0.01904\tvalid-mlogloss:1.27984\n",
      "[195]\ttrain-mlogloss:0.01896\tvalid-mlogloss:1.28150\n",
      "[196]\ttrain-mlogloss:0.01888\tvalid-mlogloss:1.28295\n",
      "[197]\ttrain-mlogloss:0.01880\tvalid-mlogloss:1.28382\n",
      "[198]\ttrain-mlogloss:0.01873\tvalid-mlogloss:1.28454\n",
      "[199]\ttrain-mlogloss:0.01866\tvalid-mlogloss:1.28401\n",
      "[200]\ttrain-mlogloss:0.01859\tvalid-mlogloss:1.28632\n",
      "[201]\ttrain-mlogloss:0.01851\tvalid-mlogloss:1.28633\n",
      "[202]\ttrain-mlogloss:0.01844\tvalid-mlogloss:1.28650\n",
      "[203]\ttrain-mlogloss:0.01838\tvalid-mlogloss:1.28617\n",
      "[204]\ttrain-mlogloss:0.01832\tvalid-mlogloss:1.28762\n",
      "[205]\ttrain-mlogloss:0.01825\tvalid-mlogloss:1.28752\n",
      "[206]\ttrain-mlogloss:0.01818\tvalid-mlogloss:1.28828\n",
      "[207]\ttrain-mlogloss:0.01811\tvalid-mlogloss:1.28937\n",
      "[208]\ttrain-mlogloss:0.01806\tvalid-mlogloss:1.28970\n",
      "[209]\ttrain-mlogloss:0.01799\tvalid-mlogloss:1.29094\n",
      "[210]\ttrain-mlogloss:0.01793\tvalid-mlogloss:1.29217\n",
      "[211]\ttrain-mlogloss:0.01786\tvalid-mlogloss:1.29261\n",
      "[Fold 10/10]\n",
      "[Fold 10/10 Prediciton:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 1 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 1], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 2 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 2 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 2], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 3 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 3 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 3], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 4 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 4 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 4], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 5 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 5 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 5], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 6 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 6 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 6], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 7 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 7 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 7], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 8 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 8 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 8], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 9 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 9 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 9], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 10 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 10 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 10], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 11 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 11 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 11], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 12 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 12 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 12], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 13 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 13 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 13], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 14 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 14 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 14], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 15 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 15 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 15], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 16 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 16 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 16], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 17 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 17 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 17], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 18 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 18 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 18], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 19 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 19 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 19], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 20 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 20 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 20], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 21 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 21 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 21], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 22 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 22 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 22], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 23 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 23 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 23], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 24 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 24 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 24], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 25 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 25 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 25], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 26 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 26 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 26], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 27 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 27 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 27], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 28 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 28 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 28], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 29 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 29 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 29], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 30 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 30 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 30], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 31 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 31 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 31], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 32 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 32 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 32], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 33 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 33 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 33], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 34 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 34 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 34], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 35 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 35 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 35], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 36 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 36 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 36], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 37 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 37 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 37], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 38 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 38 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 38], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 39 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 39 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 39], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 40 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 40 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 40], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 41 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 41 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 41], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 42 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 42 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 42], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 43 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 43 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 43], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 44 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 44 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 44], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 45 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 45 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 45], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 46 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 46 [160/260 (56%)]\tTrain Loss: 0.422956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH: 46], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 47 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 47 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 47], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 48 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 48 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 48], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 49 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 49 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 49], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 50 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 50 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 50], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 51 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 51 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 51], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 52 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 52 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 52], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 53 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 53 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 53], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 54 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 54 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 54], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 55 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 55 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 55], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 56 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 56 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 56], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 57 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 57 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 57], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 58 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 58 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 58], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 59 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 59 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 59], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 60 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 60 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 60], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 61 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 61 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 61], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 62 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 62 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 62], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 63 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 63 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 63], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 64 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 64 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 64], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 65 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 65 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 65], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 66 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 66 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 66], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 67 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 67 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 67], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 68 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 68 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 68], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 69 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 69 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 69], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 70 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 70 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 70], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 71 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 71 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 71], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 72 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 72 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 72], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 73 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 73 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 73], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 74 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 74 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 74], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 75 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 75 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 75], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 76 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 76 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 76], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 77 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 77 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 77], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 78 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 78 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 78], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 79 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 79 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 79], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 80 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 80 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 80], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 81 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 81 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 81], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 82 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 82 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 82], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 83 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 83 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 83], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 84 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 84 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 84], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 85 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 85 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 85], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 86 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 86 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 86], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 87 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 87 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 87], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 88 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 88 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 88], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 89 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 89 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 89], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 90 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 90 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 90], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 91 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 91 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 91], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 92 [0/260 (0%)]\tTrain Loss: 0.394448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 92 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 92], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 93 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 93 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 93], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 94 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 94 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 94], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 95 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 95 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 95], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 96 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 96 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 96], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 97 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 97 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 97], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 98 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 98 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 98], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 99 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 99 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 99], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "Train Epoch: 100 [0/260 (0%)]\tTrain Loss: 0.394448\n",
      "Train Epoch: 100 [160/260 (56%)]\tTrain Loss: 0.422956\n",
      "\n",
      "[EPOCH: 100], \tValidation Loss: 0.011116, \tValidation Accuracy: 89.285714 % \n",
      "\n",
      "\n",
      "Test Loss: 0.0301, \tTest Accuracy: 63.6364 % \n",
      "\n",
      "[16:47:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping\", \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.00883\tvalid-mlogloss:1.06986\n",
      "[1]\ttrain-mlogloss:0.93253\tvalid-mlogloss:1.06159\n",
      "[2]\ttrain-mlogloss:0.86165\tvalid-mlogloss:1.05488\n",
      "[3]\ttrain-mlogloss:0.79980\tvalid-mlogloss:1.05150\n",
      "[4]\ttrain-mlogloss:0.74580\tvalid-mlogloss:1.04286\n",
      "[5]\ttrain-mlogloss:0.69376\tvalid-mlogloss:1.04184\n",
      "[6]\ttrain-mlogloss:0.64951\tvalid-mlogloss:1.04631\n",
      "[7]\ttrain-mlogloss:0.60750\tvalid-mlogloss:1.03742\n",
      "[8]\ttrain-mlogloss:0.56736\tvalid-mlogloss:1.04453\n",
      "[9]\ttrain-mlogloss:0.53160\tvalid-mlogloss:1.03869\n",
      "[10]\ttrain-mlogloss:0.49817\tvalid-mlogloss:1.04127\n",
      "[11]\ttrain-mlogloss:0.46559\tvalid-mlogloss:1.04289\n",
      "[12]\ttrain-mlogloss:0.43642\tvalid-mlogloss:1.04646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\ttrain-mlogloss:0.41046\tvalid-mlogloss:1.04807\n",
      "[14]\ttrain-mlogloss:0.38515\tvalid-mlogloss:1.04252\n",
      "[15]\ttrain-mlogloss:0.36222\tvalid-mlogloss:1.03738\n",
      "[16]\ttrain-mlogloss:0.34097\tvalid-mlogloss:1.04415\n",
      "[17]\ttrain-mlogloss:0.32099\tvalid-mlogloss:1.04858\n",
      "[18]\ttrain-mlogloss:0.30323\tvalid-mlogloss:1.05411\n",
      "[19]\ttrain-mlogloss:0.28647\tvalid-mlogloss:1.05912\n",
      "[20]\ttrain-mlogloss:0.27178\tvalid-mlogloss:1.06573\n",
      "[21]\ttrain-mlogloss:0.25709\tvalid-mlogloss:1.06863\n",
      "[22]\ttrain-mlogloss:0.24492\tvalid-mlogloss:1.08493\n",
      "[23]\ttrain-mlogloss:0.23355\tvalid-mlogloss:1.08907\n",
      "[24]\ttrain-mlogloss:0.22174\tvalid-mlogloss:1.09329\n",
      "[25]\ttrain-mlogloss:0.21158\tvalid-mlogloss:1.09636\n",
      "[26]\ttrain-mlogloss:0.20149\tvalid-mlogloss:1.09535\n",
      "[27]\ttrain-mlogloss:0.19221\tvalid-mlogloss:1.10041\n",
      "[28]\ttrain-mlogloss:0.18355\tvalid-mlogloss:1.09925\n",
      "[29]\ttrain-mlogloss:0.17538\tvalid-mlogloss:1.09532\n",
      "[30]\ttrain-mlogloss:0.16754\tvalid-mlogloss:1.09555\n",
      "[31]\ttrain-mlogloss:0.16043\tvalid-mlogloss:1.10013\n",
      "[32]\ttrain-mlogloss:0.15358\tvalid-mlogloss:1.10278\n",
      "[33]\ttrain-mlogloss:0.14769\tvalid-mlogloss:1.11144\n",
      "[34]\ttrain-mlogloss:0.14176\tvalid-mlogloss:1.11168\n",
      "[35]\ttrain-mlogloss:0.13601\tvalid-mlogloss:1.12217\n",
      "[36]\ttrain-mlogloss:0.13095\tvalid-mlogloss:1.12727\n",
      "[37]\ttrain-mlogloss:0.12632\tvalid-mlogloss:1.13403\n",
      "[38]\ttrain-mlogloss:0.12157\tvalid-mlogloss:1.14403\n",
      "[39]\ttrain-mlogloss:0.11745\tvalid-mlogloss:1.14913\n",
      "[40]\ttrain-mlogloss:0.11334\tvalid-mlogloss:1.15583\n",
      "[41]\ttrain-mlogloss:0.10976\tvalid-mlogloss:1.15831\n",
      "[42]\ttrain-mlogloss:0.10613\tvalid-mlogloss:1.16884\n",
      "[43]\ttrain-mlogloss:0.10279\tvalid-mlogloss:1.16938\n",
      "[44]\ttrain-mlogloss:0.09956\tvalid-mlogloss:1.17911\n",
      "[45]\ttrain-mlogloss:0.09645\tvalid-mlogloss:1.18666\n",
      "[46]\ttrain-mlogloss:0.09354\tvalid-mlogloss:1.19541\n",
      "[47]\ttrain-mlogloss:0.09082\tvalid-mlogloss:1.20264\n",
      "[48]\ttrain-mlogloss:0.08819\tvalid-mlogloss:1.20703\n",
      "[49]\ttrain-mlogloss:0.08601\tvalid-mlogloss:1.21124\n",
      "[50]\ttrain-mlogloss:0.08384\tvalid-mlogloss:1.21605\n",
      "[51]\ttrain-mlogloss:0.08175\tvalid-mlogloss:1.21837\n",
      "[52]\ttrain-mlogloss:0.07984\tvalid-mlogloss:1.22117\n",
      "[53]\ttrain-mlogloss:0.07796\tvalid-mlogloss:1.22428\n",
      "[54]\ttrain-mlogloss:0.07608\tvalid-mlogloss:1.23047\n",
      "[55]\ttrain-mlogloss:0.07425\tvalid-mlogloss:1.23255\n",
      "[56]\ttrain-mlogloss:0.07250\tvalid-mlogloss:1.23399\n",
      "[57]\ttrain-mlogloss:0.07077\tvalid-mlogloss:1.24022\n",
      "[58]\ttrain-mlogloss:0.06918\tvalid-mlogloss:1.24193\n",
      "[59]\ttrain-mlogloss:0.06773\tvalid-mlogloss:1.24902\n",
      "[60]\ttrain-mlogloss:0.06628\tvalid-mlogloss:1.24906\n",
      "[61]\ttrain-mlogloss:0.06488\tvalid-mlogloss:1.24622\n",
      "[62]\ttrain-mlogloss:0.06362\tvalid-mlogloss:1.24528\n",
      "[63]\ttrain-mlogloss:0.06234\tvalid-mlogloss:1.24611\n",
      "[64]\ttrain-mlogloss:0.06107\tvalid-mlogloss:1.24978\n",
      "[65]\ttrain-mlogloss:0.05985\tvalid-mlogloss:1.24963\n",
      "[66]\ttrain-mlogloss:0.05879\tvalid-mlogloss:1.25389\n",
      "[67]\ttrain-mlogloss:0.05771\tvalid-mlogloss:1.25637\n",
      "[68]\ttrain-mlogloss:0.05668\tvalid-mlogloss:1.26135\n",
      "[69]\ttrain-mlogloss:0.05557\tvalid-mlogloss:1.26184\n",
      "[70]\ttrain-mlogloss:0.05448\tvalid-mlogloss:1.26533\n",
      "[71]\ttrain-mlogloss:0.05354\tvalid-mlogloss:1.26744\n",
      "[72]\ttrain-mlogloss:0.05259\tvalid-mlogloss:1.27179\n",
      "[73]\ttrain-mlogloss:0.05168\tvalid-mlogloss:1.27087\n",
      "[74]\ttrain-mlogloss:0.05074\tvalid-mlogloss:1.27457\n",
      "[75]\ttrain-mlogloss:0.04992\tvalid-mlogloss:1.27886\n",
      "[76]\ttrain-mlogloss:0.04911\tvalid-mlogloss:1.28111\n",
      "[77]\ttrain-mlogloss:0.04828\tvalid-mlogloss:1.28420\n",
      "[78]\ttrain-mlogloss:0.04755\tvalid-mlogloss:1.28140\n",
      "[79]\ttrain-mlogloss:0.04680\tvalid-mlogloss:1.28825\n",
      "[80]\ttrain-mlogloss:0.04616\tvalid-mlogloss:1.29022\n",
      "[81]\ttrain-mlogloss:0.04546\tvalid-mlogloss:1.29315\n",
      "[82]\ttrain-mlogloss:0.04482\tvalid-mlogloss:1.29234\n",
      "[83]\ttrain-mlogloss:0.04418\tvalid-mlogloss:1.29361\n",
      "[84]\ttrain-mlogloss:0.04360\tvalid-mlogloss:1.29520\n",
      "[85]\ttrain-mlogloss:0.04297\tvalid-mlogloss:1.29681\n",
      "[86]\ttrain-mlogloss:0.04243\tvalid-mlogloss:1.30011\n",
      "[87]\ttrain-mlogloss:0.04178\tvalid-mlogloss:1.30128\n",
      "[88]\ttrain-mlogloss:0.04127\tvalid-mlogloss:1.30481\n",
      "[89]\ttrain-mlogloss:0.04075\tvalid-mlogloss:1.30395\n",
      "[90]\ttrain-mlogloss:0.04026\tvalid-mlogloss:1.30271\n",
      "[91]\ttrain-mlogloss:0.03978\tvalid-mlogloss:1.30359\n",
      "[92]\ttrain-mlogloss:0.03926\tvalid-mlogloss:1.30853\n",
      "[93]\ttrain-mlogloss:0.03876\tvalid-mlogloss:1.30908\n",
      "[94]\ttrain-mlogloss:0.03827\tvalid-mlogloss:1.31212\n",
      "[95]\ttrain-mlogloss:0.03782\tvalid-mlogloss:1.31497\n",
      "[96]\ttrain-mlogloss:0.03736\tvalid-mlogloss:1.31394\n",
      "[97]\ttrain-mlogloss:0.03688\tvalid-mlogloss:1.31724\n",
      "[98]\ttrain-mlogloss:0.03643\tvalid-mlogloss:1.31483\n",
      "[99]\ttrain-mlogloss:0.03607\tvalid-mlogloss:1.31693\n",
      "[100]\ttrain-mlogloss:0.03572\tvalid-mlogloss:1.31604\n",
      "[101]\ttrain-mlogloss:0.03533\tvalid-mlogloss:1.31565\n",
      "[102]\ttrain-mlogloss:0.03497\tvalid-mlogloss:1.31758\n",
      "[103]\ttrain-mlogloss:0.03464\tvalid-mlogloss:1.31901\n",
      "[104]\ttrain-mlogloss:0.03428\tvalid-mlogloss:1.31880\n",
      "[105]\ttrain-mlogloss:0.03396\tvalid-mlogloss:1.31750\n",
      "[106]\ttrain-mlogloss:0.03362\tvalid-mlogloss:1.32072\n",
      "[107]\ttrain-mlogloss:0.03335\tvalid-mlogloss:1.31984\n",
      "[108]\ttrain-mlogloss:0.03301\tvalid-mlogloss:1.32105\n",
      "[109]\ttrain-mlogloss:0.03269\tvalid-mlogloss:1.32309\n",
      "[110]\ttrain-mlogloss:0.03237\tvalid-mlogloss:1.32507\n",
      "[111]\ttrain-mlogloss:0.03209\tvalid-mlogloss:1.32645\n",
      "[112]\ttrain-mlogloss:0.03181\tvalid-mlogloss:1.32607\n",
      "[113]\ttrain-mlogloss:0.03152\tvalid-mlogloss:1.32691\n",
      "[114]\ttrain-mlogloss:0.03120\tvalid-mlogloss:1.32627\n",
      "[115]\ttrain-mlogloss:0.03092\tvalid-mlogloss:1.33102\n",
      "[116]\ttrain-mlogloss:0.03066\tvalid-mlogloss:1.33226\n",
      "[117]\ttrain-mlogloss:0.03040\tvalid-mlogloss:1.33041\n",
      "[118]\ttrain-mlogloss:0.03013\tvalid-mlogloss:1.33062\n",
      "[119]\ttrain-mlogloss:0.02989\tvalid-mlogloss:1.33025\n",
      "[120]\ttrain-mlogloss:0.02966\tvalid-mlogloss:1.33422\n",
      "[121]\ttrain-mlogloss:0.02939\tvalid-mlogloss:1.33607\n",
      "[122]\ttrain-mlogloss:0.02916\tvalid-mlogloss:1.33564\n",
      "[123]\ttrain-mlogloss:0.02889\tvalid-mlogloss:1.34019\n",
      "[124]\ttrain-mlogloss:0.02867\tvalid-mlogloss:1.34021\n",
      "[125]\ttrain-mlogloss:0.02844\tvalid-mlogloss:1.34371\n",
      "[126]\ttrain-mlogloss:0.02820\tvalid-mlogloss:1.34414\n",
      "[127]\ttrain-mlogloss:0.02799\tvalid-mlogloss:1.34734\n",
      "[128]\ttrain-mlogloss:0.02781\tvalid-mlogloss:1.34805\n",
      "[129]\ttrain-mlogloss:0.02760\tvalid-mlogloss:1.35081\n",
      "[130]\ttrain-mlogloss:0.02737\tvalid-mlogloss:1.34974\n",
      "[131]\ttrain-mlogloss:0.02715\tvalid-mlogloss:1.35008\n",
      "[132]\ttrain-mlogloss:0.02695\tvalid-mlogloss:1.35138\n",
      "[133]\ttrain-mlogloss:0.02678\tvalid-mlogloss:1.35221\n",
      "[134]\ttrain-mlogloss:0.02657\tvalid-mlogloss:1.35680\n",
      "[135]\ttrain-mlogloss:0.02639\tvalid-mlogloss:1.35666\n",
      "[136]\ttrain-mlogloss:0.02620\tvalid-mlogloss:1.35778\n",
      "[137]\ttrain-mlogloss:0.02603\tvalid-mlogloss:1.36247\n",
      "[138]\ttrain-mlogloss:0.02583\tvalid-mlogloss:1.36374\n",
      "[139]\ttrain-mlogloss:0.02568\tvalid-mlogloss:1.36704\n",
      "[140]\ttrain-mlogloss:0.02550\tvalid-mlogloss:1.36595\n",
      "[141]\ttrain-mlogloss:0.02535\tvalid-mlogloss:1.36778\n",
      "[142]\ttrain-mlogloss:0.02517\tvalid-mlogloss:1.36853\n",
      "[143]\ttrain-mlogloss:0.02503\tvalid-mlogloss:1.37042\n",
      "[144]\ttrain-mlogloss:0.02488\tvalid-mlogloss:1.37518\n",
      "[145]\ttrain-mlogloss:0.02472\tvalid-mlogloss:1.37816\n",
      "[146]\ttrain-mlogloss:0.02456\tvalid-mlogloss:1.37864\n",
      "[147]\ttrain-mlogloss:0.02442\tvalid-mlogloss:1.37927\n",
      "[148]\ttrain-mlogloss:0.02427\tvalid-mlogloss:1.37890\n",
      "[149]\ttrain-mlogloss:0.02412\tvalid-mlogloss:1.38060\n",
      "[150]\ttrain-mlogloss:0.02397\tvalid-mlogloss:1.38000\n",
      "[151]\ttrain-mlogloss:0.02383\tvalid-mlogloss:1.37960\n",
      "[152]\ttrain-mlogloss:0.02369\tvalid-mlogloss:1.38242\n",
      "[153]\ttrain-mlogloss:0.02353\tvalid-mlogloss:1.38574\n",
      "[154]\ttrain-mlogloss:0.02339\tvalid-mlogloss:1.38530\n",
      "[155]\ttrain-mlogloss:0.02327\tvalid-mlogloss:1.38368\n",
      "[156]\ttrain-mlogloss:0.02313\tvalid-mlogloss:1.38388\n",
      "[157]\ttrain-mlogloss:0.02297\tvalid-mlogloss:1.38831\n",
      "[158]\ttrain-mlogloss:0.02285\tvalid-mlogloss:1.38950\n",
      "[159]\ttrain-mlogloss:0.02273\tvalid-mlogloss:1.39074\n",
      "[160]\ttrain-mlogloss:0.02262\tvalid-mlogloss:1.39008\n",
      "[161]\ttrain-mlogloss:0.02250\tvalid-mlogloss:1.39064\n",
      "[162]\ttrain-mlogloss:0.02238\tvalid-mlogloss:1.39035\n",
      "[163]\ttrain-mlogloss:0.02229\tvalid-mlogloss:1.39259\n",
      "[164]\ttrain-mlogloss:0.02217\tvalid-mlogloss:1.39380\n",
      "[165]\ttrain-mlogloss:0.02205\tvalid-mlogloss:1.39374\n",
      "[166]\ttrain-mlogloss:0.02193\tvalid-mlogloss:1.39409\n",
      "[167]\ttrain-mlogloss:0.02182\tvalid-mlogloss:1.39566\n",
      "[168]\ttrain-mlogloss:0.02172\tvalid-mlogloss:1.39692\n",
      "[169]\ttrain-mlogloss:0.02162\tvalid-mlogloss:1.39957\n",
      "[170]\ttrain-mlogloss:0.02151\tvalid-mlogloss:1.40039\n",
      "[171]\ttrain-mlogloss:0.02139\tvalid-mlogloss:1.40655\n",
      "[172]\ttrain-mlogloss:0.02127\tvalid-mlogloss:1.40579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173]\ttrain-mlogloss:0.02118\tvalid-mlogloss:1.40560\n",
      "[174]\ttrain-mlogloss:0.02109\tvalid-mlogloss:1.40400\n",
      "[175]\ttrain-mlogloss:0.02100\tvalid-mlogloss:1.40414\n",
      "[176]\ttrain-mlogloss:0.02090\tvalid-mlogloss:1.40619\n",
      "[177]\ttrain-mlogloss:0.02081\tvalid-mlogloss:1.40814\n",
      "[178]\ttrain-mlogloss:0.02072\tvalid-mlogloss:1.40793\n",
      "[179]\ttrain-mlogloss:0.02063\tvalid-mlogloss:1.40798\n",
      "[180]\ttrain-mlogloss:0.02054\tvalid-mlogloss:1.40793\n",
      "[181]\ttrain-mlogloss:0.02044\tvalid-mlogloss:1.40873\n",
      "[182]\ttrain-mlogloss:0.02035\tvalid-mlogloss:1.40879\n",
      "[183]\ttrain-mlogloss:0.02025\tvalid-mlogloss:1.40809\n",
      "[184]\ttrain-mlogloss:0.02017\tvalid-mlogloss:1.41010\n",
      "[185]\ttrain-mlogloss:0.02008\tvalid-mlogloss:1.41013\n",
      "[186]\ttrain-mlogloss:0.01998\tvalid-mlogloss:1.41075\n",
      "[187]\ttrain-mlogloss:0.01990\tvalid-mlogloss:1.41161\n",
      "[188]\ttrain-mlogloss:0.01983\tvalid-mlogloss:1.41267\n",
      "[189]\ttrain-mlogloss:0.01973\tvalid-mlogloss:1.41151\n",
      "[190]\ttrain-mlogloss:0.01965\tvalid-mlogloss:1.41268\n",
      "[191]\ttrain-mlogloss:0.01959\tvalid-mlogloss:1.41589\n",
      "[192]\ttrain-mlogloss:0.01950\tvalid-mlogloss:1.41746\n",
      "[193]\ttrain-mlogloss:0.01942\tvalid-mlogloss:1.41989\n",
      "[194]\ttrain-mlogloss:0.01934\tvalid-mlogloss:1.42290\n",
      "[195]\ttrain-mlogloss:0.01926\tvalid-mlogloss:1.42370\n",
      "[196]\ttrain-mlogloss:0.01918\tvalid-mlogloss:1.42471\n",
      "[197]\ttrain-mlogloss:0.01911\tvalid-mlogloss:1.42323\n",
      "[198]\ttrain-mlogloss:0.01902\tvalid-mlogloss:1.42559\n",
      "[199]\ttrain-mlogloss:0.01895\tvalid-mlogloss:1.42377\n",
      "[200]\ttrain-mlogloss:0.01887\tvalid-mlogloss:1.42346\n",
      "[201]\ttrain-mlogloss:0.01880\tvalid-mlogloss:1.42425\n",
      "[202]\ttrain-mlogloss:0.01872\tvalid-mlogloss:1.42418\n",
      "[203]\ttrain-mlogloss:0.01866\tvalid-mlogloss:1.42500\n",
      "[204]\ttrain-mlogloss:0.01859\tvalid-mlogloss:1.42443\n",
      "[205]\ttrain-mlogloss:0.01851\tvalid-mlogloss:1.42691\n",
      "[206]\ttrain-mlogloss:0.01843\tvalid-mlogloss:1.42614\n",
      "[207]\ttrain-mlogloss:0.01836\tvalid-mlogloss:1.42635\n",
      "[208]\ttrain-mlogloss:0.01829\tvalid-mlogloss:1.42619\n",
      "[209]\ttrain-mlogloss:0.01823\tvalid-mlogloss:1.42488\n",
      "[210]\ttrain-mlogloss:0.01815\tvalid-mlogloss:1.42718\n",
      "[211]\ttrain-mlogloss:0.01809\tvalid-mlogloss:1.42942\n",
      "[212]\ttrain-mlogloss:0.01801\tvalid-mlogloss:1.43223\n",
      "[213]\ttrain-mlogloss:0.01795\tvalid-mlogloss:1.43265\n",
      "[214]\ttrain-mlogloss:0.01789\tvalid-mlogloss:1.43438\n",
      "[215]\ttrain-mlogloss:0.01784\tvalid-mlogloss:1.43254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562499/850794805.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  addition = m(torch.tensor(addition)).detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "image_testlabel = []\n",
    "image_testpred = []\n",
    "image_testloss = []\n",
    "image_testacc = []\n",
    "image_testprob = []\n",
    "cv_acc = []\n",
    "finalprob = []\n",
    "finalpred = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(traindata1, trainidx)):\n",
    "    \n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    \n",
    "    \n",
    "    X_train, X_valid = traindata1[train_index], traindata1[val_index]\n",
    "    X_train2, X_valid2 = traindata2[train_index], traindata2[val_index]\n",
    "    y_train, y_valid = trainidx[train_index], trainidx[val_index]\n",
    "\n",
    "    \n",
    "    trainfinal = []\n",
    "    trainfinal2 = []\n",
    "\n",
    "    for j in range(X_train.shape[0]):\n",
    "      trainfinal.append((X_train[j], y_train[j]))\n",
    "      trainfinal2.append((X_train2[j], y_train[j]))\n",
    "\n",
    "\n",
    "    valfinal = []\n",
    "    valfinal2 = []\n",
    "\n",
    "    for j in range(X_valid.shape[0]):\n",
    "      valfinal.append((X_valid[j], y_valid[j]))\n",
    "      valfinal2.append((X_valid2[j], y_valid[j]))\n",
    "\n",
    "    trainloader = DataLoader(trainfinal, batch_size = batchsize, shuffle = False)\n",
    "    trainloader2 = DataLoader(trainfinal2, batch_size = batchsize, shuffle = False)\n",
    "    validloader = DataLoader(valfinal, batch_size = batchsize, shuffle = False)\n",
    "    validloader2 = DataLoader(valfinal2, batch_size = batchsize, shuffle = False)\n",
    "    \n",
    "    \n",
    "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n",
    "    \n",
    "\n",
    "    # train phase\n",
    "    epochval = []\n",
    "    valloss = []\n",
    "    valacc = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        trainprob, trainlabel = train(model, trainloader, optimizer, log_interval = 5)\n",
    "        valid_loss, valid_accuracy, validlabel, _, validprob = evaluate(model, validloader)\n",
    "        epochval.append(epoch)\n",
    "        valloss.append(valid_loss)\n",
    "        valacc.append(valid_accuracy)\n",
    "        print(\"\\n[EPOCH: {}], \\tValidation Loss: {:.6f}, \\tValidation Accuracy: {:.6f} % \\n\".format(\n",
    "            epoch, valid_loss, valid_accuracy))\n",
    "    \n",
    "\n",
    "    # test phase\n",
    "\n",
    "    test_loss, test_accuracy, label, pred, prob = evaluate(model, testloader)\n",
    "    image_testlabel.append(label)\n",
    "    image_testpred.append(pred)\n",
    "    image_testloss.append(test_loss)\n",
    "    image_testacc.append(test_accuracy)\n",
    "    image_testprob.append(prob)\n",
    "\n",
    "    print(\"\\nTest Loss: {:.4f}, \\tTest Accuracy: {:.4f} % \\n\".format(test_loss, test_accuracy))\n",
    "\n",
    "    ## train phase\n",
    "\n",
    "    vittrainprob = torch.cat((trainprob[0], trainprob[1], trainprob[2], trainprob[3], trainprob[4], trainprob[5], trainprob[6], trainprob[7], trainprob[8]),0).detach().cpu().numpy()\n",
    "    vittrainlabel = torch.cat((trainlabel[0], trainlabel[1], trainlabel[2], trainlabel[3], trainlabel[4], trainlabel[5], trainlabel[6], trainlabel[7], trainlabel[8]),0).detach().cpu().numpy()\n",
    "   \n",
    "    # genomic data\n",
    "    trainsnpdata, trainsnplabel = evaluate2(model, trainloader2)\n",
    "    a = trainsnpdata\n",
    "    b = trainsnplabel\n",
    "    \n",
    "    snpdatatrain = torch.cat((a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8]),0).detach().cpu().numpy()\n",
    "    trainlabelgiven = torch.cat((b[0], b[1], b[2], b[3], b[4], b[5], b[6], b[7], b[8]),0).detach().cpu().numpy()\n",
    "    \n",
    "    snp_data_train = pd.DataFrame(snpdatatrain)\n",
    "    snp_data_train['target'] = trainlabelgiven.reshape(len(trainlabelgiven), 1)\n",
    "\n",
    "    \n",
    "    assert (vittrainlabel == trainlabelgiven).all()\n",
    "    \n",
    "    # fusion_data_train = np.concatenate((vittrainprob, snpdatatrain), axis=1)\n",
    "    # fusion_data_train = pd.DataFrame(fusion_data_train)\n",
    "    # fusion_data_train['target'] = trainlabelgiven.reshape(len(trainlabelgiven), 1)\n",
    "\n",
    "    ## validation phase\n",
    "    vitvalidprob = validprob[0].detach().cpu().numpy()\n",
    "    vitvalidlabel = validlabel[0].detach().cpu().numpy()\n",
    "    \n",
    "    validsnpdata, validsnplabel = evaluate2(model, validloader2)\n",
    "    c = validsnpdata\n",
    "    d = validsnplabel\n",
    "    \n",
    "    snpdatavalid = c[0].detach().cpu().numpy()\n",
    "    validlabelgiven = d[0].detach().cpu().numpy()\n",
    "    \n",
    "    snp_data_valid = pd.DataFrame(snpdatavalid)\n",
    "    snp_data_valid['target'] = validlabelgiven.reshape(len(validlabelgiven), 1)\n",
    "    \n",
    "    assert (vitvalidlabel == validlabelgiven).all()\n",
    "    \n",
    "    # fusion_data_valid = np.concatenate((vitvalidprob, snpdatavalid), axis=1)\n",
    "    # fusion_data_valid = pd.DataFrame(fusion_data_valid)\n",
    "    # fusion_data_valid['target'] = validlabelgiven.reshape(len(validlabelgiven), 1)\n",
    "\n",
    "    ## test\n",
    "    vittestprob = np.concatenate((prob[0].detach().cpu().numpy(), prob[1].detach().cpu().numpy()))\n",
    "    vittestlabel = np.concatenate((label[0].detach().cpu().numpy(), label[1].detach().cpu().numpy()))\n",
    "    \n",
    "    testsnpdata, testsnplabel = evaluate2(model, testloader2)\n",
    "    \n",
    "    snpdatatest = np.concatenate((testsnpdata[0].detach().cpu().numpy(), testsnpdata[1].detach().cpu().numpy()))\n",
    "    testlabelgiven = np.concatenate((testsnplabel[0].detach().cpu().numpy(), testsnplabel[1].detach().cpu().numpy()))\n",
    "    \n",
    "    snp_data_test = pd.DataFrame(snpdatatest)\n",
    "    snp_data_test['target'] = testlabelgiven.reshape(len(testlabelgiven), 1)\n",
    "\n",
    "    assert (vittestlabel == testlabelgiven).all()\n",
    "\n",
    "    \n",
    "    # fusion_data_test = np.concatenate((vittestprob, snpdatatest), axis=1)\n",
    "    # fusion_data_test = pd.DataFrame(fusion_data_test)\n",
    "    # fusion_data_test['target'] = labeltestgiven.reshape(33, 1)\n",
    "\n",
    "\n",
    "    X_train_new, X_valid_new = snp_data_train.drop(['target'],axis=1), snp_data_valid.drop(['target'],axis=1)\n",
    "    y_train_new, y_valid_new = snp_data_train['target'].values, snp_data_valid['target'].values\n",
    "\n",
    "    ## xgboost\n",
    "    d_train = xgb.DMatrix(X_train_new, y_train_new)\n",
    "    d_valid = xgb.DMatrix(X_valid_new, y_valid_new)\n",
    "    d_test = xgb.DMatrix(snp_data_test.drop(['target'],axis=1), snp_data_test['target'].values)\n",
    "\n",
    "    wlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    xgb_model = xgb.train(params = params, dtrain = d_train, num_boost_round = num_rounds, early_stopping_rounds = 200, evals = wlist)\n",
    "\n",
    "\n",
    "    temp = xgb_model.predict(d_test)\n",
    "    addition = vittestprob + temp\n",
    "    m = torch.nn.Softmax()\n",
    "    addition = m(torch.tensor(addition)).detach().cpu().numpy()\n",
    "    finalprob.append(addition)\n",
    "    \n",
    "    temp2 = []\n",
    "    \n",
    "    for j in range(len(testidx)):\n",
    "      temp2.append(np.argsort(addition[j])[::-1][0])\n",
    "      \n",
    "    acc = np.round(accuracy_score(testidx, temp2), 4)\n",
    "    \n",
    "    cv_acc.append(acc)\n",
    "    finalpred.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55919177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7273, 0.697, 0.7273, 0.697, 0.697, 0.697, 0.7576, 0.697, 0.6667, 0.7273]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "203fcc38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70912"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44e582f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2 = finalprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e36a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b707208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(y_real, y_pred):\n",
    "    '''\n",
    "    Calculates the True Positive Rate (tpr) and the True Negative Rate (fpr) based on real and predicted observations\n",
    "    \n",
    "    Args:\n",
    "        y_real: The list or series with the real classes\n",
    "        y_pred: The list or series with the predicted classes\n",
    "        \n",
    "    Returns:\n",
    "        tpr: The True Positive Rate of the classifier\n",
    "        fpr: The False Positive Rate of the classifier\n",
    "    '''\n",
    "    \n",
    "    # Calculates the confusion matrix and recover each element\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "    \n",
    "    # Calculates tpr and fpr\n",
    "    tpr =  TP/(TP + FN) # sensitivity - true positive rate\n",
    "    fpr = 1 - TN/(TN+FP) # 1-specificity - false positive rate\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34c451ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_roc_coordinates(y_real, y_proba):\n",
    "    '''\n",
    "    Calculates all the ROC Curve coordinates (tpr and fpr) by considering each point as a treshold for the predicion of the class.\n",
    "    \n",
    "    Args:\n",
    "        y_real: The list or series with the real classes.\n",
    "        y_proba: The array with the probabilities for each class, obtained by using the `.predict_proba()` method.\n",
    "        \n",
    "    Returns:\n",
    "        tpr_list: The list of TPRs representing each threshold.\n",
    "        fpr_list: The list of FPRs representing each threshold.\n",
    "    '''\n",
    "    tpr_list = [0, 1]\n",
    "    fpr_list = [0, 1]\n",
    "    for i in range(len(y_proba)):\n",
    "        threshold = y_proba[i]\n",
    "        y_pred = y_proba >= threshold\n",
    "        tpr, fpr = calculate_tpr_fpr(y_real, y_pred)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    return tpr_list, fpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2a6e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_blue(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax, ci=None, color='skyblue', alpha = 0.05)\n",
    "    sns.lineplot(x = [0, 1], y = [0, 1], color = 'black', ax = ax, alpha = 0.05, linestyle='--')\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4775a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_red(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax, ci=None, color='red', alpha = 0.05)\n",
    "    sns.lineplot(x = [0, 1], y = [0, 1], color = 'black', ax = ax, alpha = 0.05, linestyle='--')\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1acac667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_green(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax, ci=None, color='green', alpha = 0.05)\n",
    "    sns.lineplot(x = [0, 1], y = [0, 1], color = 'black', ax = ax, alpha = 0.05, linestyle='--')\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ce6cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_blue_2(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax, ci=None, color='blue', alpha = 1)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7d375b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_red_2(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax, ci=None, color='red', alpha = 1)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f6aea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_green_2(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax, ci=None, color='green', alpha = 1)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3fd6bf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [0, 1, 2]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01dfa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_names = ['Control', 'MDD', 'Bipolar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a52b630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "528c3813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "roc_auc_ovr = {}\n",
    "auc = []\n",
    "    \n",
    "for i in range(len(classes)):\n",
    "    \n",
    "    temptpr = []\n",
    "    tempfpr = []\n",
    "    \n",
    "    for j in range(10):\n",
    "\n",
    "        c = classes[i]\n",
    "\n",
    "        df_aux = pd.DataFrame(testdata).copy()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in testidx]\n",
    "        df_aux['prob'] = np.array(prob2)[j][:, i]\n",
    "        df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "        ax_bottom = plt.subplot(2, 3, i+4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "\n",
    "        temptpr.append(tpr)\n",
    "        tempfpr.append(fpr)\n",
    "        \n",
    "        plot_roc_curve_blue(tpr, fpr, scatter = False, ax = ax_bottom)\n",
    "        \n",
    "        \n",
    "        ax_bottom.set_title(f\"ROC Curve OvR: {classes_names[c]} vs. Rest\")\n",
    "\n",
    "        auc.append(roc_auc_score(df_aux['class'], df_aux['prob']))\n",
    "    \n",
    "    temptpr = np.array(temptpr)\n",
    "    tempfpr = np.array(tempfpr)\n",
    "    \n",
    "    num = 9 + i * 10\n",
    "    prev = 9 + (i-1)*10\n",
    "    plt.plot(np.sort(np.array(pd.DataFrame(tempfpr).mean())), np.sort(np.array(pd.DataFrame(temptpr).mean())), 'b')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f23a2981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "roc_auc_ovr = {}\n",
    "auc = []\n",
    "    \n",
    "for i in range(len(classes)):\n",
    "    \n",
    "    temptpr = []\n",
    "    tempfpr = []\n",
    "    \n",
    "    for j in range(10):\n",
    "\n",
    "        c = classes[i]\n",
    "\n",
    "        df_aux = pd.DataFrame(testdata).copy()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in testidx]\n",
    "        df_aux['prob'] = np.array(prob2)[j][:, i]\n",
    "        df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "        ax_bottom = plt.subplot(2, 3, i+4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "\n",
    "        temptpr.append(tpr)\n",
    "        tempfpr.append(fpr)\n",
    "        \n",
    "        plot_roc_curve_red(tpr, fpr, scatter = False, ax = ax_bottom)\n",
    "        \n",
    "        \n",
    "        ax_bottom.set_title(f\"ROC Curve OvR: {classes_names[c]} vs. Rest\")\n",
    "\n",
    "        auc.append(roc_auc_score(df_aux['class'], df_aux['prob']))\n",
    "    \n",
    "    temptpr = np.array(temptpr)\n",
    "    tempfpr = np.array(tempfpr)\n",
    "    \n",
    "    num = 9 + i * 10\n",
    "    prev = 9 + (i-1)*10\n",
    "    plt.plot(np.sort(np.array(pd.DataFrame(tempfpr).mean())), np.sort(np.array(pd.DataFrame(temptpr).mean())), 'r')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f792686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  values = np.array([convert(v) for v in values])\n",
      "/home/koreagen/koreagen/lib/python3.8/site-packages/pandas/core/internals/construction.py:568: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "roc_auc_ovr = {}\n",
    "auc = []\n",
    "    \n",
    "for i in range(len(classes)):\n",
    "    \n",
    "    temptpr = []\n",
    "    tempfpr = []\n",
    "    \n",
    "    for j in range(10):\n",
    "\n",
    "        c = classes[i]\n",
    "\n",
    "        df_aux = pd.DataFrame(testdata).copy()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in testidx]\n",
    "        df_aux['prob'] = np.array(prob2)[j][:, i]\n",
    "        df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "        ax_bottom = plt.subplot(2, 3, i+4)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "        \n",
    "        temptpr.append(tpr)\n",
    "        tempfpr.append(fpr)\n",
    "        \n",
    "        plot_roc_curve_green(tpr, fpr, scatter = False, ax = ax_bottom)\n",
    "        \n",
    "        \n",
    "        ax_bottom.set_title(f\"ROC Curve OvR: {classes_names[c]} vs. Rest\")\n",
    "\n",
    "        auc.append(roc_auc_score(df_aux['class'], df_aux['prob']))\n",
    "    \n",
    "    temptpr = np.array(temptpr)\n",
    "    tempfpr = np.array(tempfpr)\n",
    "    \n",
    "    num = 9 + i * 10\n",
    "    prev = 9 + (i-1)*10\n",
    "    plt.plot(np.sort(np.array(pd.DataFrame(tempfpr).mean())), np.sort(np.array(pd.DataFrame(temptpr).mean())), 'g')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73feb11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8152892561983471"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrlauc = np.mean(auc[:10])\n",
    "ctrlauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e470002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066176470588235"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mddauc = np.mean(auc[10:20])\n",
    "mddauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a2c2440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492857142857142"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipauc = np.mean(auc[20:30])\n",
    "bipauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "184b54a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903975391809617"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalauc = [ctrlauc, mddauc, bipauc]\n",
    "aucinwhole = np.mean(totalauc)\n",
    "aucinwhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7cd40ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 2, 1, 2, 0, 0, 1, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6211729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1\n",
      "0  0.872727  0.381818\n",
      "1  0.762500  0.917647\n",
      "2  0.892857  0.720000\n",
      "0    0.842695\n",
      "1    0.673155\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "sens0 = []\n",
    "spec0 = []\n",
    "sens1 = []\n",
    "spec1 = []\n",
    "sens2 = []\n",
    "spec2 = []\n",
    "\n",
    "for i in range(10):\n",
    "    for l in [0, 1, 2]:\n",
    "        prec, recall, _, _ = precision_recall_fscore_support(np.array(testidx) == l, np.array(finalpred[i]) == l, pos_label = True, average = None)\n",
    "        if l == 0:\n",
    "            sens0.append(recall[0])\n",
    "            spec0.append(recall[1])\n",
    "        elif l == 1:\n",
    "            sens1.append(recall[0])\n",
    "            spec1.append(recall[1])\n",
    "        else:\n",
    "            sens2.append(recall[0])\n",
    "            spec2.append(recall[1])\n",
    "            \n",
    "df = [[np.mean(sens0), np.mean(spec0)], [np.mean(sens1), np.mean(spec1)], [np.mean(sens2), np.mean(spec2)]]\n",
    "print(pd.DataFrame(df))\n",
    "print(pd.DataFrame(df).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09bbb5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013021262359279756"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.std(auc[:10]), np.std(auc[10:20]), np.std(auc[20:30])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c41a64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70912"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "894dbffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024240000000000015"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc327a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019671689050830114"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.std(sens0), np.std(sens1), np.std(sens2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c06036a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05438691748688266"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.std(spec0), np.std(spec1), np.std(spec2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d2ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afda2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d6c50ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "model.feature_names = list(snp.columns)\n",
    "plot_importance(xgb_model, ax=ax, max_num_features=20, height=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5dfc6a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/koreagen/koreagen/gene_revision/vit_xgboost100.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = './vit_xgboost100.pkl'\n",
    "joblib.dump(xgb_model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d487467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = joblib.load(filename)\n",
    "model.feature_names = list(snp.columns)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "xgb.plot_importance(model, ax=ax, max_num_features=20, height=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8063f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48da9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9fff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62873471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c0f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
